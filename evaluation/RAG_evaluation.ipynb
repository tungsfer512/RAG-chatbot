{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from operator import itemgetter\n",
    "from typing import Dict, List, Optional, Sequence\n",
    "\n",
    "# import weaviate\n",
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_community.chat_models import ChatCohere\n",
    "from langchain_community.vectorstores import Weaviate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    PromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.runnables import (\n",
    "    ConfigurableField,\n",
    "    Runnable,\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnablePassthrough,\n",
    "    RunnableSequence,\n",
    "    chain,\n",
    ")\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from langchain_fireworks import ChatFireworks\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith import Client\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_index</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>Who was Duke Stelmane?</td>\n",
       "      <td>Duke Stelmane was a major figure of the Knight...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document_index                question  \\\n",
       "2               7  Who was Duke Stelmane?   \n",
       "\n",
       "                                              answer  \n",
       "2  Duke Stelmane was a major figure of the Knight...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data_evaluation.csv\")\n",
    "df.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>source_url</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>https://timdettmers.com/2023/01/30/which-gpu-f...</td>\n",
       "      <td>Which GPU(s) to Get for Deep Learning: My Expe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                                         source_url  \\\n",
       "11     11  https://timdettmers.com/2023/01/30/which-gpu-f...   \n",
       "\n",
       "                                                 text  \n",
       "11  Which GPU(s) to Get for Deep Learning: My Expe...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"./documents.csv\")\n",
    "df1.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"API_KEY\"\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Sử dụng các thông tin sau đây để trả lời câu hỏi của người dùng.\\nNếu bạn không biết câu trả lời, chỉ cần nói rằng bạn không biết, đừng cố bịa ra câu trả lời.\\nHãy trả lời thật chi tiết và chính xác nhất có thể.\\nTất cả câu trả lời của bạn đều phải trả lời bằng tiếng Anh\\n\\nContext: {context}\\nQuestion: {question}\\n\\n'))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Sử dụng các thông tin sau đây để trả lời câu hỏi của người dùng.\n",
    "Nếu bạn không biết câu trả lời, chỉ cần nói rằng bạn không biết, đừng cố bịa ra câu trả lời.\n",
    "Hãy trả lời thật chi tiết và chính xác nhất có thể.\n",
    "Tất cả câu trả lời của bạn đều phải trả lời bằng tiếng Anh\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002624B2E9D90>, search_kwargs={'k': 3, 'threshold': 0.5})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "embed_model = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"./RAG_chatbot/\",\n",
    "    OpenAIEmbeddings(),\n",
    "    allow_dangerous_deserialization=True,\n",
    ")\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3, \"threshold\": 0.5})\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs_rag(question):\n",
    "    return retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": RunnableLambda(itemgetter(\"question\")) | get_docs_rag, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \"bling-phi-3\" model is not specifically mentioned in the provided context. However, based on the naming convention, it likely refers to a variant of the Phi model series, which are typically designed for natural language processing tasks. The Phi models are known for their performance in various language understanding and generation tasks. If you are looking for specific details about the \"bling-phi-3\" model, such as its architecture, capabilities, or intended use cases, I do not have that information available.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"question\": \"What kind of model is the bling-phi-3 model\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning\\n2023-01-30 by Tim Dettmers 1,664 Comments\\n\\nDeep learning is a field with intense computational requirements, and your choice of GPU will fundamentally determine your deep learning experience. But what features are important if you want to buy a new GPU? GPU RAM, cores, tensor cores, caches? How to make a cost-efficient choice? This blog post will delve into these questions, tackle common misconceptions, give you an intuitive understanding of how to think about GPUs, and will lend you advice, which will help you to make a choice that is right for you.\\n\\nThis blog post is designed to give you different levels of understanding of GPUs and the new Ampere series GPUs from NVIDIA. You have the choice: (1) If you are not interested in the details of how GPUs work, what makes a GPU fast compared to a CPU, and what is unique about the new NVIDIA RTX 40 Ampere series, you can skip right to the performance and performance per dollar charts and the recommendation section. The cost/performance numbers form the core of the blog post and the content surrounding it explains the details of what makes up GPU performance.\\n\\n(2) If you worry about specific questions, I have answered and addressed the most common questions and misconceptions in the later part of the blog post.\\n\\n(3) If you want to get an in-depth understanding of how GPUs, caches, and Tensor Cores work, the best is to read the blog post from start to finish. You might want to skip a section or two based on your understanding of the presented topics.\\n\\nContents  hide\\nOverview\\nHow do GPUs work?\\nThe Most Important GPU Specs for Deep Learning Processing Speed\\nTensor Cores\\nMatrix multiplication without Tensor Cores\\nMatrix multiplication with Tensor Cores\\nMatrix multiplication with Tensor Cores and Asynchronous copies (RTX 30/RTX 40) and TMA (H100)\\nMemory Bandwidth\\nL2 Cache / Shared Memory / L1 Cache / Registers\\nEstimating Ada / Hopper Deep Learning Performance\\nPractical Ada / Hopper Speed Estimates\\nPossible Biases in Estimates\\nAdvantages and Problems for RTX40 and RTX 30 Series\\nSparse Network Training\\nLow-precision Computation\\nFan Designs and GPUs Temperature Issues\\n3-slot Design and Power Issues\\nPower Limiting: An Elegant Solution to Solve the Power Problem?\\nRTX 4090s and Melting Power Connectors: How to Prevent Problems\\n8-bit Float Support in H100 and RTX 40 series GPUs\\nRaw Performance Ranking of GPUs\\nGPU Deep Learning Performance per Dollar\\nGPU Recommendations\\nIs it better to wait for future GPUs for an upgrade? The future of GPUs.\\nQuestion & Answers & Misconceptions\\nDo I need PCIe 4.0 or PCIe 5.0?\\nDo I need 8x/16x PCIe lanes?\\nHow do I fit 4x RTX 4090 or 3090 if they take up 3 PCIe slots each?\\nHow do I cool 4x RTX 3090 or 4x RTX 3080?\\nCan I use multiple GPUs of different GPU types?\\nWhat is NVLink, and is it useful?\\nI do not have enough money, even for the cheapest GPUs you recommend. What can I do?\\nWhat is the carbon footprint of GPUs? How can I use GPUs without polluting the environment?\\nWhat do I need to parallelize across two machines?\\nIs the sparse matrix multiplication features suitable for sparse matrices in general?\\nDo I need an Intel CPU to power a multi-GPU setup?\\nDoes computer case design matter for cooling?\\nWill AMD GPUs + ROCm ever catch up with NVIDIA GPUs + CUDA?\\nWhen is it better to use the cloud vs a dedicated GPU desktop/server?\\nVersion History\\nAcknowledgments\\nRelated\\nRelated Posts\\nOverview\\nThis blog post is structured in the following way. First, I will explain what makes a GPU fast. I will discuss CPUs vs GPUs, Tensor Cores, memory bandwidth, and the memory hierarchy of GPUs and how these relate to deep learning performance. These explanations might help you get a more intuitive sense of what to look for in a GPU. I discuss the unique features of the new NVIDIA RTX 40 Ampere GPU series that are worth considering if you buy a GPU. From there, I make GPU recommendations for different scenarios. After that follows a Q&A section of common questions posed to me in Twitter threads; in that section, I will also address common misconceptions and some miscellaneous issues, such as cloud vs desktop, cooling, AMD vs NVIDIA, and others.\\n\\nHow do GPUs work?\\nIf you use GPUs frequently, it is useful to understand how they work. This knowledge will help you to undstand cases where are GPUs fast or slow. In turn, you might be able to understand better why you need a GPU in the first place and how other future hardware options might be able to compete. You can skip this section if you just want the useful performance numbers and arguments to help you decide which GPU to buy. The best high-level explanation for the question of how GPUs work is my following Quora answer:\\n\\nRead Tim Dettmers‘ answer to Why are GPUs well-suited to deep learning? on Quora\\nThis is a high-level explanation that explains quite well why GPUs are better than CPUs for deep learning. If we look at the details, we can understand what makes one GPU better than another.\\n\\nThe Most Important GPU Specs for Deep Learning Processing Speed\\nThis section can help you build a more intuitive understanding of how to think about deep learning performance. This understanding will help you to evaluate future GPUs by yourself. This section is sorted by the importance of each component. Tensor Cores are most important, followed by memory bandwidth of a GPU, the cache hierachy, and only then FLOPS of a GPU.\\n\\nTensor Cores\\nTensor Cores are tiny cores that perform very efficient matrix multiplication. Since the most expensive part of any deep neural network is matrix multiplication Tensor Cores are very useful. In fast, they are so powerful, that I do not recommend any GPUs that do not have Tensor Cores.\\n\\nIt is helpful to understand how they work to appreciate the importance of these computational units specialized for matrix multiplication. Here I will show you a simple example of A*B=C matrix multiplication, where all matrices have a size of 32×32, what a computational pattern looks like with and without Tensor Cores. This is a simplified example, and not the exact way how a high performing matrix multiplication kernel would be written, but it has all the basics. A CUDA programmer would take this as a first “draft” and then optimize it step-by-step with concepts like double buffering, register optimization, occupancy optimization, instruction-level parallelism, and many others, which I will not discuss at this point.\\n\\nTo understand this example fully, you have to understand the concepts of cycles. If a processor runs at 1GHz, it can do 10^9 cycles per second. Each cycle represents an opportunity for computation. However, most of the time, operations take longer than one cycle. Thus we essentially have a queue where the next operations needs to wait for the next operation to finish. This is also called the latency of the operation.\\n\\nHere are some important latency cycle timings for operations. These times can change from GPU generation to GPU generation. These numbers are for Ampere GPUs, which have relatively slow caches.\\n\\nGlobal memory access (up to 80GB): ~380 cycles\\nL2 cache: ~200 cycles\\nL1 cache or Shared memory access (up to 128 kb per Streaming Multiprocessor): ~34 cycles\\nFused multiplication and addition, a*b+c (FFMA): 4 cycles\\nTensor Core matrix multiply: 1 cycle\\nEach operation is always performed by a pack of 32 threads. This pack is termed a warp of threads. Warps usually operate in a synchronous pattern — threads within a warp have to wait for each other. All memory operations on the GPU are optimized for warps. For example, loading from global memory happens at a granularity of 32*4 bytes, exactly 32 floats, exactly one float for each thread in a warp. We can have up to 32 warps = 1024 threads in a streaming multiprocessor (SM), the GPU-equivalent of a CPU core. The resources of an SM are divided up among all active warps. This means that sometimes we want to run fewer warps to have more registers/shared memory/Tensor Core resources per warp.\\n\\nFor both of the following examples, we assume we have the same computational resources. For this small example of a 32×32 matrix multiply, we use 8 SMs (about 10% of an RTX 3090) and 8 warps per SM.\\n\\nTo understand how the cycle latencies play together with resources like threads per SM and shared memory per SM, we now look at examples of matrix multiplication. While the following example roughly follows the sequence of computational steps of matrix multiplication for both with and without Tensor Cores, please note that these are very simplified examples. Real cases of matrix multiplication involve much larger shared memory tiles and slightly different computational patterns.\\n\\nMatrix multiplication without Tensor Cores\\nIf we want to do an A*B=C matrix multiply, where each matrix is of size 32×32, then we want to load memory that we repeatedly access into shared memory because its latency is about five times lower (200 cycles vs 34 cycles). A memory block in shared memory is often referred to as a memory tile or just a tile. Loading two 32×32 floats into a shared memory tile can happen in parallel by using 2*32 warps. We have 8 SMs with 8 warps each, so due to parallelization, we only need to do a single sequential load from global to shared memory, which takes 200 cycles.\\n\\nTo do the matrix multiplication, we now need to load a vector of 32 numbers from shared memory A and shared memory B and perform a fused multiply-and-accumulate (FFMA). Then store the outputs in registers C. We divide the work so that each SM does 8x dot products (32×32) to compute 8 outputs of C. Why this is exactly 8 (4 in older algorithms) is very technical. I recommend Scott Gray’s blog post on matrix multiplication to understand this. This means we have 8x shared memory accesses at the cost of 34 cycles each and 8 FFMA operations (32 in parallel), which cost 4 cycles each. In total, we thus have a cost of:\\n\\n200 cycles (global memory) + 8*34 cycles (shared memory) + 8*4 cycles (FFMA) = 504 cycles\\n\\nLet’s look at the cycle cost of using Tensor Cores.\\n\\nMatrix multiplication with Tensor Cores\\nWith Tensor Cores, we can perform a 4×4 matrix multiplication in one cycle. To do that, we first need to get memory into the Tensor Core. Similarly to the above, we need to read from global memory (200 cycles) and store in shared memory. To do a 32×32 matrix multiply, we need to do 8×8=64 Tensor Cores operations. A single SM has 8 Tensor Cores. So with 8 SMs, we have 64 Tensor Cores — just the number that we need! We can transfer the data from shared memory to the Tensor Cores with 1 memory transfers (34 cycles) and then do those 64 parallel Tensor Core operations (1 cycle). This means the total cost for Tensor Cores matrix multiplication, in this case, is:\\n\\n200 cycles (global memory) + 34 cycles (shared memory) + 1 cycle (Tensor Core) = 235 cycles.\\n\\nThus we reduce the matrix multiplication cost significantly from 504 cycles to 235 cycles via Tensor Cores. In this simplified case, the Tensor Cores reduced the cost of both shared memory access and FFMA operations.\\n\\nThis example is simplified, for example, usually each thread needs to calculate which memory to read and write to as you transfer data from global memory to shared memory. With the new Hooper (H100) architectures we additionally have the Tensor Memory Accelerator (TMA) compute these indices in hardware and thus help each thread to focus on more computation rather than computing indices.\\n\\nMatrix multiplication with Tensor Cores and Asynchronous copies (RTX 30/RTX 40) and TMA (H100)\\nThe RTX 30 Ampere and RTX 40 Ada series GPUs additionally have support to perform asynchronous transfers between global and shared memory. The H100 Hopper GPU extends this further by introducing the Tensor Memory Accelerator (TMA) unit. the TMA unit combines asynchronous copies and index calculation for read and writes simultaneously — so each thread no longer needs to calculate which is the next element to read and each thread can focus on doing more matrix multiplication calculations. This looks as follows.\\n\\nThe TMA unit fetches memory from global to shared memory (200 cycles). Once the data arrives, the TMA unit fetches the next block of data asynchronously from global memory. While this is happening, the threads load data from shared memory and perform the matrix multiplication via the tensor core. Once the threads are finished they wait for the TMA unit to finish the next data transfer, and the sequence repeats.\\n\\nAs such, due to the asynchronous nature, the second global memory read by the TMA unit is already progressing as the threads process the current shared memory tile. This means, the second read takes only 200 – 34 – 1 = 165 cycles.\\n\\nSince we do many reads, only the first memory access will be slow and all other memory accesses will be partially overlapped with the TMA unit. Thus on average, we reduce the time by 35 cycles.\\n\\n165 cycles (wait for async copy to finish) + 34 cycles (shared memory) + 1 cycle (Tensor Core) = 200 cycles.\\n\\nWhich accelerates the matrix multiplication by another 15%.\\n\\nFrom these examples, it becomes clear why the next attribute, memory bandwidth, is so crucial for Tensor-Core-equipped GPUs. Since global memory is the by far the largest cycle cost for matrix multiplication with Tensor Cores, we would even have faster GPUs if the global memory latency could be reduced. We can do this by either increasing the clock frequency of the memory (more cycles per second, but also more heat and higher energy requirements) or by increasing the number of elements that can be transferred at any one time (bus width).\\n\\nMemory Bandwidth\\nFrom the previous section, we have seen that Tensor Cores are very fast. So fast, in fact, that they are idle most of the time as they are waiting for memory to arrive from global memory. For example, during GPT-3-sized training, which uses huge matrices — the larger, the better for Tensor Cores — we have a Tensor Core TFLOPS utilization of about 45-65%, meaning that even for the large neural networks about 50% of the time, Tensor Cores are idle.\\n\\nThis means that when comparing two GPUs with Tensor Cores, one of the single best indicators for each GPU’s performance is their memory bandwidth. For example, The A100 GPU has 1,555 GB/s memory bandwidth vs the 900 GB/s of the V100. As such, a basic estimate of speedup of an A100 vs V100 is 1555/900 = 1.73x.\\n\\nL2 Cache / Shared Memory / L1 Cache / Registers\\nSince memory transfers to the Tensor Cores are the limiting factor in performance, we are looking for other GPU attributes that enable faster memory transfer to Tensor Cores. L2 cache, shared memory, L1 cache, and amount of registers used are all related. To understand how a memory hierarchy enables faster memory transfers, it helps to understand how matrix multiplication is performed on a GPU.\\n\\nTo perform matrix multiplication, we exploit the memory hierarchy of a GPU that goes from slow global memory, to faster L2 memory, to fast local shared memory, to lightning-fast registers. However, the faster the memory, the smaller it is.\\n\\nWhile logically, L2 and L1 memory are the same, L2 cache is larger and thus the average physical distance that need to be traversed to retrieve a cache line is larger. You can see the L1 and L2 caches as organized warehouses where you want to retrieve an item. You know where the item is, but to go there takes on average much longer for the larger warehouse. This is the essential difference between L1 and L2 caches. Large = slow, small = fast.\\n\\nFor matrix multiplication we can use this hierarchical separate into smaller and smaller and thus faster and faster chunks of memory to perform very fast matrix multiplications. For that, we need to chunk the big matrix multiplication into smaller sub-matrix multiplications. These chunks are called memory tiles, or often for short just tiles.\\n\\nWe perform matrix multiplication across these smaller tiles in local shared memory that is fast and close to the streaming multiprocessor (SM) — the equivalent of a CPU core. With Tensor Cores, we go a step further: We take each tile and load a part of these tiles into Tensor Cores which is directly addressed by registers. A matrix memory tile in L2 cache is 3-5x faster than global GPU memory (GPU RAM), shared memory is ~7-10x faster than the global GPU memory, whereas the Tensor Cores’ registers are ~200x faster than the global GPU memory.\\n\\nHaving larger tiles means we can reuse more memory. I wrote about this in detail in my TPU vs GPU blog post. In fact, you can see TPUs as having very, very, large tiles for each Tensor Core. As such, TPUs can reuse much more memory with each transfer from global memory, which makes them a little bit more efficient at matrix multiplications than GPUs.\\n\\nEach tile size is determined by how much memory we have per streaming multiprocessor (SM) and how much we L2 cache we have across all SMs. We have the following shared memory sizes on the following architectures:\\n\\nVolta (Titan V): 128kb shared memory / 6 MB L2\\nTuring (RTX 20s series): 96 kb shared memory / 5.5 MB L2\\nAmpere (RTX 30s series): 128 kb shared memory / 6 MB L2\\nAda (RTX 40s series): 128 kb shared memory / 72 MB L2\\nWe see that Ada has a much larger L2 cache allowing for larger tile sizes, which reduces global memory access. For example, for BERT large during training, the input and weight matrix of any matrix multiplication fit neatly into the L2 cache of Ada (but not other Us). As such, data needs to be loaded from global memory only once and then data is available throught the L2 cache, making matrix multiplication about 1.5 – 2.0x faster for this architecture for Ada. For larger models the speedups are lower during training but certain sweetspots exist which may make certain models much faster. Inference, with a batch size larger than 8 can also benefit immensely from the larger L2 caches.\\n\\nEstimating Ada / Hopper Deep Learning Performance\\nThis section is for those who want to understand the more technical details of how I derive the performance estimates for Ampere GPUs. If you do not care about these technical aspects, it is safe to skip this section.\\n\\nPractical Ada / Hopper Speed Estimates\\nSuppose we have an estimate for one GPU of a GPU-architecture like Hopper, Ada, Ampere, Turing, or Volta. It is easy to extrapolate these results to other GPUs from the same architecture/series. Luckily, NVIDIA already benchmarked the A100 vs V100 vs H100 across a wide range of computer vision and natural language understanding tasks. Unfortunately, NVIDIA made sure that these numbers are not directly comparable by using different batch sizes and the number of GPUs whenever possible to favor results for the H100 GPU. So in a sense, the benchmark numbers are partially honest, partially marketing numbers. In general, you could argue that using larger batch sizes is fair, as the H100/A100 GPU has more memory. Still, to compare GPU architectures, we should evaluate unbiased memory performance with the same batch size.\\n\\nTo get an unbiased estimate, we can scale the data center GPU results in two ways: (1) account for the differences in batch size, (2) account for the differences in using 1 vs 8 GPUs. We are lucky that we can find such an estimate for both biases in the data that NVIDIA provides.\\n\\nDoubling the batch size increases throughput in terms of images/s (CNNs) by 13.6%. I benchmarked the same problem for transformers on my RTX Titan and found, surprisingly, the very same result: 13.5% — it appears that this is a robust estimate.\\n\\nAs we parallelize networks across more and more GPUs, we lose performance due to some networking overhead. The A100 8x GPU system has better networking (NVLink 3.0) than the V100 8x GPU system (NVLink 2.0) — this is another confounding factor. Looking directly at the data from NVIDIA, we can find that for CNNs, a system with 8x A100 has a 5% lower overhead than a system of 8x V100. This means if going from 1x A100 to 8x A100 gives you a speedup of, say, 7.00x, then going from 1x V100 to 8x V100 only gives you a speedup of 6.67x.  For transformers, the figure is 7%.\\n\\nUsing these figures, we can estimate the speedup for a few specific deep learning architectures from the direct data that NVIDIA provides. The Tesla A100 offers the following speedup over the Tesla V100:\\n\\nSE-ResNeXt101: 1.43x\\nMasked-R-CNN: 1.47x\\nTransformer (12 layer, Machine Translation, WMT14 en-de): 1.70x\\nThus, the figures are a bit lower than the theoretical estimate for computer vision. This might be due to smaller tensor dimensions, overhead from operations that are needed to prepare the matrix multiplication like img2col or Fast Fourier Transform (FFT), or operations that cannot saturate the GPU (final layers are often relatively small). It could also be artifacts of the specific architectures (grouped convolution).\\n\\nThe practical transformer estimate is very close to the theoretical estimate. This is probably because algorithms for huge matrices are very straightforward. I will use these practical estimates to calculate the cost efficiency of GPUs.\\n\\nPossible Biases in Estimates\\nThe estimates above are for H100, A100 , and V100 GPUs. In the past, NVIDIA sneaked unannounced performance degradations into the “gaming” RTX GPUs: (1) Decreased Tensor Core utilization, (2) gaming fans for cooling, (3) disabled peer-to-peer GPU transfers. It might be possible that there are unannounced performance degradations in the RTX 40 series compared to the full Hopper H100.\\n\\nAs of now, one of these degradations was found for Ampere GPUs: Tensor Core performance was decreased so that RTX 30 series GPUs are not as good as Quadro cards for deep learning purposes. This was also done for the RTX 20 series, so it is nothing new, but this time it was also done for the Titan equivalent card, the RTX 3090. The RTX Titan did not have performance degradation enabled.\\n\\nCurrently, no degradation for Ada GPUs are known, but I update this post with news on this and let my followers on twitter know.\\n\\nAdvantages and Problems for RTX40 and RTX 30 Series\\nThe new NVIDIA Ampere RTX 30 series has additional benefits over the NVIDIA Turing RTX 20 series, such as sparse network training and inference. Other features, such as the new data types, should be seen more as an ease-of-use-feature as they provide the same performance boost as Turing does but without any extra programming required.\\n\\nThe Ada RTX 40 series has even further advances like 8-bit Float (FP8) tensor cores. The RTX 40 series also has similar power and temperature issues compared to the RTX 30. The issue of melting power connector cables in the RTX 40 can be easily prevented by connecting the power cable correctly.\\n\\nSparse Network Training\\nAmpere allows for fine-grained structure automatic sparse matrix multiplication at dense speeds. How does this work? Take a weight matrix and slice it into pieces of 4 elements. Now imagine 2 elements of these 4 to be zero. Figure 1 shows how this could look like.\\n\\nFigure 1: Structure supported by the sparse matrix multiplication feature in Ampere GPUs. The figure is taken from Jeff Pool's GTC 2020 presentation on  Accelerating Sparsity in the NVIDIA Ampere Architecture by the courtesy of NVIDIA.\\nFigure 1: Structure supported by the sparse matrix multiplication feature in Ampere GPUs. The figure is taken from Jeff Pool’s GTC 2020 presentation on Accelerating Sparsity in the NVIDIA Ampere Architecture by the courtesy of NVIDIA.\\nWhen you multiply this sparse weight matrix with some dense inputs, the sparse matrix tensor core feature in Ampere automatically compresses the sparse matrix to a dense representation that is half the size as can be seen in Figure 2. After this compression, the densely compressed matrix tile is fed into the tensor core which computes a matrix multiplication of twice the usual size. This effectively yields a 2x speedup since the bandwidth requirements during matrix multiplication from shared memory are halved.\\n\\nFigure 2: The sparse matrix is compressed to a dense representation before the matrix multiplication is performed.\\nFigure 2: The sparse matrix is compressed to a dense representation before the matrix multiplication is performed. The figure is taken from Jeff Pool’s GTC 2020 presentation on Accelerating Sparsity in the NVIDIA Ampere Architecture by the courtesy of NVIDIA.\\nI was working on sparse network training in my research and I also wrote a blog post about sparse training. One criticism of my work was that “You reduce the FLOPS required for the network, but it does not yield speedups because GPUs cannot do fast sparse matrix multiplication.” Well, with the addition of the sparse matrix multiplication feature for Tensor Cores, my algorithm, or other sparse training algorithms, now actually provide speedups of up to 2x during training.\\n\\nFigure 3: The sparse training algorithm that I developed has three stages: (1) Determine the importance of each layer. (2) Remove the smallest, unimportant weights. (3) Grow new weights proportional to the importance of each layer. Read more about my work in my sparse training blog post.\\nFigure 3: The sparse training algorithm that I developed has three stages: (1) Determine the importance of each layer. (2) Remove the smallest, unimportant weights. (3) Grow new weights proportional to the importance of each layer. Read more about my work in my sparse training blog post.\\nWhile this feature is still experimental and training sparse networks are not commonplace yet, having this feature on your GPU means you are ready for the future of sparse training.\\n\\nLow-precision Computation\\nIn my work, I’ve previously shown that new data types can improve stability during low-precision backpropagation.\\n\\nFigure 4: Low-precision deep learning 8-bit datatypes that I developed. Deep learning training benefits from highly specialized data types. My dynamic tree datatype uses a dynamic bit that indicates the beginning of a binary bisection tree that quantized the range [0, 0.9] while all previous bits are used for the exponent. This allows to dynamically represent numbers that are both large and small with high precision.\\nFigure 4: Low-precision deep learning 8-bit datatypes that I developed. Deep learning training benefits from highly specialized data types. My dynamic tree datatype uses a dynamic bit that indicates the beginning of a binary bisection tree that quantized the range [0, 0.9] while all previous bits are used for the exponent. This allows to dynamically represent numbers that are both large and small with high precision.\\nCurrently, if you want to have stable backpropagation with 16-bit floating-point numbers (FP16), the big problem is that ordinary FP16 data types only support numbers in the range [-65,504, 65,504]. If your gradient slips past this range, your gradients explode into NaN values. To prevent this during FP16 training, we usually perform loss scaling where you multiply the loss by a small number before backpropagating to prevent this gradient explosion.\\n\\nThe BrainFloat 16 format (BF16) uses more bits for the exponent such that the range of possible numbers is the same as for FP32: [-3*10^38, 3*10^38]. BF16 has less precision, that is significant digits, but gradient precision is not that important for learning. So what BF16 does is that you no longer need to do any loss scaling or worry about the gradient blowing up quickly. As such, we should see an increase in training stability by using the BF16 format as a slight loss of precision.\\n\\nWhat this means for you: With BF16 precision, training might be more stable than with FP16 precision while providing the same speedups. With 32-bit TensorFloat (TF32) precision, you get near FP32 stability while giving the speedups close to FP16. The good thing is, to use these data types, you can just replace FP32 with TF32 and FP16 with BF16 — no code changes required!\\n\\nOverall, though, these new data types can be seen as lazy data types in the sense that you could have gotten all the benefits with the old data types with some additional programming efforts (proper loss scaling, initialization, normalization, using Apex). As such, these data types do not provide speedups but rather improve ease of use of low precision for training.\\n\\nFan Designs and GPUs Temperature Issues\\nWhile the new fan design of the RTX 30 series performs very well to cool the GPU, different fan designs of non-founders edition GPUs might be more problematic. If your GPU heats up beyond 80C, it will throttle itself and slow down its computational speed / power. This overheating can happen in particular if you stack multiple GPUs next to each other. A solution to this is to use PCIe extenders to create space between GPUs.\\n\\nSpreading GPUs with PCIe extenders is very effective for cooling, and other fellow PhD students at the University of Washington and I use this setup with great success. It does not look pretty, but it keeps your GPUs cool! This has been running with no problems at all for 4 years now. It can also help if you do not have enough space to fit all GPUs in the PCIe slots. For example, if you can find the space within a desktop computer case, it might be possible to buy standard 3-slot-width RTX 4090 and spread them with PCIe extenders within the case. With this, you might solve both the space issue and cooling issue for a 4x RTX 4090 setup with a single simple solution.\\n\\nFigure 5: 4x GPUs with PCIe extenders. It looks like a mess, but it is very effective for cooling. I used this rig for 2 years and cooling is excellent despite problematic RTX 2080 Ti Founders Edition GPUs.\\nFigure 5: 4x GPUs with PCIe extenders. It looks like a mess, but it is very effective for cooling. I used this rig for 4 years and cooling is excellent despite problematic RTX 2080 Ti Founders Edition GPUs.\\n3-slot Design and Power Issues\\nThe RTX 3090 and RTX 4090 are 3-slot GPUs, so one will not be able to use it in a 4x setup with the default fan design from NVIDIA. This is kind of justified because it runs at over 350W TDP, and it will be difficult to cool in a multi-GPU 2-slot setting. The RTX 3080 is only slightly better at 320W TDP, and cooling a 4x RTX 3080 setup will also be very difficult.\\n\\nIt is also difficult to power a 4x 350W = 1400W or 4x 450W = 1800W system in the 4x RTX 3090 or 4x RTX 4090 case. Power supply units (PSUs) of 1600W are readily available, but having only 200W to power the CPU and motherboard can be too tight. The components’ maximum power is only used if the components are fully utilized, and in deep learning, the CPU is usually only under weak load. With that, a 1600W PSU might work quite well with a 4x RTX 3080 build, but for a 4x RTX 3090 build, it is better to look for high wattage PSUs (+1700W). Some of my followers have had great success with cryptomining PSUs — have a look in the comment section for more info about that. Otherwise, it is important to note that not all outlets support PSUs above 1600W, especially in the US. This is the reason why in the US, there are currently few standard desktop PSUs above 1600W on the market. If you get a server or cryptomining PSUs, beware of the form factor — make sure it fits into your computer case.\\n\\nPower Limiting: An Elegant Solution to Solve the Power Problem?\\nIt is possible to set a power limit on your GPUs. So you would be able to programmatically set the power limit of an RTX 3090 to 300W instead of their standard 350W. In a 4x GPU system, that is a saving of 200W, which might just be enough to build a 4x RTX 3090 system with a 1600W PSU feasible. It also helps to keep the GPUs cool. So setting a power limit can solve the two major problems of a 4x RTX 3080 or 4x RTX 3090 setups, cooling, and power, at the same time. For a 4x setup, you still need effective blower GPUs (and the standard design may prove adequate for this), but this resolves the PSU problem.\\n\\nFigure 6: Reducing the power limit has a slight cooling effect. Reducing the RTX 2080 Ti power limit by 50-60 W decreases temperatures slightly and fans run more silent.\\nFigure 6: Reducing the power limit has a slight cooling effect. Reducing the RTX 2080 Ti power limit by 50-60 W decreases temperatures slightly and fans run more silent.\\nYou might ask, “Doesn’t this slow down the GPU?” Yes, it does, but the question is by how much. I benchmarked the 4x RTX 2080 Ti system shown in Figure 5 under different power limits to test this. I benchmarked the time for 500 mini-batches for BERT Large during inference (excluding the softmax layer). I choose BERT Large inference since, from my experience, this is the deep learning model that stresses the GPU the most. As such, I would expect power limiting to have the most massive slowdown for this model. As such, the slowdowns reported here are probably close to the maximum slowdowns that you can expect. The results are shown in Figure 7.\\n\\nFigure 7: Measured slowdown for a given power limit on an RTX 2080 Ti. Measurements taken are mean processing times for 500 mini-batches of BERT Large during inference (excluding softmax layer).\\nFigure 7: Measured slowdown for a given power limit on an RTX 2080 Ti. Measurements taken are mean processing times for 500 mini-batches of BERT Large during inference (excluding softmax layer).\\nAs we can see, setting the power limit does not seriously affect performance. Limiting the power by 50W — more than enough to handle 4x RTX 3090 — decreases performance by only 7%.\\n\\nRTX 4090s and Melting Power Connectors: How to Prevent Problems\\nThere was a misconception that RTX 4090 power cables melt because they were bent. However, it was found that only 0.1% of users had this problem and the problem occured due to user error. Here a video that shows that the main problem is that cables were not inserted correctly.\\n\\nSo using RTX 4090 cards is perfectly safe if you follow the following install instructions:\\n\\nIf you use an old cable or old GPU make sure the contacts are free of debri / dust.\\nUse the power connector and stick it into the socket until you hear a *click* — this is the most important part.\\nTest for good fit by wiggling the power cable left to right. The cable should not move.\\nCheck the contact with the socket visually, there should be no gap between cable and socket.\\n8-bit Float Support in H100 and RTX 40 series GPUs\\nThe support of the 8-bit Float (FP8) is a huge advantage for the RTX 40 series and H100 GPUs. With 8-bit inputs it allows you to load the data for matrix multiplication twice as fast, you can store twice as much matrix elements in your caches which in the Ada and Hopper architecture are very large, and now with FP8 tensor cores you get 0.66 PFLOPS of compute for a RTX 4090 — this is more FLOPS then the entirety of the worlds fastest supercomputer in year 2007. 4x RTX 4090 with FP8 compute rival the faster supercomputer in the world in year 2010 (deep learning started to work just in 2009).\\n\\nThe main problem with using 8-bit precision is that transformers can get very unstable with so few bits and crash during training or generate non-sense during inference. I have written a paper about the emergence of instabilities in large language models and I also written a more accessible blog post.\\n\\nThe main take-way is this: Using 8-bit instead of 16-bit makes things very unstable, but if you keep a couple of dimensions in high precision everything works just fine.\\n\\n\\nMain results from my work on 8-bit matrix multiplication for Large Language Models (LLMs). We can see that the best 8-bit baseline fails to deliver good zero-shot performance. The method that I developed, LLM.int8(), can perform Int8 matrix multiplication with the same results as the 16-bit baseline.\\nBut Int8 was already supported by the RTX 30 / A100 / Ampere generation GPUs, why is FP8 in the RTX 40 another big upgrade? The FP8 data type is much more stable than the Int8 data type and its easy to use it in functions like layer norm or non-linear functions, which are difficult to do with Integer data types. This will make it very straightforward to use it in training and inference. I think this will make FP8 training and inference relatively common in a couple of months.\\n\\nIf you want to read more about the advantages of Float vs Integer data types you can read my recent paper about k-bit inference scaling laws. Below you can see one relevant main result for Float vs Integer data types from this paper. We can see that bit-by-bit, the FP4 data type preserve more information than Int4 data type and thus improves the mean LLM zeroshot accuracy across 4 tasks.\\n\\n\\n4-bit Inference scaling laws for Pythia Large Language Models for different data types. We see that bit-by-bit, 4-bit float data types have better zeroshot accuracy compared to the Int4 data types.\\nRaw Performance Ranking of GPUs\\nBelow we see a chart of raw relevative performance across all GPUs. We see that there is a gigantic gap in 8-bit performance of H100 GPUs and old cards that are optimized for 16-bit performance.\\n\\n\\nShown is raw relative transformer performance of GPUs. For example, an RTX 4090 has about 0.33x performance of a H100 SMX for 8-bit inference. In other words, a H100 SMX is three times faster for 8-bit inference compared to a RTX 4090.\\nFor this data, I did not model 8-bit compute for older GPUs. I did so, because 8-bit Inference and training are much more effective on Ada/Hopper GPUs because of the 8-bit Float data type and Tensor Memory Accelerator (TMA) which saves the overhead of computing read/write indices which is particularly helpful for 8-bit matrix multiplication. Ada/Hopper also have FP8 support, which makes in particular 8-bit training much more effective.\\n\\nI did not model numbers for 8-bit training because to model that I need to know the latency of L1 and L2 caches on Hopper/Ada GPUs, and they are unknown and I do not have access to such GPUs. On Hopper/Ada, 8-bit training performance can well be 3-4x of 16-bit training performance if the caches are as fast as rumored.\\n\\nBut even with the new FP8 tensor cores there are some additional issues which are difficult to take into account when modeling GPU performance. For example, FP8 tensor cores do not support transposed matrix multiplication which means backpropagation needs either a separate transpose before multiplication or one needs to hold two sets of weights — one transposed and one non-transposed — in memory. I used two sets of weight when I experimented with Int8 training in my LLM.int8() project and this reduced the overall speedups quite significantly. I think one can do better with the right algorithms/software, but this shows that missing features like a transposed matrix multiplication for tensor cores can affect performance.\\n\\nFor old GPUs, Int8 inference performance is close to the 16-bit inference performance for models below 13B parameters. Int8 performance on old GPUs is only relevant if you have relatively large models with 175B parameters or more. If you are interested in 8-bit performance of older GPUs, you can read the Appendix D of my LLM.int8() paper where I benchmark Int8 performance.\\n\\nGPU Deep Learning Performance per Dollar\\nBelow we see the chart for the performance per US dollar for all GPUs sorted by 8-bit inference performance. How to use the chart to find a suitable GPU for you is as follows:\\n\\nDetermine the amount of GPU memory that you need (rough heuristic: at least 12 GB for image generation; at least 24 GB for work with transformers)\\nWhile 8-bit inference and training is experimental, it will become standard within 6 months. You might need to do some extra difficult coding to work with 8-bit in the meantime. Is that OK for you? If not, select for 16-bit performance.\\nUsing the metric determined in (2), find the GPU with the highest relative performance/dollar that has the amount of memory you need.\\nWe can see that the RTX 4070 Ti is most cost-effective for 8-bit and 16-bit inference while the RTX 3080 remains most cost-effective for 16-bit training. While these GPUs are most cost-effective, they are not necessarily recommended as they do not have sufficient memory for many use-cases. However, it might be the ideal cards to get started on your deep learning journey. Some of these GPUs are excellent for Kaggle competition where one can often rely on smaller models. Since to do well in Kaggle competitions the method of how you work is more important than the models size, many of these smaller GPUs are excellent for Kaggle competitions.\\n\\nThe best GPUs for academic and startup servers seem to be A6000 Ada GPUs (not to be confused with A6000 Turing). The H100 SXM GPU is also very cost effective and has high memory and very strong performance. If I would build a small cluster for a company/academic lab, I would use 66-80% A6000 GPUs and 20-33% H100 SXM GPUs. If I get a good deal on L40 GPUs, I would also pick them instead of A6000, so you can always ask for a quote on these.\\n\\n\\nShown is relative performance per US Dollar of GPUs normalized by the cost for a desktop computer and the average Amazon and eBay price for each GPU. Additionally, the electricity cost of ownership for 5 years is added with an electricity price of 0.175 USD per kWh and a 15% GPU utilization rate. The electricity cost for a RTX 4090 is about $100 per year. How to read and interpret the chart: a desktop computer with RTX 4070 Ti cards owned for 5 years yields about 2x more 8-bit inference performance per dollar compared to a RTX 3090 GPU.\\nGPU Recommendations\\nI have a create a recommendation flow-chart that you can see below (click here for interactive app from Nan Xiao). While this chart will help you in 80% of cases, it might not quite work for you because the options might be too expensive. In that case, try to look at the benchmarks above and pick the most cost effective GPU that still has enough GPU memory for your use-case. You can estimate the GPU memory needed by running your problem in the vast.ai or Lambda Cloud for a while so you know what you need. The vast.ai or Lambda Cloud might also work well if you only need a GPU very sporadically (every couple of days for a few hours) and you do not need to download and process large dataset to get started. However, cloud GPUs are usually not a good option if you use your GPU for many months with a high usage rate each day (12 hours each day). You can use the example in the “When is it better to use the cloud vs a dedicated GPU desktop/server?” section below to determine if cloud GPUs are good for you.\\n\\n\\nGPU recommendation chart for Ada/Hopper GPUs. Follow the answers to the Yes/No questions to find the GPU that is most suitable for you. While this chart works well in about 80% of cases, you might end up with a GPU that is too expensive. Use the cost/performance charts above to make a selection instead. [interactive app]\\nIs it better to wait for future GPUs for an upgrade? The future of GPUs.\\nTo understand if it makes sense to skip this generation and buy the next generation of GPUs, it makes sense to talk a bit about what improvements in the future will look like.\\n\\nIn the past it was possible to shrink the size of transistors to improve speed of a processor. This is coming to an end now. For example, while shrinking SRAM increased its speed (smaller distance, faster memory access), this is no longer the case. Current improvements in SRAM do not improve its performance anymore and might even be negative. While logic such as Tensor Cores get smaller, this does not necessarily make GPU faster since the main problem for matrix multiplication is to get memory to the tensor cores which is dictated by SRAM and GPU RAM speed and size. GPU RAM still increases in speed if we stack memory modules into high-bandwidth modules (HBM3+), but these are too expensive to manufacture for consumer applications. The main way to improve raw speed of GPUs is to use more power and more cooling as we have seen in the RTX 30s and 40s series. But this cannot go on for much longer.\\n\\nChiplets such as used by AMD CPUs are another straightforward way forward. AMD beat Intel by developing CPU chiplets. Chiplets are small chips that are fused together with a high speed on-chip network. You can think about them as two GPUs that are so physically close together that you can almost consider them a single big GPU. They are cheaper to manufacture, but more difficult to combine into one big chip. So you need know-how and fast connectivity between chiplets. AMD has a lot of experience with chiplet design. AMD’s next generation GPUs are going to be chiplet designs, while NVIDIA currently has no public plans for such designs. This may mean that the next generation of AMD GPUs might be better in terms of cost/performance compared to NVIDIA GPUs.\\n\\nHowever, the main performance boost for GPUs is currently specialized logic. For example, the asynchronous copy hardware units on the Ampere generation (RTX 30 / A100 / RTX 40) or the extension, the Tensor Memory Accelerator (TMA), both reduce the overhead of copying memory from the slow global memory to fast shared memory (caches) through specialized hardware and so each thread can do more computation. The TMA also reduces overhead by performing automatic calculations of read/write indices which is particularly important for 8-bit computation where one has double the elements for the same amount of memory compared to 16-bit computation. So specialized hardware logic can accelerate matrix multiplication further.\\nLow-bit precision is another straightforward way forward for a couple of years. We will see widespread adoption of 8-bit inference and training in the next months. We will see widespread 4-bit inference in the next year. Currently, the technology for 4-bit training does not exists, but research looks promising and I expect the first high performance FP4 Large Language Model (LLM) with competitive predictive performance to be trained in 1-2 years time.\\n\\nGoing to 2-bit precision for training currently looks pretty impossible, but it is a much easier problem than shrinking transistors further. So progress in hardware mostly depends on software and algorithms that make it possible to use specialized features offered by the hardware.\\n\\nWe will probably be able to still improve the combination of algorithms + hardware to the year 2032, but after that will hit the end of GPU improvements (similar to smartphones). The wave of performance improvements after 2032 will come from better networking algorithms and mass hardware. It is uncertain if consumer GPUs will be relevant at this point. It might be that you need an RTX 9090 to run run Super HyperStableDiffusion Ultra Plus 9000 Extra or OpenChatGPT 5.0, but it might also be that some company will offer a high-quality API that is cheaper than the electricity cost for a RTX 9090 and you want to use a laptop + API for image generation and other tasks.\\n\\nOverall, I think investing into a 8-bit capable GPU will be a very solid investment for the next 9 years. Improvements at 4-bit and 2-bit are likely small and other features like Sort Cores would only become relevant once sparse matrix multiplication can be leveraged well. We will probably see some kind of other advancement in 2-3 years which will make it into the next GPU 4 years from now, but we are running out of steam if we keep relying on matrix multiplication. This makes investments into new GPUs last longer.\\n\\nQuestion & Answers & Misconceptions\\nDo I need PCIe 4.0 or PCIe 5.0?\\nGenerally, no. PCIe 5.0 or 4.0 is great if you have a GPU cluster. It is okay if you have an 8x GPU machine, but otherwise, it does not yield many benefits. It allows better parallelization and a bit faster data transfer. Data transfers are not a bottleneck in any application. In computer vision, in the data transfer pipeline, the data storage can be a bottleneck, but not the PCIe transfer from CPU to GPU. So there is no real reason to get a PCIe 5.0 or 4.0 setup for most people. The benefits will be maybe 1-7% better parallelization in a 4 GPU setup.\\n\\nDo I need 8x/16x PCIe lanes?\\nSame as with PCIe 4.0 — generally, no. PCIe lanes are needed for parallelization and fast data transfers, which are seldom a bottleneck. Operating GPUs on 4x lanes is fine, especially if you only have 2 GPUs. For a 4 GPU setup, I would prefer 8x lanes per GPU, but running them at 4x lanes will probably only decrease performance by around 5-10% if you parallelize across all 4 GPUs.\\n\\nHow do I fit 4x RTX 4090 or 3090 if they take up 3 PCIe slots each?\\nYou need to get one of the two-slot variants, or you can try to spread them out with PCIe extenders. Besides space, you should also immediately think about cooling and a suitable PSU.\\n\\nPCIe extenders might also solve both space and cooling issues, but you need to make sure that you have enough space in your case to spread out the GPUs. Make sure your PCIe extenders are long enough!\\n\\nHow do I cool 4x RTX 3090 or 4x RTX 3080?\\nSee the previous section.\\n\\nCan I use multiple GPUs of different GPU types?\\nYes, you can! But you cannot parallelize efficiently across GPUs of different types since you will often go at the speed of the slowest GPU (data and fully sharded parallelism). So different GPUs work just fine, but parallelization across those GPUs will be inefficient since the fastest GPU will wait for the slowest GPU to catch up to a synchronization point (usually gradient update).\\n\\nWhat is NVLink, and is it useful?\\nGenerally, NVLink is not useful. NVLink is a high speed interconnect between GPUs. It is useful if you have a GPU cluster with +128 GPUs. Otherwise, it yields almost no benefits over standard PCIe transfers.\\n\\nI do not have enough money, even for the cheapest GPUs you recommend. What can I do?\\nDefinitely buy used GPUs. You can buy a small cheap GPU for prototyping and testing and then roll out for full experiments to the cloud like vast.ai or Lambda Cloud. This can be cheap if you train/fine-tune/inference on large models only every now and then and spent more time protoyping on smaller models.\\n\\nWhat is the carbon footprint of GPUs? How can I use GPUs without polluting the environment?\\nI built a carbon calculator for calculating your carbon footprint for academics (carbon from flights to conferences + GPU time). The calculator can also be used to calculate a pure GPU carbon footprint. You will find that GPUs produce much, much more carbon than international flights. As such, you should make sure you have a green source of energy if you do not want to have an astronomical carbon footprint. If no electricity provider in our area provides green energy, the best way is to buy carbon offsets. Many people are skeptical about carbon offsets. Do they work? Are they scams?\\n\\nI believe skepticism just hurts in this case, because not doing anything would be more harmful than risking the probability of getting scammed. If you worry about scams, just invest in a portfolio of offsets to minimize risk.\\n\\nI worked on a project that produced carbon offsets about ten years ago. The carbon offsets were generated by burning leaking methane from mines in China. UN officials tracked the process, and they required clean digital data and physical inspections of the project site. In that case, the carbon offsets that were produced were highly reliable. I believe many other projects have similar quality standards.\\n\\nWhat do I need to parallelize across two machines?\\nIf you want to be on the safe side, you should get at least +50Gbits/s network cards to gain speedups if you want to parallelize across machines. I recommend having at least an EDR Infiniband setup, meaning a network card with at least 50 GBit/s bandwidth. Two EDR cards with cable are about $500 on eBay.\\n\\nIn some cases, you might be able to get away with 10 Gbit/s Ethernet, but this is usually only the case for special networks (certain convolutional networks) or if you use certain algorithms (Microsoft DeepSpeed).\\n\\nIs the sparse matrix multiplication features suitable for sparse matrices in general?\\nIt does not seem so. Since the granularity of the sparse matrix needs to have 2 zero-valued elements, every 4 elements, the sparse matrices need to be quite structured. It might be possible to adjust the algorithm slightly, which involves that you pool 4 values into a compressed representation of 2 values, but this also means that precise arbitrary sparse matrix multiplication is not possible with Ampere GPUs.\\n\\nDo I need an Intel CPU to power a multi-GPU setup?\\nI do not recommend Intel CPUs unless you heavily use CPUs in Kaggle competitions (heavy linear algebra on the CPU). Even for Kaggle competitions AMD CPUs are still great, though. AMD CPUs are cheaper and better than Intel CPUs in general for deep learning. For a 4x GPU built, my go-to CPU would be a Threadripper. We built dozens of systems at our university with Threadrippers, and they all work great — no complaints yet. For 8x GPU systems, I would usually go with CPUs that your vendor has experience with. CPU and PCIe/system reliability is more important in 8x systems than straight performance or straight cost-effectiveness.\\n\\nDoes computer case design matter for cooling?\\nNo. GPUs are usually perfectly cooled if there is at least a small gap between GPUs. Case design will give you 1-3 C better temperatures, space between GPUs will provide you with 10-30 C improvements. The bottom line, if you have space between GPUs, cooling does not matter. If you have no space between GPUs, you need the right cooler design (blower fan) or another solution (water cooling, PCIe extenders), but in either case, case design and case fans do not matter.\\n\\nWill AMD GPUs + ROCm ever catch up with NVIDIA GPUs + CUDA?\\nNot in the next 1-2 years. It is a three-way problem: Tensor Cores, software, and community.\\n\\nAMD GPUs are great in terms of pure silicon: Great FP16 performance, great memory bandwidth. However, their lack of Tensor Cores or the equivalent makes their deep learning performance poor compared to NVIDIA GPUs. Packed low-precision math does not cut it. Without this hardware feature, AMD GPUs will never be competitive. Rumors show that some data center card with Tensor Core equivalent is planned for 2020, but no new data emerged since then. Just having data center cards with a Tensor Core equivalent would also mean that few would be able to afford such AMD GPUs, which would give NVIDIA a competitive advantage.\\n\\nLet’s say AMD introduces a Tensor-Core-like-hardware feature in the future. Then many people would say, “But there is no software that works for AMD GPUs! How am I supposed to use them?” This is mostly a misconception. The AMD software via ROCm has come to a long way, and support via PyTorch is excellent. While I have not seen many experience reports for AMD GPUs + PyTorch, all the software features are integrated. It seems, if you pick any network, you will be just fine running it on AMD GPUs. So here AMD has come a long way, and this issue is more or less solved.\\n\\nHowever, if you solve software and the lack of Tensor Cores, AMD still has a problem: the lack of community. If you have a problem with NVIDIA GPUs, you can Google the problem and find a solution. That builds a lot of trust in NVIDIA GPUs. You have the infrastructure that makes using NVIDIA GPUs easy (any deep learning framework works, any scientific problem is well supported). You have the hacks and tricks that make usage of NVIDIA GPUs a breeze (e.g., apex). You can find experts on NVIDIA GPUs and programming around every other corner while I knew much less AMD GPU experts.\\n\\nIn the community aspect, AMD is a bit like Julia vs Python. Julia has a lot of potential, and many would say, and rightly so, that it is the superior programming language for scientific computing. Yet, Julia is barely used compared to Python. This is because the Python community is very strong. Numpy, SciPy, Pandas are powerful software packages that a large number of people congregate around. This is very similar to the NVIDIA vs AMD issue.\\n\\nThus, it is likely that AMD will not catch up until Tensor Core equivalent is introduced (1/2 to 1 year?) and a strong community is built around ROCm (2 years?). AMD will always snatch a part of the market share in specific subgroups (e.g., cryptocurrency mining, data centers). Still, in deep learning, NVIDIA will likely keep its monopoly for at least a couple more years.\\n\\nWhen is it better to use the cloud vs a dedicated GPU desktop/server?\\nRule-of-thumb: If you expect to do deep learning for longer than a year, it is cheaper to get a desktop GPU. Otherwise, cloud instances are preferable unless you have extensive cloud computing skills and want the benefits of scaling the number of GPUs up and down at will.\\n\\nNumbers in the following paragraphs are going to change, but it serves as a scenario that helps you to understand the rough costs. You can use similar math to determine if cloud GPUs are the best solution for you.\\n\\nFor the exact point in time when a cloud GPU is more expensive than a desktop depends highly on the service that you are using, and it is best to do a little math on this yourself. Below I do an example calculation for an AWS V100 spot instance with 1x V100 and compare it to the price of a desktop with a single RTX 3090 (similar performance). The desktop with RTX 3090 costs $2,200 (2-GPU barebone + RTX 3090). Additionally, assuming you are in the US, there is an additional $0.12 per kWh for electricity. This compares to $2.14 per hour for the AWS on-demand instance.\\n\\nAt 15% utilization per year, the desktop uses:\\n\\n(350 W (GPU) + 100 W (CPU))*0.15 (utilization) * 24 hours * 365 days = 591 kWh per year\\n\\nSo 591 kWh of electricity per year, that is an additional $71.\\n\\nThe break-even point for a desktop vs a cloud instance at 15% utilization (you use the cloud instance 15% of time during the day), would be about 300 days ($2,311 vs $2,270):\\n\\n$2.14/h * 0.15 (utilization) * 24 hours * 300 days = $2,311\\n\\nSo if you expect to run deep learning models after 300 days, it is better to buy a desktop instead of using AWS on-demand instances.\\n\\nYou can do similar calculations for any cloud service to make the decision if you go for a cloud service or a desktop.\\n\\nCommon utilization rates are the following:\\n\\nPhD student personal desktop: < 15%\\nPhD student slurm GPU cluster: > 35%\\nCompany-wide slurm research cluster: > 60%\\nIn general, utilization rates are lower for professions where thinking about cutting edge ideas is more important than developing practical products. Some areas have low utilization rates (interpretability research), while other areas have much higher rates (machine translation, language modeling). In general, the utilization of personal machines is almost always overestimated. Commonly, most personal systems have a utilization rate between 5-10%. This is why I would highly recommend slurm GPU clusters for research groups and companies instead of individual desktop GPU machines.\\n\\nVersion History\\n2023-01-30: Improved font and recommendation chart. Added 5 years cost of ownership electricity perf/USD chart. Updated Async copy and TMA functionality. Slight update to FP8 training. General improvements.\\n2023-01-16: Added Hopper and Ada GPUs. Added GPU recommendation chart. Added information about the TMA unit and L2 cache.\\n2020-09-20: Added discussion of using power limiting to run 4x RTX 3090 systems. Added older GPUs to the performance and cost/performance charts. Added figures for sparse matrix multiplication.\\n2020-09-07: Added NVIDIA Ampere series GPUs. Included lots of good-to-know GPU details.\\n2019-04-03: Added RTX Titan and GTX 1660 Ti. Updated TPU section. Added startup hardware discussion.\\n2018-11-26: Added discussion of overheating issues of RTX cards.\\n2018-11-05: Added RTX 2070 and updated recommendations. Updated charts with hard performance data. Updated TPU section.\\n2018-08-21: Added RTX 2080 and RTX 2080 Ti; reworked performance analysis\\n2017-04-09: Added cost-efficiency analysis; updated recommendation with NVIDIA Titan Xp\\n2017-03-19: Cleaned up blog post; added GTX 1080 Ti\\n2016-07-23: Added Titan X Pascal and GTX 1060; updated recommendations\\n2016-06-25: Reworked multi-GPU section; removed simple neural network memory section as no longer relevant; expanded convolutional memory section; truncated AWS section due to not being efficient anymore; added my opinion about the Xeon Phi; added updates for the GTX 1000 series\\n2015-08-20: Added section for AWS GPU instances; added GTX 980 Ti to the comparison relation\\n2015-04-22: GTX 580 no longer recommended; added performance relationships between cards\\n2015-03-16: Updated GPU recommendations: GTX 970 and GTX 580\\n2015-02-23: Updated GPU recommendations and memory calculations\\n2014-09-28: Added emphasis for memory requirement of CNNs\\nAcknowledgments\\nI thank Suhail for making me aware of outdated prices on H100 GPUs, Gjorgji Kjosev for pointing out font issues, Anonymous for pointing out that the TMA unit does not exist on Ada GPUs, Scott Gray for pointing out that FP8 tensor cores have no transposed matrix multiplication, and reddit and HackerNews users for pointing out many other improvements.\\n\\nFor past updates of this blog post, I want to thank Mat Kelcey for helping me to debug and test custom code for the GTX 970; I want to thank Sander Dieleman for making me aware of the shortcomings of my GPU memory advice for convolutional nets; I want to thank Hannes Bretschneider for pointing out software dependency problems for the GTX 580; and I want to thank Oliver Griesel for pointing out notebook solutions for AWS instances. I want to thank Brad Nemire for providing me with an RTX Titan for benchmarking purposes. I want to thank Agrin Hilmkil, Ari Holtzman, Gabriel Ilharco, Nam Pho for their excellent feedback on the previous version of this blog post.\", metadata={'source': None, 'seq_num': 12}),\n",
       " Document(page_content='The best sci-fi and fantasy books of 2023\\nIt’s been a stellar year in speculative fiction\\n\\nBy Nicole Clark, Sadie Gennis, and Polygon Staff  Updated Dec 8, 2023, 10:00am EST  34 Comments / 34 New\\nIf you buy something from a Polygon link, Vox Media may earn a commission. See our ethics statement.\\n\\nIt’s been another banner year for science fiction and fantasy books. Many of our favorites once again blur the line between sci-fi and fantasy, but this year was a particular standout for books blurring the line between SFF and other genres. This includes everything from historical fiction — both speculative histories and Westerns — to fable retellings to intergenerational sagas in translation.\\n\\nThough we seem to have crested the wave of pandemic novels, that sense of dread and discoloration has lingered, written into novels of new forms. There’s a preponderance of post-post-apocalyptic science fiction unpacking lofty ideas like sentience and humanity, often set on different planets or among the stars. It has also been a standout year for supernatural horrors and thrillers, particularly ones that mix queer longing with a dose of body horror. Last but not least, it’s been a great year for kissing books set in fantastical worlds.\\n\\nRELATED\\n\\nLooking for more recs? Here are our favorite books of 2022\\nSo jump in and take your pick. Whichever direction you head in, it will be sure to grip you — and make you think. This list is in reverse chronological order, so the newest releases are listed first. We updated this list throughout 2023, sometimes retroactively adding in entries that we missed from earlier in the year. We’ve also included our favorite runners-up.\\n\\nHONORABLE MENTIONS\\nEmily Wilde’s Encyclopaedia of Faeries by Heather Fawcett, Victory City by Salman Rushdie, The Crane Husband by Kelly Barnhill, The Mimicking of Known Successes by Malka Older, Monstrilio by Gerardo Sámano Córdova, White Cat, Black Dog by Kelly Link, Divine Rivals by Rebecca Ross, Our Hideous Progeny by C.E. McGill, The Cheat Code (Wisdom Revolution #3) by Misba, The Deep Sky by Yume Kitasei, Silver Nitrate by Silvia Moreno-Garcia, Vampires of El Norte by Isabel Cañas, Prophet by Sin Blaché and Helen Macdonald, Terrace Story by Hilary Leichter, Her Radiant Curse by Elizabeth Lim, Starling House by Alix E. Harrow, System Collapse (The Murderbot Diaries #7) by Martha Wells, Dark Heir (Dark Rise #2) by C.S. Pacat\\n\\nCover image for Ed Park’s Same Bed Different Dreams, a split image between what looks like Earth and Mars.\\nImage: Random House\\nSAME BED DIFFERENT DREAMS BY ED PARK\\nSame Bed Different Dreams is a remarkable achievement, and not for the faint of heart. Through three storylines, the book creates a kind of speculative history of Korea, with an emphasis on World War II and Japan’s colonial rule and aftermath (and, crucially, the United States’ involvement). One story thread builds out a hefty alternative history of the Korean Provisional Government’s role and reach. Another story thread focuses on a Black Korean War vet who wrote a sci-fi epic series called 2333, which is later adapted into a video game. And yet another story thread has a more futuristic flavor, focusing on a has-been writer who now works for a tech company called GLOAT. These threads periodically intersect — for example, GLOAT ends up owning the rights to 2333, and turns it into a kind of edutainment.\\n\\nIf it sounds like there’s a lot going on, it’s because there is. And it’s made even denser by the author’s Pynchonian sense of humor. Some of its best moments are utterly weird or feel like the writer was smirking — like a character’s dog who can’t stop “archiving” by burying found manuscript pages, the fact that GLOAT employees truly don’t know what the acronym stands for, or the idea that Marilyn Monroe is a member of the Korean Provisional Government. These absurd bits only make it harder to comb apart what’s real and what’s Ed Park’s “alternate history” in sections with realistic-sounding combinations of fact and fiction.\\n\\nIt’s got the same ambitious patchwork as Jennifer Egan’s The Candy House and Namwali Serpell’s The Old Drift. Critics have compared it to everything from David Mitchell’s Cloud Atlas to David Foster Wallace’s Infinite Jest. There’s also, of course, books within the book. It’s a fever dream of a thing, and one I’d heartily recommend, but perhaps with a notebook in hand or some sticky notes to help track the references. (Or perhaps, as I did, just letting the wave of information roll over you, until you’re left with a vast impression and a desire to reread.) —Nicole Clark\\n\\nCover image for Kylie Lee Baker’s The Scarlet Alchemist, featuring a woman in a red outfit with a large crown set against a dark skyline.\\nImage: Inkyard Press\\nTHE SCARLET ALCHEMIST (THE SCARLET ALCHEMIST #1) BY KYLIE LEE BAKER\\nDo not go into The Scarlet Alchemist expecting typical YA fare. What Kylie Lee Baker delivers is a story of visceral brutality, interlaced with elements of Chinese history and thoughtful meditations on family, race, and belonging. It’s a book that can turn your stomach as easily as it can break your heart.\\n\\nSet in an alternate Tang dynasty, the novel follows Zilan, a profoundly talented young alchemist who travels to the capital in hopes of landing a coveted position in the royal service. But being a poor, half Scotian girl means the odds are stacked inordinately high against her in the imperial service exams — and that’s before her skills with the illegal art of resurrection catch the prince’s attention and pull her into a dangerous political game. While the premise seems familiar (underdog competes in trials, falls into star-crossed romance), Baker’s skills with immersive world-building, knotty characters, and genuinely gruesome horror make The Scarlet Alchemist a dazzling and singular tale that left me rushing to read her back catalog. —Sadie Gennis\\n\\nCover image of C Pam Zhang’s Land of Milk and Honey, featuring rollicking hills of white, blue, and yellow.\\nImage: Riverhead\\nLAND OF MILK AND HONEY BY C PAM ZHANG\\nAfter I read How Much of These Hills is Gold in 2020, C Pam Zhang became an instant must-read author in my household. Land of Milk and Honey is entirely unlike her debut — where her debut’s language was sparse and pointed, this book is florid and indulgent — though similar in the extent to which it transported me somewhere entirely new, and more than a little threatening.\\n\\nIn Land of Milk and Honey the climate apocalypse has rendered fresh produce, at scale, a thing of the past — which is to say a provision of the extremely rich. The protagonist, listless and hungry, applies for a job as a private chef for a mysterious family in the Italian Alps (those who live around it call it “\\u200b\\u200bla terra di latte e miele”). While there, she unravels the family’s true intentions, while making them delicious meals from rare ingredients.\\n\\nZhang sensuously describes all pleasures of the tongue, moving from descriptions of lapping of culinary delicacies to the folds of the flesh. Food feels hyperreal, with an emphasis on the texture and taste of every ingredient — and sometimes the cruelty of that ingredient’s procurement. The same can be said of its scenes depicting queer intimacy; that texture and taste take precedent, and the cruelties of human emotion, too. Even after I finished, I was hungry for more. —N. Clark\\n\\nCover image for Megan Kamalei Kakimoto’s Every Drop is a Man’s Nightmare, featuring a red and yellow flower against a painted backdrop.\\nImage: Bloomsbury\\nEVERY DROP IS A MAN’S NIGHTMARE BY MEGAN KAMALEI KAKIMOTO\\nThis short story collection initially caught my attention with its cover, which depicts a woman springing up from the center of a corpse flower, like a stalk standing against the wind. Each story weaves together Hawaiian mythology and the everyday lives of the Hawaiian and mixed-race Japanese women who live there.\\n\\nThese stories range from fabulism to science fiction, all speculative fiction in their own way. In one story, a woman’s encounter with a wild pig ends up foreshadowing a complicated pregnancy later in her life. In another story, a Brazilian waxing company allows people to pay for hairless skin by giving up personality traits. In another story, the narrator falls for a woman who lives with her family — in one of numerous queer stories in the collection — but has to cope with that woman’s decision to return to “what remains of Kaua’i” and join their protests.\\n\\nThe author’s own words, published in The Guardian, sum it up best: “There is a mythical idealisation of the islands of Hawaii as paradise, peace in the tropics; some even call it a modern utopia. Yet this flattening of Hawaii to a postcard image divests our homeland of its culture and colour, reducing us to a place and history that is easily digestible. But we are not easily digestible, and our stories are not meant to be easy for you.” —N. Clark\\n\\nCover image for Shelley Parker-Chan’s He Who Drowned the World, a painted image of ships on a yellow sea, with the moon looming over them.\\nImage: Tor\\nHE WHO DROWNED THE WORLD (THE RADIANT EMPEROR #2) BY SHELLEY PARKER-CHAN\\nAn alternate history of the founding of the Ming dynasty, He Who Drowned the World shifts between four tragically ambitious figures willing to pay any price to materialize their destiny, whether that’s revenge on the empire or crowning themselves the ruler of it. They pursue these goals with unshakeable inertia, doing endlessly cruel and sadistic actions with only the occasional doubts as to whether happiness could be possible if they chose a different path.\\n\\nThis is a relentlessly brutal sequel, and there’s a hopelessness that weighs heavy throughout the book. But Parker-Chan’s penetrating ability to bring empathy and nuance into even the darkest corners of humanity sparks an undeniable connection with these characters, whose self-destructive natures would otherwise be too hard to bear witness to. He Who Drowned the World is a dark and difficult read, yet Parker-Chan’s prose is so brilliant, her character work so complex, that I still found myself sad to leave this world behind. —SG\\n\\nCover image for M.A. Carricks’s Labyrinth’s Heart, featuring a mask-wearing figure with purple wings sprouting out of the top of the mask.\\nImage: Orbit\\nLABYRINTH’S HEART (ROOK & ROSE #3) BY M.A. CARRICK\\nOne of my favorite fantasy series of the past five years, Rook & Rose is an intricately layered trilogy where there are so many secrets, schemes, and conspiracies that at times it’s admittedly difficult to keep track of them all. Because of that, there were a lot of loose ends to tie up in the anticipated conclusion, Labyrinth’s Heart. (Ren alone was juggling four different identities at the novel’s start.) So imagine my surprise when I discovered M.A. Carrick not only managed to leave no question unanswered by the series’ end, but wrapped up even the most complicated storylines in big, bright bows.\\n\\nThere are elements of Labyrinth’s Heart that feel like they were precisely crafted to cater to fans, but here’s the thing: I don’t really care. Carrick created such a lush world populated by lovable characters, an interesting magic system, and a lived-in cultural history that I was just happy to be back in Nadežra after a two-year wait. While things may have been tied up a bit too neatly for my usual tastes, that didn’t stop me from whipping through pages and smiling the whole way through. Sometimes it’s nice to simply soak in a happy ending rather than bathe in the bittersweet. —SG\\n\\nCover art for Kiersten White’s Mister Magic, which features a melting television against a pink background.\\nImage: Del Rey Books\\nMISTER MAGIC BY KIERSTEN WHITE\\nThe latest fantasy-with-an-irresistible-pop-premise from the author of Hide, Mister Magic revolves around a children’s TV show no viewer can forget … or prove it ever existed in the first place. There are no official records of it, no YouTube videos or merchandise or passed-around VHS tapes, and any discussion of it on the internet rapidly disappears. But the people who remember seeing it are convinced the special effects were remarkably vivid and realistic. They agree the central concept is unnerving: a creepy magician-figure leading a group of children in imagination-games aimed at teaching some decidedly non-standard lessons about embracing conformity and meekness. And they’re all sure that something horrible happened while they were watching, though they can’t agree on what.\\n\\nA reunion between five of the former child cast members, taking place 30 years after the show ended, slowly unravels its mysteries, which are even weirder than the description above suggests. Mister Magic is a startling dark fantasy with a lot of foreboding, foreshadowing, and eerie twists. At heart, though, it’s also an incisive story about the kinds of people who revel in control over other people’s lives, and about what an act of rebellion imagination can be. —Tasha Robinson\\n\\nCover image for Rebekah Bergman’s The Museum of Human History, featuring a painted image of a naked figure with a red cloud over the top of their head.\\nImage: Tin House\\nTHE MUSEUM OF HUMAN HISTORY BY REBEKAH BERGMAN\\nA poetic reflection on memory, loss, and connection, The Museum of Human History is a stunning debut reminiscent of the work of Emily St. John Mandel. Slipping backward and forward in time, this introspective mosaic weaves between an identical twin whose sister fell asleep at age 8 and has never aged in the 25 years since, a museum director who questions his place within the family legacy, a widower who lost his most cherished memories as a result of an anti-aging treatment, and others equally struggling with the passage of time. There is a lyrical detachment in Bergman’s prose that leaves you feeling like you’re watching events unfold through a pane of thick glass, never fully able to connect with the characters, yet you remain helplessly transfixed by the haunting cycle they’re caught in. It’s an incredibly melancholy book, but the kind of aching sadness you’re happy to sink into. —SG\\n\\nCover image for Sara Hashem’s The Jasad Heir, featuring what looks like statues of a snake,, a bull, and a griffin.\\nImage: Orbit Books\\nTHE JASAD HEIR (THE SCORCHED THRONE #1) BY SARA HASHEM\\n“Arin of Nizahl was maddeningly elegant. I wanted to cut him open and compare our bones to understand why his gave him grace and mine gave me back pain.” This was the line that absolutely sold me on The Jasad Heir, an irresistible enemies-to-lovers fantasy that reminded me why I’ll never quit this genre.\\n\\nHeadstrong Sylvia is the presumed dead heir of Jasad, a kingdom that was destroyed by the neighboring Nizahl and saw its citizens’ innate magic outlawed. Sylvia managed to carve out a relatively normal life for herself as a chemist’s apprentice, but everything falls apart after she accidentally reveals her magic to the heir of Nizahl. Using her life as leverage, the calculating Arin strikes a deal with Sylvia to help him capture a group of Jasadi rebels and act as his champion in a series of deadly trials. It’s a familiar setup, but one impeccably done by Hashem, who delivers sharp political intrigue, sparkling banter, and touching friendships on top of Sylvia and Arin’s simmering romance. —SG\\n\\nCover image for Kritika H. Rao’s The Surviving Sky, featuring a floating island overgrowing with buildings and plant life, above a stormy planet.\\nImage: Titan Books\\nTHE SURVIVING SKY (THE RAGES TRILOGY #1) BY KRITIKA H. RAO\\nAfter I finished The Surviving Sky, I wouldn’t shut up about it and tried (not always successfully) to get everyone I know to read it. So let me try once more, and maybe with less yelling this time:\\n\\nWith the planet’s surface made unlivable by catastrophic storms, the remains of humanity survive on floating cities constructed of and powered by plants that only a select group of people, known as architects, can control. An archeologist without the ability to traject plants, Ahilya has dedicated her life to finding a way to unshackle humanity’s survival from the architects’ powers and return to the surface. It’s not hard to see why this mission causes friction in her marriage to Iravan, one of the most powerful architects in their city, and one with an arrogance to match his revered status. Though estranged, Ahilya and Iravan come together to help clear his name after he’s accused of pushing his powers dangerously far, an accusation, which if proved true, carries dire consequences for the architect.\\n\\nBut the deeper they look into trajection and its risks, the more Ahilya and Iravan realize they don’t actually know much about where their people – and their powers – came from. And as the floating cities begin to sink toward the earthrages below, the race to save their civilization may also be the end of society as it stands, as Ahilya and Iravan uncover long-buried truths that previous generations worked hard to keep hidden.\\n\\nSo did I do it? Did I convince you to read this Hindu philosophy-inspired debut with some of the most inventive world-building and one of the most complex romances I’ve read in years? Please say yes. You’ll be doing us both a favor. —SG\\n\\nCover image for Alexander Darwin’s The Combat Codes, which features a metallic dragon against a black background.\\nImage: Orbit\\nTHE COMBAT CODES AND GRIEVAR’S BLOOD (THE COMBAT CODES SAGA #1-2) BY ALEXANDER DARWIN\\nIn the world of The Combat Codes, war no longer exists as it used to. Neither does justice — both concepts have been replaced by proxies who fight on behalf of nations or individuals, solving disputes with their fists.\\n\\nAlexander Darwin’s debut novel effectively builds a world around this core concept, bringing it to life with compelling characters and locations (including a classic “magical school for gifted youngsters” situation). The Combat Codes follows Cego, a young abandoned boy skilled at fighting, and Murray, a washed-up former fighter now tasked with scouting the next generation of combatants, whose discovery of Cego changes his entire world.\\n\\nDarwin is also a Brazilian jiu-jitsu practitioner and teacher, and uses that experience in the books’ excellent fight sequences. His evocative and visceral descriptions not only deliver excitement and suspense in this underdog story; they build your understanding of the characters through how they fight. The Combat Codes and its equally fun sequel, Grievar’s Blood, which adds new exciting characters and points-of-view, are the first two parts of a planned trilogy, and I can’t wait for the conclusion next year. —Pete Volk\\n\\nCover image for Katie Williams’ My Murder, showing a woman’s face peering outside of red vertical lines.\\nImage: Riverhead Books\\nMY MURDER BY KATIE WILLIAMS\\nFans of Sarah Gailey’s The Echo Wife won’t want to miss My Murder, which shares some key elements and themes with Gailey’s novel while also taking them in a unique direction. In a near-future with only a few light sci-fi elements, Lou has been resurrected along with a handful of other women murdered by a single serial killer. The politics of resurrection in her world are complicated, and few people qualify. That leaves her and her fellow victims (whose therapy circle recalls Grady Hendrix’s The Final Girl Support Group) a bit at sea as they try to come to terms with their deaths, which none of them can recall, and their new lives as celebrities for all the wrong reasons.\\n\\nLike The Echo Wife, My Murder ends up thoughtfully exploring issues around women subjected to violent men — not just the personal and internal response, but the society that shapes that violence, and responds to it in ways that raise endless questions. The victims all respond to their deaths differently, questioning their culpability and the possible failures that might have made them targets, and navigating their families’ unpredictable responses to their revival. There’s one big mystery at the heart of My Murder, and a whole lot of abrupt and compelling surprises. But at the core, it’s a sci-fi twist on the survivor story, letting some very different people explore what it means to be victimized, and how to reclaim the lives that have been abruptly handed back to them. —TR\\n\\nCover image for Ann Leckie’s Translation State, a minimalist drawing with red, orange, and green, a silhouette of a person, and circular lines.\\nImage: Orbit\\nTRANSLATION STATE BY ANN LECKIE\\nSet in the same universe as Leckie’s Imperial Radch trilogy, Translation State follows Enae, who leaves hir long-standing isolation for what was supposed to be an interstellar goose chase. After hir demanding grandmaman dies, Enae is given a diplomat title and assigned to investigate a missing Presgr translator no one expects to be found (but that the government still wants the goodwill for pretending to look for). Only, Enae doesn’t just pretend to look; sie discovers sie has quite the knack for investigating the 200-year-old cold case.\\n\\nThis is how hir path crosses that of Reet, an adopted maintenance worker whose mysterious origins and unsettling impulses might be explained by being the child of the fugitive translator, if you ask Enae, or the last descendant of a lost sovereign line, if you ask one particularly zealous diaspora social group. Rounding out the POV characters is Qven, a young Presgr terrified of their species’ ritual of merging with an elder, a rite of passage which will see Qven’s selfhood entirely dissolved. Enae, Reet, and Qven’s explorations of their own identities wind up having interplanetary consequences, but it’s the way Leckie gives weight to the small moments, both personal and shared, that make this book sing.\\n\\nThough I’m sure there are layers that only those familiar with the Imperial Radch trilogy will notice and appreciate, the standalone Translation State and its rich exploration of self-identification and personhood serve as a fantastic introduction to Leckie’s world. So don’t hesitate to jump into Translation State if you’re – like me – new to Radch and simply drawn to a thrilling mystery where the most intimate emotions can fuel a universal upheaval. —SG\\n\\nCover image for Rita Chang-Eppig’s Deep as the Sky, Red as the Sea, with facial features set against a crashing wave.\\nImage: Bloomsbury Publishing\\nDEEP AS THE SKY, RED AS THE SEA BY RITA CHANG-EPPIG\\nI still remember standing in my local bookstore, struck by the cover of this book, and reading the summary. It had me at “Chinese pirate queen.”\\n\\nIn Deep as the Sky, Red as the Sea, Chang-Eppig writes a historical fantasy about Shek Yeung, a fearsome Chinese pirate who must navigate her fleet after the death of her powerful husband. She marries her late husband’s second-in-command, with the promise of bearing an heir, in order to retain power over the fleet — and stay a major player as the Chinese Emperor seeks to rid the waters of piracy.\\n\\nThe book isn’t paced like a thriller, so don’t make the mistake of assuming so when you start it. It’s equal parts historical exposition, strategy, and warfare — and it especially excels in its characterization of a complicated woman forced to make difficult decisions and sacrifices in order to protect her power. Fantasy can put its villains and heroes on pedestals, but Deep as the Sky, Red as the Sea never errs in its very human portrayal of Shek Yeung, and how deftly she must play this game of political chess for survival. I was riveted. —N. Clark\\n\\nCover art for Emma Törzs’ Ink Blood Sister Scribe, featuring a dripping pen growing out of the bottom of a tree against a purple background.\\nImage: William Morrow\\nINK BLOOD SISTER SCRIBE BY EMMA TÖRZS\\nThere’s nothing cozier than a magical book about the magic of books — though this tale bends a little darker, and tells a story about witchcraft and complicated family dynamics. In Ink Blood Sister Scribe, two estranged sisters come together to solve the mystery of their family, and prevent further tragedies. In this world, blood can be concocted into ink — wielded by scribes for the creation of books with arcane powers — though the creation of such books drains a scribe’s health. When others read these books, they create magic; willing flowers to bloom, or making magical carpets that can fly in the air.\\n\\nInk Blood Sister Scribe is the perfect sister thriller to read in one sitting. It doesn’t reinvent the wheel, but it doesn’t need to — it simply delivers on a wonderfully entertaining premise. —N. Clark\\n\\nCover art for Martha Wells’ Witch King, featuring a person running across the cover while wearing a cloak and dress fitting for a fantasy setting.\\nImage: Tor\\nWITCH KING BY MARTHA WELLS\\nIn an era where a lot of fantasy fans value quick or cozy reads, Martha Wells’ Witch King feels like a gauntlet thrown at readers’ feet. It’s a complex, meaty fantasy that opens well into what a more linear book would consider the third act, as Kai, the witch king of the title, is exhumed from a watery grave and starts exploring who betrayed him and trapped him there. Readers have to learn everything about Kai’s world as his story unfolds in multiple intertwined timelines. That includes figuring out what a “witch king” is, unwrapping the layers of what Kai actually is and why it matters. It also means being introduced to a wide variety of allies and enemies while alternately flashing back to how he met them, and slowly coming to understand the dense political machinations that shaped all their lives in the past and present.\\n\\nAs with Wells’ Murderbot books and her Books of the Raksura series in particular, part of the draw here is a powerful, skilled protagonist whose biggest struggles are often internal. Kai has a lot of intense emotional responses to the world, but lacks the tools to understand what to do with those feelings, or who to trust with them. Wells packs Witch King with a lot of audacious, expansive world-building for a standalone novel (albeit one that could easily invite sequels or prequels), but what makes Witch King an enjoyable read instead of a frustrating one is the way all the book’s complications and surprises are filtered through Kai’s vivid inner life, giving readers something to hold onto as they’re untangling the puzzlebox aspects of this cleverly structured novel. —TR\\n\\nCover image for Justin Lee Anderson’s The Lost War, featuring five figures walking through white grass after emerging from a dark green forest. Three of the figures wear green cloaks, while two wear white.\\nImage: Orbit\\nTHE LOST WAR (THE EIDYN SAGA #1) BY JUSTIN LEE ANDERSON\\nOriginally self-published in 2019, The Lost War is a traditional fantasy adventure that follows a rag-tag group of strangers on a mission across a war-torn country, fighting monsters and uncovering mysteries along the way. Despite the strong buzz leading up to the novel’s expanded publication by Orbit this year, I found myself hesitant to pick it up since it seemed so similar to many books I’ve read before. But while it’s true The Lost War doesn’t rewrite the genre – it’s filled with well-worn tropes and classic adventurer archetypes – Anderson’s skillful execution left me completely charmed. There is a real Dungeons and Dragons feel to The Lost War, and though the characters are familiar (the honorable paladin, the hard-drinking haunted soldier), Anderson does a fantastic job developing unique dynamics between the party members that vault the book beyond the sum of its parts. And it all builds up to a massive twist at the end that completely upends your understanding of what you’ve read and any previous expectations for where the second book will go. The delightfully unexpected ending once again has the fantasy community buzzing ahead of Anderson’s next release – only this time I’m right there with them. —SG\\n\\nCover image for Moniquill Blackgoose’s To Shape a Dragon’s Breath, a red cover with flowers and a dragon’s head/mask on it.\\nImage: Del Rey\\nTO SHAPE A DRAGON’S BREATH (NAMPESHIWEISIT #1) BY MONIQUILL BLACKGOOSE\\nTo Shape a Dragon’s Breath’s description hooked me immediately: It’s got dragons, a magic school, and a strong teenage main character. Moniquill Blackgoose has taken several different fantasy tropes and created a fantasy novel that’s unlike anything I’ve read; To Shape a Dragon’s Breath is set in an evolving steampunk world as Anglish settlers push the Indigenous Masquapaug people out of their land and onto a remote island. Dragons had long been important cultural touchstones to the Indigenous people, but colonization has, too, pushed them away. To Shape a Dragon’s Breath begins as 15-year-old Anequs finds a dragon egg — the first to be spotted in the area in generations. Anequs is named a Nampeshiweisit, or a dragon rider, as the community helps raise and hatch the dragon’s egg.\\n\\nThe colonizing nation quickly finds out and forces Anequs and her dragon into the Anglish dragon school; if she resists, the dragon will be eliminated. To Shape a Dragon’s Breath is about the growing relationship between her and her dragon Kasaqua, but also about her resistance to the Anglish traditions relating to dragons. The Anglish treat dragons as something to be conquered — they use them as tools and weapons, whereas the Indigenous people have historically partnered with dragons for a relationship built on both tradition and respect.\\n\\nThat partnership means Anequs now has the power to take on colonialism and racism in a new way. Where To Shape a Dragon’s Breath really shines is in that growing relationship between Anequs and Kasaqua; the partnership — and power for both that comes with it — is in stark contrast to the Anglish ways. Bonus: To Shape a Dragon’s Breath has well-written, complex bisexual and neurodivergent characters, too. —Nicole Carpenter\\n\\nCover image for Melvin Burgess’s Loki, a black cover with a black snake wrapped around gold letters with the title.\\nImage: Pegasus\\nLOKI BY MELVIN BURGESS\\nMelvin Burgess has spent a career writing confrontationally frank children’s literature like Junk, his 1990s book about heroin-addicted teenagers. His first adult book, published at age 69, is a blistering, transgressive, and hugely entertaining reframing of the Norse myths, as told by the most unreliable narrator imaginable: Loki himself, the god of tricks, inventions, and political intrigue. But what does reliable mean, anyway, in the mutable world of myth? Burgess paints Loki (or rather, has him paint himself, as he addresses the reader directly in first person) as an eternal outsider, shaking his head sagely at the follies of the gods, and challenging their might-is-right order. But of course, that’s what he’d want us to think. Burgess’ best trick, though, is the way he rolls together the deeply weird, muddy, shape-shifting mystery of the tales themselves with a bracing modernity in characterization and language, somehow without one clashing with the other. In doing so he brings the wild, ancient power of the Norse myths to vivid life. —Oli Welsh\\n\\nCover image for Nana Kwame Adjei-Brenyah’s Chain-Gang All-Stars, featuring a scythe chopping through the words with a bright yellow background.\\nImage: Pantheon Books\\nCHAIN-GANG ALL-STARS BY NANA KWAME ADJEI-BRENYAH\\nIn Chain-Gang All-Stars, prison inmates fight to the death in a series of gladiatorial matches — and all of it is televised to a hungry audience. It’s a program called CAPE, the Criminal Action Penal Entertainment, which promises freedom to inmates who survive three years of its brutality. The average life expectancy for anyone who enters is three months. Within this system, Loretta Thurwar and Hammara Stacker (called Hurricane Staxxx by her fans) emerge as two frontrunners.\\n\\nThis National Book Award finalist takes on the viciousness of the carceral system, with more than a bit of The Hunger Games’ DNA sprinkled in. “Hard action” fans salivate over matches, a self-obsessed announcer resents the fact that contestants don’t offer more banter, and the women who top the leaderboards become sex symbols in pop culture. But where other fight-to-the-death dystopias — among the greats, like Battle Royale or Lord of the Flies — spin a more fantastical yarn, Chain-Gang All-Stars is aimed right at the heart of the all-too-real cruelties of our existing for-profit penal system.\\n\\nEarly in the book, Thurwar kills a 16-year-old boy in a gladiator match. Fans in the stands lament not the death of the boy, but the idea that the fight wasn’t entertaining because it wasn’t a fair matchup. In a footnote, Adjei-Brenyah writes of George Stinney Jr., a 14-year-old Black boy who was convicted for murder and executed in 1944. Chain-Gang All-Stars also illustrates the ways in which imprisonment is simply “slavery by another name,” showing all manner of menial labor the contestants are forced to perform. In 2022, the ACLU reported that inmates made between 13 and 52 cents an hour, and sometimes nothing.\\n\\nCritics have said this book is an “act of protest” but that it doesn’t “straightforwardly preach,” or that it’s more entertaining than “an attempt to convince its readers of the case for prison abolition has any right to be.” I understand why you’d want to say this book is “fun” despite an abolitionist message, especially in a political climate where radical writing is often appreciated only as a teaching tool. But I think that kind of delineation undercuts Adjei-Brenyah’s talent as a novelist, and his skill in heightening the real as a form of storytelling. I’d call it thrilling, over calling it fun. And the fact that it is thrilling is inextricable from its openly abolitionist values — it’s the very knowledge of real life that Adjei-Brenyah wields to craft suspense. —N. Clark\\n\\nCover image for Rebecca Yarros’ Fourth Wing, which features a circle image behind black text, with clouds and some flying creatures.\\nImage: Entangled\\nFOURTH WING BY REBECCA YARROS\\nThis action-packed, fantasy romance feels like a grown up version of all of my favorite young adult books. It’s got all of the fun nostalgic tropes — a magical school, deadly trials, dragon riding, and a love triangle between the main character, a golden retriever love interest, and a misunderstood emo rival — but it’s also extremely horny, as all fun fantasy romance must be.\\n\\nViolet Sorrengail is thrown into a series of trials in order to prove whether she can be a dragon rider. There are a few problems with this: she trained as a scribe, never thought she’d be thrust into danger, and she also must deal with Xaden Riorson, her sworn enemy (wink). She also manages a joint condition, which leaves her in chronic pain — a fact the book handles gracefully. In one of my favorite climactic moments of the book, Violet is given a mobility device to help her with her trials; those close to her remind her that it doesn’t diminish her power, but is a tool like any other, and one that allows her to flourish. I’m thrilled to read the next installment, when it comes out in November. —N. Clark\\n\\nCover art for Adrian Tchaikovsky’s Lords of Uncreation, which shows a spaceship approaching what looks like a space battle next to a planet, with exploding orbs in space and a lot of spaceships in the distance.\\nImage: Orbit\\nLORDS OF UNCREATION (THE FINAL ARCHITECTURE #3) BY ADRIAN TCHAIKOVSKY\\nReading the Final Architecture series, I had to accept long ago that I would never fully grasp the nuances of some of its central concepts, even if I understood them on an instinctual level.\\n\\nThis acceptance set me up well for Lords of Uncreation, which revolves around concepts that even the characters find impossible to understand, and whose minds may literally break if they try to. Like looking directly into the sun, confronting the blurred space between the real and unreal (as well as the eldritch terrors that lurk within) poses a grave threat to those doing so head-on – at least to anyone other than weary intermediary Idris Tellemier, whose risk is merely reduced rather than eliminated. But the characters Adrian Tchaikovsky has populated this world with are so grounded, so emotionally rich, and so vibrant that the details of the brain-bending threats lurking within unspace become secondary to their impact on the lives of and relationships between the Vulture God’s crew.\\n\\nThis is not to say that Tchaikovsky does not deliver an incredibly satisfying conclusion to the mysteries of unspace (he does!). But what I’ll remember most is how he crafted the perfect emotional resolution to this intellectually intricate tale that left me in tears and has stayed with me since. —SG\\n\\nLead art for Justin Cronin’s The Ferryman, which pictures a cloudy sky over the horizon, as a single sail boat sits on the water.\\nImage: Ballantine Books\\nTHE FERRYMAN BY JUSTIN CRONIN\\nProctor Bennett is a ferryman, whose duty is to guide unhappy citizens from the utopian Propersa to the Nursery, where they retire their old selves before returning in younger bodies with no memories of their former lives. But when Proctor is assigned to retire his own father, the troubling encounter sends him careening off the path of conformity. He begins questioning prescribed truths and confronting the darker side of Prospera, which runs off the work of a disenfranchised support staff whose discontent is building towards a revolution that pulls Proctor into its orbit.\\n\\nThough this premise may feel familiar, The Ferryman is anything but. This tightly-wound, atmospheric thriller weaves together layers of knotted mystery with Proctor’s haunting POV as he grapples with his relationship to grief, happiness, family, and identity. It’s a sharply complex mystery with a cinematic quality to it. Throughout reading, I couldn’t help but fan-cast who would star in a Christopher Nolan adaptation of it. But even if you aren’t an Inception fan, it’ll be easy to become immersed in The Ferryman’s distinct dystopian world. —SG\\n\\nCover image for Emily Tesh’s Some Desperate Glory, featuring a woman walking confidently in front of a wall opening to reveal a planetary body.\\nImage: Tor\\nSOME DESPERATE GLORY BY EMILY TESH\\nAround September, as the pile of unpainted plastic miniatures here in my home office began to get particularly deep, I suddenly ran out of Warhammer 40,000 Black Library audiobooks by Games Workshop that I was the least bit interested in listening to. That’s when I stumbled upon Some Desperate Glory by Emily Tesh. Billed as a space opera told from the perspective of one of humanity’s last genetically engineered super soldiers, I fell for the premise hook, line, and sinker. Then, about 50 pages in, I let it sucker-punch me right in the gut.\\n\\nWith Some Desperate Glory, Tesh has envisioned a deeply affecting reality where the children of a subjugated, war-torn race slowly come to realize that they have been lied to — manipulated into an amoral war of vengeance without end. Tesh shows incredible restraint throughout, reeling out a thick and binding thread of painful realizations from deep within the main character, Kyr. After grappling with my personal love for the grim darkness of the far future for quite a few years now, this book helped me come to terms with how much I despise those tropes even as I find myself drawn toward them time and time again.\\n\\nSome Desperate Glory is, in my opinion, required reading for anyone who has ever painted a Space Marine in earnest – and a new fixture in the canon of queer science fiction. —Charlie Hall\\n\\nCover image for Jade Song’s Chlorine, featuring a large fin in the ocean waves.\\nImage: William Morrow & Company\\nCHLORINE BY JADE SONG\\nI think I have been waiting my whole life for this book — for someone to write adolescence like the body horror it is, with all of the cultural specificity of being a Chinese American girl, simply bursting at the seams with sapphic longing. Chlorine stars Ren Yu, a swimmer who believes that she is a mermaid. But she is tethered to land by her human ambition: By the parents who constantly push her to achieve, and by a swim coach who pays inappropriate attention to her — pushing her to swim faster times, while also making her feel uncomfortable in her skin.\\n\\nRen’s steadfast belief in being a mermaid feels both like a flight of fancy, and increasingly like a means of dissociating from the horrors of everyday life. Being a young girl is hard enough without having to contend with the high expectations of parents, the predation of adult men, and the casual racism of peers. Jade Song’s writing is gruesomely lyrical, contrasting the sublime with the deeply disturbing. There were several points where this book almost made me throw up, and I mean that as a high compliment. —N. Clark\\n\\nA Black woman stands alone in a field, her face covered by shadow, in the cover art for Lone Women by Victor LaValle.\\nImage: One World\\nLONE WOMEN BY VICTOR LAVALLE\\nAdelaide Henry is traveling to Montana, where she plans on making a new life as a homesteader — leaving the flames of her California home, and the bodies of her parents, behind. But she has a heavy weight to carry. She lugs an enormous steam trunk wherever she goes; whenever the trunk opens, people around her die. In 1915, Montana is in the middle of a homestead boom, and though Adelaide aims to make a new start, not everyone is welcoming to a Black woman traveling alone.\\n\\nVictor LaValle mixes horror and fantasy in this expertly paced tale. It’s satisfyingly bloody, while making incisive commentary on the price of being an outsider. The Western genre has long fixated on the white imagination, perhaps occasionally making space for the early struggle of the suffragettes. But LaValle’s vision of history emphasizes just how powerful white women are in upholding the interests of their white husbands, and how far these women will go to protect the societal structures that put them in proximity to power. Lone Women also examines how shame, and the family unit, ultimately uphold these unspoken rules — ostracizing those who might otherwise find community support.\\n\\nThis book was so good that I am now reading my way through every interview LaValle has given on the Lone Women press circuit, too, and then reading every book he references. What a gift! —N. Clark\\n\\nCover image of Nathan Ballingrud’s The Strange, depicting a diner on Mars.\\nImage: Gallery/Saga Press\\nTHE STRANGE BY NATHAN BALLINGRUD\\nNathan Ballingrud’s debut novel was added to my TBR pile after seeing it marketed as a blend of Ray Bradbury’s The Martian Chronicles and Charles Portis’ True Grit. I’m always dubious about marketing comparisons, but was thrilled when The Strange delivered on this high promise.\\n\\nIn an alternate history where humanity colonized Mars in the early 1900s, the red planet has lost all communication with Earth, leaving the fate of 14-year-old Annabelle Crisp’s mother unknown. When a thief steals Annabelle’s sole voice recording of her mom, she and her beloved Kitchen Engine, Watson, set off into the desert to retrieve what’s hers and see justice served. The longer Annabelle’s adventure goes on, the more she loses perspective and drifts away from righteousness in dogged pursuit of her own selfish desires. Struggling to comprehend that the world can’t be divided into binaries like right or wrong and black or white, Annabelle converts her fear into anger, lashing out and harming those around her, including those providing aid.\\n\\nAnnabelle can be vengeful and cruel, and though I often disagreed with her choices, Ballingrud makes it impossible not to understand and empathize with her. Annabelle Crisp isn’t a hero and she isn’t a villain, but she is an outstanding protagonist in a wonderfully original sci-fi tale. —SG\\n\\nCover image for Moses Ose Utomi’s The Lies of the Ajungo, featuring a figure walking upside down on mounds of sand as a castle lurks in front.\\nImage: Tor\\nTHE LIES OF THE AJUNGO (THE FOREVER DESERT #1) BY MOSES OSE UTOMI\\nIn his debut novella, Moses Ose Utomi wields his precise prose to tell a dark, visceral fable about a young boy from the City of Lies, a metropolis reliant on the brutal Ajungo Empire for their supply of water. But the cost of this trade is high: At 13, every child of the City of Lies has their tongue cut out and sent to the Ajungo.\\n\\nEven with this gruesome tithe, the Ajungo send barely enough water for the population to survive, and far from what they’d need to do so comfortably, let alone thrive. Shortly before his thirteenth birthday, the brave Tutu sets out on a dangerous journey to save his mother and the city by finding their own water supply. As Tutu explores the outside world for the first time, his perception of truth and history is challenged, and he comes to understand how the decisions and deceptions of those in power rewrite the past and shape the future to uphold those with privilege and foster compliance in those who don’t. —SG\\n\\nCover image for Edward Ashton’s Antimatter Blues, A Mickey7 Novel. It features an astronaut from behind on a rocky planet, looking out at another planet in the distance.\\nImage: St. Martin’s Press\\nANTIMATTER BLUES BY EDWARD ASHTON\\nEdward Ashton’s sequel to Mickey 7, the 2022 novel Parasite director Bong Joon-ho is adapting as a movie starring Robert Pattinson, takes up two years after the first book left off, with “Expendable”-status planetary colonist Mickey still on the outs with the leadership of his struggling colony after a gutsy bluff he made to ensure his own survival. The sixth clone of the original Mickey, who accepted life as a disposable body for suicide missions in exchange for a ticket to space, Mickey 7 has walked off that job. His ongoing draw on the colony’s resources is only tolerated because he’s exaggerated his diplomatic connections with the local aliens. Then the base commander orders him to do something impossible, or the entire colony will die.\\n\\nAntimatter Blues is knottier than the first book in the series, with more to take in about the ethics of survival and humanity’s predisposition toward xenophobia and selfish, self-serving behavior. It sure isn’t a pleasant book to read: A lot of Mickey’s co-colonists are bigots, most of them are indifferent to anyone else’s suffering, and at times, the book reads as though Earth deliberately sent all the worst people into space, the better to be free of them. Even Mickey himself is, at absolute minimum, generally more focused on his own safety and comfort than on the horrific results of some of his choices. But as soon as he’s placed in what seems like an unsurvivable situation, that dynamic leads to high drama, and Antimatter Blues becomes a breathless book rocketing to a surprising conclusion. Prepare to feel sorry for various alien races who have to deal with icky humanity. —TR\\n\\nCover image for Samantha Shannon’s A Day of Fallen Night, a colorful image with a a dragon swirling around it\\nImage: Bloomsbury\\nA DAY OF FALLEN NIGHT (THE ROOTS OF CHAOS #0) BY SAMANTHA SHANNON\\nSamantha Shannon’s A Day of Fallen Night is her second book in the Roots of Chaos series, but a prequel to The Priory of the Orange Tree. Like The Priory of the Orange Tree, A Day of Fallen Night is an epic, far-flung fantasy novel set in a world of magic and dragons. A Day of Fallen Night is set hundreds of years before The Priory of the Orange Tree, and follows several of the original book’s ancestors as the world fears the return of an evil wyrm, the Nameless One. You don’t have to have read The Priory of the Orange Tree to enjoy A Day of Fallen Night; in fact, it’s likely a good place to start if you’ve been interested in reading Shannon’s original, massive fantasy book. Of course, this is a slow-burn 800-page book that precedes another 800-page book, so it’s definitely a time investment regardless of the path.\\n\\nThough A Day of Fallen Night deals with a world-shaping, cataclysmic threat and widespread political machinations, the book is rooted within four characters from around the book’s world: Sabran, Glorian, Dumai, and Tunuva Melim. The stories of these characters intertwine as their regional beliefs tied to wyrms and dragons conflict, muddying up the necessary collaboration in fighting off the looming threat. In between all that catastrophe, Shannon gives the women of the book rich stories of personal relationships, sacrifice, and conflicting feelings. Motherhood and bodily autonomy are also strong themes throughout the book; both Sabran and Glorian (mother and daughter) have their bodily autonomy tied to the fate of their region.\\n\\nIt’s not easy to describe A Day of Fallen Night in a short blurb — it does so many things and goes so many places. Shannon’s created a series that has the scale of The Lord of the Rings, wrapped up in a world of queer, female power. The Roots of Chaos, as a whole, is one of my favorite fantasy series ever. —N. Carpenter\\n\\nCover image for Mariana Enriquez’s Our Share of Night, featuring a red hand with long yellow fingernails.\\nImage: Hogarth Press\\nOUR SHARE OF NIGHT BY MARIANA ENRÍQUEZ\\nThis literary tome defies categorization, so I’ll paint a scene instead: A father (Juan) whisks his son (Gaspar) away on a trip. Juan is mercurial; at turns terrifying and violent, at turns bewilderingly tender, nearly infinite in love. But he is a closed book. And if you think you’ve seen his hands elongate, spindly fingers yielding to piercing claws — well no, you didn’t.\\n\\nSlow, dreadful, and razor-sharp, Our Share of Night charts a family’s desperate attempt at escaping the clutches of a death cult in Argentina. Its members seek the secrets of immortality, and many are willing to pay any price to obtain it. Set in 1981, the novel’s supernatural terrors intertwine with those of the Dirty War, the authoritarian violence offering cover for the cult to operate uninhibited.\\n\\nI will read anything Mariana Enríquez writes next, it’s an absolute joy to experience her work. —N. Clark\\n\\nCover image for Annalee Newitz’s The Terraformers, which features a futuristic cityscape with lush greenery.\\nImage: Tor Books\\nTHE TERRAFORMERS BY ANNALEE NEWITZ\\nThe Terraformers concerns itself with one question: As a species evolves, what behaviors stick around? Set more than 50,000 years in the future (yes, you read that number right), The Terraformers details the process of terraforming and developing a privatized planet into a tourism joint for the super rich. Technology has advanced in barely fathomable ways, allowing, for instance, the extension of human-level intelligence to animals and robots. But some aspects of society might seem familiar: Real estate developers who jack up rent with no warning? Local governments that abhor public transit? That every video call still has one person who can’t get the camera to work?\\n\\nEqual parts prescient and absurd, The Terraformers splits its story over three novellas, each 700 years apart. One of those stars a sentient train who teams up with an investigative journalist ... who also happens to be a cat ... who’s also trying to prove this ostensibly privatized planet is in fact public land. Written by a leading science journalist of our era (author Annalee Newitz is the founder of io9 and has written for basically every major science publication under our sun), The Terraformers is unexpectedly one of the most accurate representations of the journalistic process I’ve ever read. And it all culminates in an undeniable stance: That capitalistic power must still be held in check by the truth. Even 50,000 years in the future, a free press is among society’s most essential facets. The more things change... —Ari Notis\\n\\nThe cover image of Adrian Tchaikovsky’s Children of Memory, which depicts a spaceship approaching a large orange planet.\\nImage: Orbit\\nCHILDREN OF MEMORY (CHILDREN OF TIME #3) BY ADRIAN TCHAIKOVSKY\\nAdrian Tchaikovsky’s highly anticipated third book in the Children of Time trilogy once again delves into some of science fiction’s headiest topics. There are parallels to earlier installments — Tchaikovsky once again uses another hyper-intelligent animal species to examine the idea of what being “alive” really means. But he also takes readers somewhere completely and utterly new, outside the scope of the previous titles, and incredibly difficult to describe without spoiling the premise entirely.\\n\\nAll I can say is hold on for the ride. This is an author who dives head first into Asimov-esque ideas, and who is willing to take the plot in fanciful directions. I still can’t believe that I have recommended a book about sentient spider colonies to so many friends, but here we are. This finale is worth your time. —N. Clark', metadata={'source': None, 'seq_num': 19}),\n",
       " Document(page_content='Semantic and Textual Inference Chatbot Interface (STICI-Note) - Part 1: Planning and Prototyping\\n\\nThe start of my RAG to riches story\\n\\nSTICI-note\\n\\nPublished: Mon, 27 May 2024\\n\\nLast modified: Tue, 04 Jun 2024\\n\\nIntroduction\\n\\nIn this three-part series, I will be talking you through how I built the Semantic and Textual Inference Chatbot Interface (or STICI-note for short), a locally run RAG chatbot that uses unstructured text documents to enhance its responses. I came up with the name when I was discussing this project with a friend and asked him whether he had any ideas of what to call it. He said, \"You should call it sticky because it sounds funny.\" The name... stuck...\\n\\nThe code for this project is available here.\\n\\nIn this part, I will be planning the project from the tech stack to the techniques I will use, and I will be building a prototype. I will be discussing all of the choices I made and all of the choices I didn’t make, so I hope you find this insightful. Without further ado, let’s get started.\\n\\nThe Problem\\n\\nIn my spare time, I occasionally play Dungeons and Dragons (DnD), a tabletop roleplaying game, and the stories are often told over several months, so details can be easily lost or forgotten over time. I can write notes on my laptop, but sometimes regular text search does not always provide me with the results I want when trying to search for specific notes. Some common examples include when a keyword is used often (e.g., I might write a lot about the actions of “Miles Lawson,” but only one segment of text might describe who he is, making searching for information on his character like finding a needle in a haystack) or when I simply cannot think of the correct keyword to search (e.g., what if I search “silver” instead of “gold”?).\\n\\nOne day, I thought to myself that it’d be great if I had a tool that I could write my DnD notes into in an unstructured way and retrieve the information at any time with simple questions like “Who is Miles Lawson?” or “How much silver did I pay for a stay in ye olde tavern?”. This tool could be extended to be used for querying my notes on many things that are not available online (and therefore not searchable on a search engine), such as documentation on software that I build, notes on things that I’m learning about, such as AWS cloud infrastructure, and my diary of my deepest thoughts and feelings (at least I hope this is not available online). And thus, I decided to start working on STICI-note because the tools available online that do this cost money and run on the cloud, and I’m a tight-fisted guy who’s very sceptical about company promises to not sell your data.\\n\\nNarrowing Down Features\\n\\nAs with all projects, I began by deciding what features I needed from this tool.\\n\\nRequired features:\\n\\nChatbot that you can ask questions and get answers in response (conversational memory is not required).\\nInformation is taken from an unstructured text file.\\nIt must be able to tell me if it doesn’t know the answer to my question.\\nFast.\\nEfficient enough to run on my MacBook with other programs without any performance issues.\\nLocally run for privacy and to ensure it will always be free, runnable, and consistent.\\nConversational memory is the memory of previous interactions given to an LLM. I decided not to require it as a feature because I just need the AI to answer my questions about the given text. It might be added as a feature in the future if I feel like I need it, but I do not plan to include it in the initial version of STICI-note.\\n\\nI knew that limiting it to running on my M1 MacBook with 8 GB of memory would greatly limit the performance of the tool as I would not be able to access truly large language models like GPT-4 and Claude 3 Opus, but I decided to do it anyway primarily for privacy but also to remove dependencies on external organisations to reduce the maintenance required for the tool in the future.\\n\\nPlanning How to Evaluate and Compare Solutions\\n\\nIf you don’t evaluate a solution, how do you know whether it’s an effective solution? You don’t.\\n\\nI next planned how I would evaluate different variations of the tool. While I do not evaluate anything in this part, I decided to sketch out a rough plan of how I would evaluate different solutions to encourage designing a testable AI in the same way that Test-Driven Development (TDD) encourages you to write testable code.\\n\\nAt first, I considered using Wikipedia pages as the data source and making my own questions about the content of the pages before I realised that this would lead to data leakage as many LLMs are trained on Wikipedia data.\\n\\nAn alternative dataset that I considered using for evaluation is the TREC 2024 RAG Corpus. This is a 28 GB corpus of text documents scraped from the internet. This corpus comes with thousands of queries, and the relevant documents for each query have been tagged as such. This is an amazing corpus for training and evaluating vector DataBase (DBs). Ignoring the fact that its questions do not come with answers, meaning I would have to write my own answers to use the document, there is one glaring flaw that makes it unusable for my use case: the documents are generally relatively short and describe a large variety of things. In my use case, I expect documents to be long and typically written about the same topic. If I were to use the corpus, I would have to stick documents together to present a realistic challenge in the semantic search of the vector DB vector space, but as each document will likely be about very different topics (e.g., one might be about aviation while another might be about economics), context retrieval would be unrealistically easy.\\n\\nAnother alternative evaluation dataset that I considered using was a synthetic dataset. By following a method like this, I can use an LLM to generate synthetic context and questions automatically. I decided not to do this as I was concerned that this would produce bad-quality data with a massive bias towards things an LLM might already know, despite the use case expecting data that the LLM does not already know.\\n\\nBecause the documents in my evaluation corpus need to be thousands of words in length while staying relevant to a topic and they need to include information that will not be in the LLM’s training data, I decided that it would be best to manually curate a small dataset to evaluate my models. I plan to create documents from sources on the internet like videogame wikis, people’s blogs, and scientific journals and write my own pairs of questions and answers about them. I will then evaluate the difference between the model’s answer and my answer using a semantic similarity score.\\n\\nRAG vs a Really Big Context Window vs Infinite Context\\n\\nTo be able to answer questions about documents, the LLM would need to have access to information from the documents. I thought of three potential solutions for this:\\n\\nRetrieval Augmented Generation (RAG)\\nAn LLM with a really big context window\\nAn LLM that supports infinite context length\\nUsing an LLM with a really big context window such as Anthropic’s Claude 3 and Google’s Gemini 1.5 would certainly give me the best results as it would allow inference using completely unfiltered and uncompressed context as they can handle inputs of over 700,000 words, but these models are closed-source, and there is absolutely no chance of a model of this size fitting into my tiny M1 MacBook with 8 GB of memory.\\n\\nBy “an LLM that supports infinite context length,” I mean models like Mamba, Megalodon, and Infini-attention that compress context into a finite space. I decided not to use a model like this for two main reasons. Firstly, I have concerns about the performance. These architectures are in their infancy, and I do not expect them to outperform equivalently-sized traditional transformers. Secondly, as these architectures are very new and experimental, I do not expect much support for them, especially for Apple’s M-series of chips, which have their own graphics API, metal, that is required for GPU acceleration on my MacBook. These architectures are very interesting, and I would love to try them out, but for this project, I will have to settle for a more tried-and-tested approach.\\n\\nThe more tried and tested approach that I settled with is RAG. It is an incredibly popular technique for allowing LLMs to make use of information that is too big to fit in their context windows. This technique is known to perform very well, is incredibly well supported by LLM frameworks like LangChain and llamaindex, and works well in resource-constrained environments like on my laptop. Given all this, RAG was an obvious choice.\\n\\nOptimising Models for Limited Memory Environments\\n\\nNext, I decided to investigate what kinds of optimisation strategies were available to use to try to fit bigger models into my M1 chip, as bigger LLMs typically perform better (I know, a groundbreaking revelation). To optimise the LLMs that I use, I considered four different techniques:\\n\\nQuantisation\\nModel pruning\\nModel/knowledge distillation\\nAirLLM\\nQuantisation is the most common method for making ML models smaller (and therefore faster and more capable of fitting into smaller spaces). It’s well known for improving speed and memory usage with little loss in accuracy in return, which makes it very popular for production-level AI. Quantising a model would require being able to fit it into your GPU, but I’m trying to quantise a model so that it can fit into my GPU, so without additional computing power, it’s a bit of a chicken and egg problem. Luckily, because this is such a popular technique, there are many quantised versions of large, high-performance LLMs available on HuggingFace that I can use, so there is no need to do this myself.\\n\\nModel pruning is a less common method for reducing model sizes, but it is not a technique that one should overlook. This is a technique that can be combined with quantisation (or used on its own) to further reduce models at the expense of accuracy, but I do not plan to apply it myself due to its complexity and the fact that quantisation has the same effect. There are pruned models available on HuggingFace, but they don’t typically perform as well as equivalently sized quantised models, so I do not plan to use any unless they have particularly good evaluation results on a common LLM benchmark.\\n\\nModel/knowledge distillation is another size reduction technique that I considered. Unlike the previous techniques, model distillation can actually improve accuracy in domain-specific tasks while making a smaller model. As with quantisation and model pruning, I will use pre-distiled models, but I will not distil any models myself due to the computing power it requires (which admittedly is far less than training a model from scratch) and the complexity it would add to the project.\\n\\nThe final optimisation technique that I considered, AirLLM, is quite different from the others in that instead of optimising the model weights, it optimises the model inference. Typically, LLMs are loaded onto the GPU in their entirety, requiring a lot of VRAM to run the larger, better-performing models. AirLLM is an open-source library that tackles this problem by using layered inference, an inference technique that involves loading layers individually when they are needed instead of all at once. This allows larger models to fit into smaller memory spaces without degrading performance. This method definitely has a high potential for accuracy, but I decided not to use it as I am concerned about compatibility and reliability issues as it is a new tool and the GitHub repo has been developed by a single person, so support for it is likely to be limited. Additionally, my M1 chip only has 8 GB of memory shared between the CPU and GPU, which is excellent for reducing data loading overhead costs, but it means that larger models that require AirLLM will be loaded directly from the SSD, so I am concerned that the model layer loading and unloading will become a massive bottleneck when doing inference on larger models. I will reconsider this option if I find that the models that can run on my MacBook do not have satisfactory accuracy.\\n\\nWhat Models Even Run on My MacBook?\\n\\nAfter getting an idea of what kinds of optimisation techniques were available, I decided to conduct some tests to find out what LLMs would actually run on my MacBook. You could argue that since I am only building a prototype right now, I only need to find one LLM that performs well on my MacBook, but I decided to find five models instead to give me an idea of what kinds of models I will be able to use. In particular, I wanted to know how big the models I could run were and what precision the weights would likely be.\\n\\nI tested models that I had heard were good or showed decent results on the Hugging Face H4 Open LLM Leaderboard. I found LM Studio incredibly useful for testing out LLMs without having to write any code, which saved me a lot of time. Below are the five suitable models that I found that could run on my MacBook and were fast enough to satisfy me:\\n\\ntinyllama-1.1b-chat-v1.0 Q6_K\\nPhi 3 Q4_K_M\\nbartowski/dolphin-2.8-experiment26-7b-GGUF Q3_K_L\\nmgonzs13/Mistroll-7B-v2.2-GGU\\nQuantFactory/Meta-Llama-3-8B-Instruct Q3_K_M\\nThese models range from 1.17 GB up to 4.02 GB in size. I chose not to use any models that were any larger than 4 GB, as with only 8 GB of memory available, I expect that models that are any bigger would seriously impact the other applications that the user (i.e., me) is running on their device.\\n\\nI will likely test out more models than this while testing out different configurations for the tool, but for the prototype, this is enough.\\n\\nA Model Without a Framework is Like a Car Engine Without a Chassis\\n\\nTo run my models, I could have written a framework for loading, unloading, and executing the models, passing context and queries to the models, and integrating the vector DB (more on that later) with the inference model from scratch, but I didn’t because I’m not insane and I am not trying to learn how to make ML frameworks. A lot of university students (myself included) are conditioned to try to build things from scratch for fear of plagiarism and because they are used to building things from scratch as a learning exercise (a very effective one in my opinion), so it’s difficult to unlearn the DIY mindset, but it’s simply a lot quicker and a lot more reliable to use libraries than to reinvent the wheel. Saying that, I decided to use a relatively simple tech stack.\\n\\nPython was an obvious choice for me, given that I have a lot of experience with it and that it has an abundance of support for machine learning applications. I decided to use LangChain to orchestrate my RAG process from the vector DB to the inference, as it is a flexible tool for composing NLP pipelines. It is very popular and reliable, and it includes a lot of tools that make developing NLP applications easier. I considered using LlamaIndex as it is built more specifically for RAG applications, but LangChain is more general-purpose, which I expect will make it more extensible for times when I might want to add more features in the future. Additionally, I am more likely to use LangChain again for other applications in the future, so the experience will be more useful. I also considered using LitGPT, but I had some issues getting it to work with the M1 chip’s Metal Performance Shaders (MPS), so I decided not to use it for fear of incompatibility. LitGPT is also intended more for training and fine-tuning LLMs, so it is likely not the best tool for simply deploying them in an application.\\n\\nTo run inference on my models, I will need another library to actually execute the model. As I am using a range of pre-trained models, I will mainly use HuggingFace’s transformers library and the Python bindings for llama.cpp library to load and execute models, as these provide simple interfaces for inference, and I don’t need the additional control that deep learning frameworks like TensorFlow, PyTorch, and JAX provide as I’m using pre-trained models. As I mentioned earlier, AirLLM is still on the table if I need better performance, but I will find out while evaluating models whether this is necessary.\\n\\nMagnitude + Direction DBs\\n\\nSince I was using RAG, I needed a vector DB. Deciding which one to use was the final step of the research and also the most difficult one, as vector DBs are the technology that I am least familiar with. For the vector DB, my main requirements were simple: it needed to be lightweight, locally runnable on a laptop, fast, and compatible with my MacBook. Lightweight and locally runnable sound like similar things, but I mean different things by each phrase. The locally runnable one is quite self-explanatory, but by lightweight, I mean quite minimal computation requirements that don’t add features like heavy amounts of redundancy and heavy caching, which are useful for large-scale systems, but will simply drain resources in my application that is designed to run alongside an LLM and the user’s other applications on 8 GB of memory shared between the CPU and GPU.\\n\\nI considered sixteen different vector DBs, but there were three different solutions that stood out to me for my use case: Chroma, Qdrant, and Vespa. These were all lightweight vector DBs that fit all of my requirements, but I decided to pick Chroma out of the three of them because it has a very batteries included approach, which is very useful for getting an application working quickly.\\n\\nBelow is an overview of my planned tech stack. I did not include any UI, as there are many options for this, and I plan to decide on the UI when building the final product.\\n\\nThe tech stack and data flow that I plan to use for STICI-note.\\nThe tech stack and data flow that I plan to use for STICI-note.\\n\\nMaking the Prototype (A.K.A. the Easy Part)\\n\\nThe final step was to actually build the prototype/proof of concept (PoC) to find out whether this idea can actually be implemented using the tech stack that I have picked. I decided to implement it using the smallest of the 5 models that I had found earlier, tinyllama-1.1b-chat-v1.0 Q6_K, to minimise the chances of having any memory issues.\\n\\nYou can find my prototype code here.\\n\\nFor the prototype, I decided to make very basic configurations for everything to develop it quickly, and I tested it by asking a question about Grace Hopper, one of the most influential programmers to have ever lived, in two versions of the prototype: one with Grace Hopper’s Wikipedia page in its vector DB and one with Alan Turing’s Wikipedia page in its vector DB.\\n\\nThe question was:\\n\\n“What programming language was Grace Hopper known for creating?”\\n\\nAs expected, the version with Alan Turing’s Wikipedia page failed miserably, hallucinating its entire response:\\n\\nGrace Hopper was a computer scientist who is best known for her work on developing an early version of BASIC, which was one of the first low-level programming languages designed for mainframe computers. Hopper worked at Digital Equipment Corporation (DEC) in the mid-1960s and was responsible for creating several key BASIC versions that were used by DEC\\'s mainframe computers. In 1973, she left DEC to work on a new low-level programming language called PASCAL, which became one of the most popular programming languages of its time. However, her contributions to BASIC are still remembered today as it was one of the first widely used and easy-to-understand programming languages for mainframe computers.\\n\\nThis behaviour of hallucinating will need to be fixed in the final product, likely through prompt engineering.\\n\\nAs expected, the version with Grace Hopper’s Wikipedia page gave a much more accurate response but it unexpectedly contained far more spelling errors:\\n\\nGrace Hopper is credited with creating the first computer programming languaige called FLOW-MATIC. This language was developed in 1954 and was based on English words. Hopper\\'s work on this languaige led to a machine-indepenent programming language, COBOL, which used English words instead of symbols for input and output. Her compiler, A-0, also utilized the FLOW-MATIIC language as its basis for compiling programs into executable code. In 1952, Hopper published her first paper on the topic of compiler design, \"The Compiler\" in the IEEE Transactions on Computers, in which she discussed her work with A-0 and other programming languaiges.\\n\\nUnfortunately, this response was not free from hallucinations either. Hopper’s A-0 compiler did not use the FLOW-MATIC programming language, and she never published a paper called “The Compiler” in IEEE Transactions on Computers (a real journal that is not mentioned in the Wikipedia page). It looks like hallucinations are likely to be a major issue for this tool, but that is a problem I will solve when refining the AI.\\n\\nOn the bright side, inference was ~120 tokens/second, so at least this model will output words much faster than I can read them.\\n\\nConclusion\\n\\nIn this blog, I built a locally run prototype for my chatbot for querying unstructured text documents. It doesn’t have a UI, and it hallucinates a lot, but it is nonetheless capable of querying unstructured text.\\n\\nIt’s such a shame that after I had done the research and written all of the code, while I was writing this blog, I read about llmware, a very promising Python framework for building RAG pipelines with small models (sound familiar?). It was even chosen for GitHub Accelerator 2024, a competition for open-source projects on GitHub where chosen projects are given funding, mentorship, and access to resources to help them grow their project. Since I had already built the prototype in LangChain, it didn’t make much sense to tear it down and rebuild it in a fancy new framework that wasn’t as tried-and-tested. I’d love to try the framework out one day if I build another RAG application after this one.\\n\\nIn the next part of the STICI-note blog series, I will be building an evaluation suite to test and compare different inference models and vector DB configurations, so stay tuned and follow me on LinkedIn to be notified when it comes out!', metadata={'source': None, 'seq_num': 3})]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = get_docs_rag(\"What kind of model is the bling-phi-3 model\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning\\n2023-01-30 by Tim Dettmers 1,664 Comments\\n\\nDeep learning is a field with intense computational requirements, and your choice of GPU will fundamentally determine your deep learning experience. But what features are important if you want to buy a new GPU? GPU RAM, cores, tensor cores, caches? How to make a cost-efficient choice? This blog post will delve into these questions, tackle common misconceptions, give you an intuitive understanding of how to think about GPUs, and will lend you advice, which will help you to make a choice that is right for you.\\n\\nThis blog post is designed to give you different levels of understanding of GPUs and the new Ampere series GPUs from NVIDIA. You have the choice: (1) If you are not interested in the details of how GPUs work, what makes a GPU fast compared to a CPU, and what is unique about the new NVIDIA RTX 40 Ampere series, you can skip right to the performance and performance per dollar charts and the recommendation section. The cost/performance numbers form the core of the blog post and the content surrounding it explains the details of what makes up GPU performance.\\n\\n(2) If you worry about specific questions, I have answered and addressed the most common questions and misconceptions in the later part of the blog post.\\n\\n(3) If you want to get an in-depth understanding of how GPUs, caches, and Tensor Cores work, the best is to read the blog post from start to finish. You might want to skip a section or two based on your understanding of the presented topics.\\n\\nContents  hide\\nOverview\\nHow do GPUs work?\\nThe Most Important GPU Specs for Deep Learning Processing Speed\\nTensor Cores\\nMatrix multiplication without Tensor Cores\\nMatrix multiplication with Tensor Cores\\nMatrix multiplication with Tensor Cores and Asynchronous copies (RTX 30/RTX 40) and TMA (H100)\\nMemory Bandwidth\\nL2 Cache / Shared Memory / L1 Cache / Registers\\nEstimating Ada / Hopper Deep Learning Performance\\nPractical Ada / Hopper Speed Estimates\\nPossible Biases in Estimates\\nAdvantages and Problems for RTX40 and RTX 30 Series\\nSparse Network Training\\nLow-precision Computation\\nFan Designs and GPUs Temperature Issues\\n3-slot Design and Power Issues\\nPower Limiting: An Elegant Solution to Solve the Power Problem?\\nRTX 4090s and Melting Power Connectors: How to Prevent Problems\\n8-bit Float Support in H100 and RTX 40 series GPUs\\nRaw Performance Ranking of GPUs\\nGPU Deep Learning Performance per Dollar\\nGPU Recommendations\\nIs it better to wait for future GPUs for an upgrade? The future of GPUs.\\nQuestion & Answers & Misconceptions\\nDo I need PCIe 4.0 or PCIe 5.0?\\nDo I need 8x/16x PCIe lanes?\\nHow do I fit 4x RTX 4090 or 3090 if they take up 3 PCIe slots each?\\nHow do I cool 4x RTX 3090 or 4x RTX 3080?\\nCan I use multiple GPUs of different GPU types?\\nWhat is NVLink, and is it useful?\\nI do not have enough money, even for the cheapest GPUs you recommend. What can I do?\\nWhat is the carbon footprint of GPUs? How can I use GPUs without polluting the environment?\\nWhat do I need to parallelize across two machines?\\nIs the sparse matrix multiplication features suitable for sparse matrices in general?\\nDo I need an Intel CPU to power a multi-GPU setup?\\nDoes computer case design matter for cooling?\\nWill AMD GPUs + ROCm ever catch up with NVIDIA GPUs + CUDA?\\nWhen is it better to use the cloud vs a dedicated GPU desktop/server?\\nVersion History\\nAcknowledgments\\nRelated\\nRelated Posts\\nOverview\\nThis blog post is structured in the following way. First, I will explain what makes a GPU fast. I will discuss CPUs vs GPUs, Tensor Cores, memory bandwidth, and the memory hierarchy of GPUs and how these relate to deep learning performance. These explanations might help you get a more intuitive sense of what to look for in a GPU. I discuss the unique features of the new NVIDIA RTX 40 Ampere GPU series that are worth considering if you buy a GPU. From there, I make GPU recommendations for different scenarios. After that follows a Q&A section of common questions posed to me in Twitter threads; in that section, I will also address common misconceptions and some miscellaneous issues, such as cloud vs desktop, cooling, AMD vs NVIDIA, and others.\\n\\nHow do GPUs work?\\nIf you use GPUs frequently, it is useful to understand how they work. This knowledge will help you to undstand cases where are GPUs fast or slow. In turn, you might be able to understand better why you need a GPU in the first place and how other future hardware options might be able to compete. You can skip this section if you just want the useful performance numbers and arguments to help you decide which GPU to buy. The best high-level explanation for the question of how GPUs work is my following Quora answer:\\n\\nRead Tim Dettmers‘ answer to Why are GPUs well-suited to deep learning? on Quora\\nThis is a high-level explanation that explains quite well why GPUs are better than CPUs for deep learning. If we look at the details, we can understand what makes one GPU better than another.\\n\\nThe Most Important GPU Specs for Deep Learning Processing Speed\\nThis section can help you build a more intuitive understanding of how to think about deep learning performance. This understanding will help you to evaluate future GPUs by yourself. This section is sorted by the importance of each component. Tensor Cores are most important, followed by memory bandwidth of a GPU, the cache hierachy, and only then FLOPS of a GPU.\\n\\nTensor Cores\\nTensor Cores are tiny cores that perform very efficient matrix multiplication. Since the most expensive part of any deep neural network is matrix multiplication Tensor Cores are very useful. In fast, they are so powerful, that I do not recommend any GPUs that do not have Tensor Cores.\\n\\nIt is helpful to understand how they work to appreciate the importance of these computational units specialized for matrix multiplication. Here I will show you a simple example of A*B=C matrix multiplication, where all matrices have a size of 32×32, what a computational pattern looks like with and without Tensor Cores. This is a simplified example, and not the exact way how a high performing matrix multiplication kernel would be written, but it has all the basics. A CUDA programmer would take this as a first “draft” and then optimize it step-by-step with concepts like double buffering, register optimization, occupancy optimization, instruction-level parallelism, and many others, which I will not discuss at this point.\\n\\nTo understand this example fully, you have to understand the concepts of cycles. If a processor runs at 1GHz, it can do 10^9 cycles per second. Each cycle represents an opportunity for computation. However, most of the time, operations take longer than one cycle. Thus we essentially have a queue where the next operations needs to wait for the next operation to finish. This is also called the latency of the operation.\\n\\nHere are some important latency cycle timings for operations. These times can change from GPU generation to GPU generation. These numbers are for Ampere GPUs, which have relatively slow caches.\\n\\nGlobal memory access (up to 80GB): ~380 cycles\\nL2 cache: ~200 cycles\\nL1 cache or Shared memory access (up to 128 kb per Streaming Multiprocessor): ~34 cycles\\nFused multiplication and addition, a*b+c (FFMA): 4 cycles\\nTensor Core matrix multiply: 1 cycle\\nEach operation is always performed by a pack of 32 threads. This pack is termed a warp of threads. Warps usually operate in a synchronous pattern — threads within a warp have to wait for each other. All memory operations on the GPU are optimized for warps. For example, loading from global memory happens at a granularity of 32*4 bytes, exactly 32 floats, exactly one float for each thread in a warp. We can have up to 32 warps = 1024 threads in a streaming multiprocessor (SM), the GPU-equivalent of a CPU core. The resources of an SM are divided up among all active warps. This means that sometimes we want to run fewer warps to have more registers/shared memory/Tensor Core resources per warp.\\n\\nFor both of the following examples, we assume we have the same computational resources. For this small example of a 32×32 matrix multiply, we use 8 SMs (about 10% of an RTX 3090) and 8 warps per SM.\\n\\nTo understand how the cycle latencies play together with resources like threads per SM and shared memory per SM, we now look at examples of matrix multiplication. While the following example roughly follows the sequence of computational steps of matrix multiplication for both with and without Tensor Cores, please note that these are very simplified examples. Real cases of matrix multiplication involve much larger shared memory tiles and slightly different computational patterns.\\n\\nMatrix multiplication without Tensor Cores\\nIf we want to do an A*B=C matrix multiply, where each matrix is of size 32×32, then we want to load memory that we repeatedly access into shared memory because its latency is about five times lower (200 cycles vs 34 cycles). A memory block in shared memory is often referred to as a memory tile or just a tile. Loading two 32×32 floats into a shared memory tile can happen in parallel by using 2*32 warps. We have 8 SMs with 8 warps each, so due to parallelization, we only need to do a single sequential load from global to shared memory, which takes 200 cycles.\\n\\nTo do the matrix multiplication, we now need to load a vector of 32 numbers from shared memory A and shared memory B and perform a fused multiply-and-accumulate (FFMA). Then store the outputs in registers C. We divide the work so that each SM does 8x dot products (32×32) to compute 8 outputs of C. Why this is exactly 8 (4 in older algorithms) is very technical. I recommend Scott Gray’s blog post on matrix multiplication to understand this. This means we have 8x shared memory accesses at the cost of 34 cycles each and 8 FFMA operations (32 in parallel), which cost 4 cycles each. In total, we thus have a cost of:\\n\\n200 cycles (global memory) + 8*34 cycles (shared memory) + 8*4 cycles (FFMA) = 504 cycles\\n\\nLet’s look at the cycle cost of using Tensor Cores.\\n\\nMatrix multiplication with Tensor Cores\\nWith Tensor Cores, we can perform a 4×4 matrix multiplication in one cycle. To do that, we first need to get memory into the Tensor Core. Similarly to the above, we need to read from global memory (200 cycles) and store in shared memory. To do a 32×32 matrix multiply, we need to do 8×8=64 Tensor Cores operations. A single SM has 8 Tensor Cores. So with 8 SMs, we have 64 Tensor Cores — just the number that we need! We can transfer the data from shared memory to the Tensor Cores with 1 memory transfers (34 cycles) and then do those 64 parallel Tensor Core operations (1 cycle). This means the total cost for Tensor Cores matrix multiplication, in this case, is:\\n\\n200 cycles (global memory) + 34 cycles (shared memory) + 1 cycle (Tensor Core) = 235 cycles.\\n\\nThus we reduce the matrix multiplication cost significantly from 504 cycles to 235 cycles via Tensor Cores. In this simplified case, the Tensor Cores reduced the cost of both shared memory access and FFMA operations.\\n\\nThis example is simplified, for example, usually each thread needs to calculate which memory to read and write to as you transfer data from global memory to shared memory. With the new Hooper (H100) architectures we additionally have the Tensor Memory Accelerator (TMA) compute these indices in hardware and thus help each thread to focus on more computation rather than computing indices.\\n\\nMatrix multiplication with Tensor Cores and Asynchronous copies (RTX 30/RTX 40) and TMA (H100)\\nThe RTX 30 Ampere and RTX 40 Ada series GPUs additionally have support to perform asynchronous transfers between global and shared memory. The H100 Hopper GPU extends this further by introducing the Tensor Memory Accelerator (TMA) unit. the TMA unit combines asynchronous copies and index calculation for read and writes simultaneously — so each thread no longer needs to calculate which is the next element to read and each thread can focus on doing more matrix multiplication calculations. This looks as follows.\\n\\nThe TMA unit fetches memory from global to shared memory (200 cycles). Once the data arrives, the TMA unit fetches the next block of data asynchronously from global memory. While this is happening, the threads load data from shared memory and perform the matrix multiplication via the tensor core. Once the threads are finished they wait for the TMA unit to finish the next data transfer, and the sequence repeats.\\n\\nAs such, due to the asynchronous nature, the second global memory read by the TMA unit is already progressing as the threads process the current shared memory tile. This means, the second read takes only 200 – 34 – 1 = 165 cycles.\\n\\nSince we do many reads, only the first memory access will be slow and all other memory accesses will be partially overlapped with the TMA unit. Thus on average, we reduce the time by 35 cycles.\\n\\n165 cycles (wait for async copy to finish) + 34 cycles (shared memory) + 1 cycle (Tensor Core) = 200 cycles.\\n\\nWhich accelerates the matrix multiplication by another 15%.\\n\\nFrom these examples, it becomes clear why the next attribute, memory bandwidth, is so crucial for Tensor-Core-equipped GPUs. Since global memory is the by far the largest cycle cost for matrix multiplication with Tensor Cores, we would even have faster GPUs if the global memory latency could be reduced. We can do this by either increasing the clock frequency of the memory (more cycles per second, but also more heat and higher energy requirements) or by increasing the number of elements that can be transferred at any one time (bus width).\\n\\nMemory Bandwidth\\nFrom the previous section, we have seen that Tensor Cores are very fast. So fast, in fact, that they are idle most of the time as they are waiting for memory to arrive from global memory. For example, during GPT-3-sized training, which uses huge matrices — the larger, the better for Tensor Cores — we have a Tensor Core TFLOPS utilization of about 45-65%, meaning that even for the large neural networks about 50% of the time, Tensor Cores are idle.\\n\\nThis means that when comparing two GPUs with Tensor Cores, one of the single best indicators for each GPU’s performance is their memory bandwidth. For example, The A100 GPU has 1,555 GB/s memory bandwidth vs the 900 GB/s of the V100. As such, a basic estimate of speedup of an A100 vs V100 is 1555/900 = 1.73x.\\n\\nL2 Cache / Shared Memory / L1 Cache / Registers\\nSince memory transfers to the Tensor Cores are the limiting factor in performance, we are looking for other GPU attributes that enable faster memory transfer to Tensor Cores. L2 cache, shared memory, L1 cache, and amount of registers used are all related. To understand how a memory hierarchy enables faster memory transfers, it helps to understand how matrix multiplication is performed on a GPU.\\n\\nTo perform matrix multiplication, we exploit the memory hierarchy of a GPU that goes from slow global memory, to faster L2 memory, to fast local shared memory, to lightning-fast registers. However, the faster the memory, the smaller it is.\\n\\nWhile logically, L2 and L1 memory are the same, L2 cache is larger and thus the average physical distance that need to be traversed to retrieve a cache line is larger. You can see the L1 and L2 caches as organized warehouses where you want to retrieve an item. You know where the item is, but to go there takes on average much longer for the larger warehouse. This is the essential difference between L1 and L2 caches. Large = slow, small = fast.\\n\\nFor matrix multiplication we can use this hierarchical separate into smaller and smaller and thus faster and faster chunks of memory to perform very fast matrix multiplications. For that, we need to chunk the big matrix multiplication into smaller sub-matrix multiplications. These chunks are called memory tiles, or often for short just tiles.\\n\\nWe perform matrix multiplication across these smaller tiles in local shared memory that is fast and close to the streaming multiprocessor (SM) — the equivalent of a CPU core. With Tensor Cores, we go a step further: We take each tile and load a part of these tiles into Tensor Cores which is directly addressed by registers. A matrix memory tile in L2 cache is 3-5x faster than global GPU memory (GPU RAM), shared memory is ~7-10x faster than the global GPU memory, whereas the Tensor Cores’ registers are ~200x faster than the global GPU memory.\\n\\nHaving larger tiles means we can reuse more memory. I wrote about this in detail in my TPU vs GPU blog post. In fact, you can see TPUs as having very, very, large tiles for each Tensor Core. As such, TPUs can reuse much more memory with each transfer from global memory, which makes them a little bit more efficient at matrix multiplications than GPUs.\\n\\nEach tile size is determined by how much memory we have per streaming multiprocessor (SM) and how much we L2 cache we have across all SMs. We have the following shared memory sizes on the following architectures:\\n\\nVolta (Titan V): 128kb shared memory / 6 MB L2\\nTuring (RTX 20s series): 96 kb shared memory / 5.5 MB L2\\nAmpere (RTX 30s series): 128 kb shared memory / 6 MB L2\\nAda (RTX 40s series): 128 kb shared memory / 72 MB L2\\nWe see that Ada has a much larger L2 cache allowing for larger tile sizes, which reduces global memory access. For example, for BERT large during training, the input and weight matrix of any matrix multiplication fit neatly into the L2 cache of Ada (but not other Us). As such, data needs to be loaded from global memory only once and then data is available throught the L2 cache, making matrix multiplication about 1.5 – 2.0x faster for this architecture for Ada. For larger models the speedups are lower during training but certain sweetspots exist which may make certain models much faster. Inference, with a batch size larger than 8 can also benefit immensely from the larger L2 caches.\\n\\nEstimating Ada / Hopper Deep Learning Performance\\nThis section is for those who want to understand the more technical details of how I derive the performance estimates for Ampere GPUs. If you do not care about these technical aspects, it is safe to skip this section.\\n\\nPractical Ada / Hopper Speed Estimates\\nSuppose we have an estimate for one GPU of a GPU-architecture like Hopper, Ada, Ampere, Turing, or Volta. It is easy to extrapolate these results to other GPUs from the same architecture/series. Luckily, NVIDIA already benchmarked the A100 vs V100 vs H100 across a wide range of computer vision and natural language understanding tasks. Unfortunately, NVIDIA made sure that these numbers are not directly comparable by using different batch sizes and the number of GPUs whenever possible to favor results for the H100 GPU. So in a sense, the benchmark numbers are partially honest, partially marketing numbers. In general, you could argue that using larger batch sizes is fair, as the H100/A100 GPU has more memory. Still, to compare GPU architectures, we should evaluate unbiased memory performance with the same batch size.\\n\\nTo get an unbiased estimate, we can scale the data center GPU results in two ways: (1) account for the differences in batch size, (2) account for the differences in using 1 vs 8 GPUs. We are lucky that we can find such an estimate for both biases in the data that NVIDIA provides.\\n\\nDoubling the batch size increases throughput in terms of images/s (CNNs) by 13.6%. I benchmarked the same problem for transformers on my RTX Titan and found, surprisingly, the very same result: 13.5% — it appears that this is a robust estimate.\\n\\nAs we parallelize networks across more and more GPUs, we lose performance due to some networking overhead. The A100 8x GPU system has better networking (NVLink 3.0) than the V100 8x GPU system (NVLink 2.0) — this is another confounding factor. Looking directly at the data from NVIDIA, we can find that for CNNs, a system with 8x A100 has a 5% lower overhead than a system of 8x V100. This means if going from 1x A100 to 8x A100 gives you a speedup of, say, 7.00x, then going from 1x V100 to 8x V100 only gives you a speedup of 6.67x.  For transformers, the figure is 7%.\\n\\nUsing these figures, we can estimate the speedup for a few specific deep learning architectures from the direct data that NVIDIA provides. The Tesla A100 offers the following speedup over the Tesla V100:\\n\\nSE-ResNeXt101: 1.43x\\nMasked-R-CNN: 1.47x\\nTransformer (12 layer, Machine Translation, WMT14 en-de): 1.70x\\nThus, the figures are a bit lower than the theoretical estimate for computer vision. This might be due to smaller tensor dimensions, overhead from operations that are needed to prepare the matrix multiplication like img2col or Fast Fourier Transform (FFT), or operations that cannot saturate the GPU (final layers are often relatively small). It could also be artifacts of the specific architectures (grouped convolution).\\n\\nThe practical transformer estimate is very close to the theoretical estimate. This is probably because algorithms for huge matrices are very straightforward. I will use these practical estimates to calculate the cost efficiency of GPUs.\\n\\nPossible Biases in Estimates\\nThe estimates above are for H100, A100 , and V100 GPUs. In the past, NVIDIA sneaked unannounced performance degradations into the “gaming” RTX GPUs: (1) Decreased Tensor Core utilization, (2) gaming fans for cooling, (3) disabled peer-to-peer GPU transfers. It might be possible that there are unannounced performance degradations in the RTX 40 series compared to the full Hopper H100.\\n\\nAs of now, one of these degradations was found for Ampere GPUs: Tensor Core performance was decreased so that RTX 30 series GPUs are not as good as Quadro cards for deep learning purposes. This was also done for the RTX 20 series, so it is nothing new, but this time it was also done for the Titan equivalent card, the RTX 3090. The RTX Titan did not have performance degradation enabled.\\n\\nCurrently, no degradation for Ada GPUs are known, but I update this post with news on this and let my followers on twitter know.\\n\\nAdvantages and Problems for RTX40 and RTX 30 Series\\nThe new NVIDIA Ampere RTX 30 series has additional benefits over the NVIDIA Turing RTX 20 series, such as sparse network training and inference. Other features, such as the new data types, should be seen more as an ease-of-use-feature as they provide the same performance boost as Turing does but without any extra programming required.\\n\\nThe Ada RTX 40 series has even further advances like 8-bit Float (FP8) tensor cores. The RTX 40 series also has similar power and temperature issues compared to the RTX 30. The issue of melting power connector cables in the RTX 40 can be easily prevented by connecting the power cable correctly.\\n\\nSparse Network Training\\nAmpere allows for fine-grained structure automatic sparse matrix multiplication at dense speeds. How does this work? Take a weight matrix and slice it into pieces of 4 elements. Now imagine 2 elements of these 4 to be zero. Figure 1 shows how this could look like.\\n\\nFigure 1: Structure supported by the sparse matrix multiplication feature in Ampere GPUs. The figure is taken from Jeff Pool's GTC 2020 presentation on  Accelerating Sparsity in the NVIDIA Ampere Architecture by the courtesy of NVIDIA.\\nFigure 1: Structure supported by the sparse matrix multiplication feature in Ampere GPUs. The figure is taken from Jeff Pool’s GTC 2020 presentation on Accelerating Sparsity in the NVIDIA Ampere Architecture by the courtesy of NVIDIA.\\nWhen you multiply this sparse weight matrix with some dense inputs, the sparse matrix tensor core feature in Ampere automatically compresses the sparse matrix to a dense representation that is half the size as can be seen in Figure 2. After this compression, the densely compressed matrix tile is fed into the tensor core which computes a matrix multiplication of twice the usual size. This effectively yields a 2x speedup since the bandwidth requirements during matrix multiplication from shared memory are halved.\\n\\nFigure 2: The sparse matrix is compressed to a dense representation before the matrix multiplication is performed.\\nFigure 2: The sparse matrix is compressed to a dense representation before the matrix multiplication is performed. The figure is taken from Jeff Pool’s GTC 2020 presentation on Accelerating Sparsity in the NVIDIA Ampere Architecture by the courtesy of NVIDIA.\\nI was working on sparse network training in my research and I also wrote a blog post about sparse training. One criticism of my work was that “You reduce the FLOPS required for the network, but it does not yield speedups because GPUs cannot do fast sparse matrix multiplication.” Well, with the addition of the sparse matrix multiplication feature for Tensor Cores, my algorithm, or other sparse training algorithms, now actually provide speedups of up to 2x during training.\\n\\nFigure 3: The sparse training algorithm that I developed has three stages: (1) Determine the importance of each layer. (2) Remove the smallest, unimportant weights. (3) Grow new weights proportional to the importance of each layer. Read more about my work in my sparse training blog post.\\nFigure 3: The sparse training algorithm that I developed has three stages: (1) Determine the importance of each layer. (2) Remove the smallest, unimportant weights. (3) Grow new weights proportional to the importance of each layer. Read more about my work in my sparse training blog post.\\nWhile this feature is still experimental and training sparse networks are not commonplace yet, having this feature on your GPU means you are ready for the future of sparse training.\\n\\nLow-precision Computation\\nIn my work, I’ve previously shown that new data types can improve stability during low-precision backpropagation.\\n\\nFigure 4: Low-precision deep learning 8-bit datatypes that I developed. Deep learning training benefits from highly specialized data types. My dynamic tree datatype uses a dynamic bit that indicates the beginning of a binary bisection tree that quantized the range [0, 0.9] while all previous bits are used for the exponent. This allows to dynamically represent numbers that are both large and small with high precision.\\nFigure 4: Low-precision deep learning 8-bit datatypes that I developed. Deep learning training benefits from highly specialized data types. My dynamic tree datatype uses a dynamic bit that indicates the beginning of a binary bisection tree that quantized the range [0, 0.9] while all previous bits are used for the exponent. This allows to dynamically represent numbers that are both large and small with high precision.\\nCurrently, if you want to have stable backpropagation with 16-bit floating-point numbers (FP16), the big problem is that ordinary FP16 data types only support numbers in the range [-65,504, 65,504]. If your gradient slips past this range, your gradients explode into NaN values. To prevent this during FP16 training, we usually perform loss scaling where you multiply the loss by a small number before backpropagating to prevent this gradient explosion.\\n\\nThe BrainFloat 16 format (BF16) uses more bits for the exponent such that the range of possible numbers is the same as for FP32: [-3*10^38, 3*10^38]. BF16 has less precision, that is significant digits, but gradient precision is not that important for learning. So what BF16 does is that you no longer need to do any loss scaling or worry about the gradient blowing up quickly. As such, we should see an increase in training stability by using the BF16 format as a slight loss of precision.\\n\\nWhat this means for you: With BF16 precision, training might be more stable than with FP16 precision while providing the same speedups. With 32-bit TensorFloat (TF32) precision, you get near FP32 stability while giving the speedups close to FP16. The good thing is, to use these data types, you can just replace FP32 with TF32 and FP16 with BF16 — no code changes required!\\n\\nOverall, though, these new data types can be seen as lazy data types in the sense that you could have gotten all the benefits with the old data types with some additional programming efforts (proper loss scaling, initialization, normalization, using Apex). As such, these data types do not provide speedups but rather improve ease of use of low precision for training.\\n\\nFan Designs and GPUs Temperature Issues\\nWhile the new fan design of the RTX 30 series performs very well to cool the GPU, different fan designs of non-founders edition GPUs might be more problematic. If your GPU heats up beyond 80C, it will throttle itself and slow down its computational speed / power. This overheating can happen in particular if you stack multiple GPUs next to each other. A solution to this is to use PCIe extenders to create space between GPUs.\\n\\nSpreading GPUs with PCIe extenders is very effective for cooling, and other fellow PhD students at the University of Washington and I use this setup with great success. It does not look pretty, but it keeps your GPUs cool! This has been running with no problems at all for 4 years now. It can also help if you do not have enough space to fit all GPUs in the PCIe slots. For example, if you can find the space within a desktop computer case, it might be possible to buy standard 3-slot-width RTX 4090 and spread them with PCIe extenders within the case. With this, you might solve both the space issue and cooling issue for a 4x RTX 4090 setup with a single simple solution.\\n\\nFigure 5: 4x GPUs with PCIe extenders. It looks like a mess, but it is very effective for cooling. I used this rig for 2 years and cooling is excellent despite problematic RTX 2080 Ti Founders Edition GPUs.\\nFigure 5: 4x GPUs with PCIe extenders. It looks like a mess, but it is very effective for cooling. I used this rig for 4 years and cooling is excellent despite problematic RTX 2080 Ti Founders Edition GPUs.\\n3-slot Design and Power Issues\\nThe RTX 3090 and RTX 4090 are 3-slot GPUs, so one will not be able to use it in a 4x setup with the default fan design from NVIDIA. This is kind of justified because it runs at over 350W TDP, and it will be difficult to cool in a multi-GPU 2-slot setting. The RTX 3080 is only slightly better at 320W TDP, and cooling a 4x RTX 3080 setup will also be very difficult.\\n\\nIt is also difficult to power a 4x 350W = 1400W or 4x 450W = 1800W system in the 4x RTX 3090 or 4x RTX 4090 case. Power supply units (PSUs) of 1600W are readily available, but having only 200W to power the CPU and motherboard can be too tight. The components’ maximum power is only used if the components are fully utilized, and in deep learning, the CPU is usually only under weak load. With that, a 1600W PSU might work quite well with a 4x RTX 3080 build, but for a 4x RTX 3090 build, it is better to look for high wattage PSUs (+1700W). Some of my followers have had great success with cryptomining PSUs — have a look in the comment section for more info about that. Otherwise, it is important to note that not all outlets support PSUs above 1600W, especially in the US. This is the reason why in the US, there are currently few standard desktop PSUs above 1600W on the market. If you get a server or cryptomining PSUs, beware of the form factor — make sure it fits into your computer case.\\n\\nPower Limiting: An Elegant Solution to Solve the Power Problem?\\nIt is possible to set a power limit on your GPUs. So you would be able to programmatically set the power limit of an RTX 3090 to 300W instead of their standard 350W. In a 4x GPU system, that is a saving of 200W, which might just be enough to build a 4x RTX 3090 system with a 1600W PSU feasible. It also helps to keep the GPUs cool. So setting a power limit can solve the two major problems of a 4x RTX 3080 or 4x RTX 3090 setups, cooling, and power, at the same time. For a 4x setup, you still need effective blower GPUs (and the standard design may prove adequate for this), but this resolves the PSU problem.\\n\\nFigure 6: Reducing the power limit has a slight cooling effect. Reducing the RTX 2080 Ti power limit by 50-60 W decreases temperatures slightly and fans run more silent.\\nFigure 6: Reducing the power limit has a slight cooling effect. Reducing the RTX 2080 Ti power limit by 50-60 W decreases temperatures slightly and fans run more silent.\\nYou might ask, “Doesn’t this slow down the GPU?” Yes, it does, but the question is by how much. I benchmarked the 4x RTX 2080 Ti system shown in Figure 5 under different power limits to test this. I benchmarked the time for 500 mini-batches for BERT Large during inference (excluding the softmax layer). I choose BERT Large inference since, from my experience, this is the deep learning model that stresses the GPU the most. As such, I would expect power limiting to have the most massive slowdown for this model. As such, the slowdowns reported here are probably close to the maximum slowdowns that you can expect. The results are shown in Figure 7.\\n\\nFigure 7: Measured slowdown for a given power limit on an RTX 2080 Ti. Measurements taken are mean processing times for 500 mini-batches of BERT Large during inference (excluding softmax layer).\\nFigure 7: Measured slowdown for a given power limit on an RTX 2080 Ti. Measurements taken are mean processing times for 500 mini-batches of BERT Large during inference (excluding softmax layer).\\nAs we can see, setting the power limit does not seriously affect performance. Limiting the power by 50W — more than enough to handle 4x RTX 3090 — decreases performance by only 7%.\\n\\nRTX 4090s and Melting Power Connectors: How to Prevent Problems\\nThere was a misconception that RTX 4090 power cables melt because they were bent. However, it was found that only 0.1% of users had this problem and the problem occured due to user error. Here a video that shows that the main problem is that cables were not inserted correctly.\\n\\nSo using RTX 4090 cards is perfectly safe if you follow the following install instructions:\\n\\nIf you use an old cable or old GPU make sure the contacts are free of debri / dust.\\nUse the power connector and stick it into the socket until you hear a *click* — this is the most important part.\\nTest for good fit by wiggling the power cable left to right. The cable should not move.\\nCheck the contact with the socket visually, there should be no gap between cable and socket.\\n8-bit Float Support in H100 and RTX 40 series GPUs\\nThe support of the 8-bit Float (FP8) is a huge advantage for the RTX 40 series and H100 GPUs. With 8-bit inputs it allows you to load the data for matrix multiplication twice as fast, you can store twice as much matrix elements in your caches which in the Ada and Hopper architecture are very large, and now with FP8 tensor cores you get 0.66 PFLOPS of compute for a RTX 4090 — this is more FLOPS then the entirety of the worlds fastest supercomputer in year 2007. 4x RTX 4090 with FP8 compute rival the faster supercomputer in the world in year 2010 (deep learning started to work just in 2009).\\n\\nThe main problem with using 8-bit precision is that transformers can get very unstable with so few bits and crash during training or generate non-sense during inference. I have written a paper about the emergence of instabilities in large language models and I also written a more accessible blog post.\\n\\nThe main take-way is this: Using 8-bit instead of 16-bit makes things very unstable, but if you keep a couple of dimensions in high precision everything works just fine.\\n\\n\\nMain results from my work on 8-bit matrix multiplication for Large Language Models (LLMs). We can see that the best 8-bit baseline fails to deliver good zero-shot performance. The method that I developed, LLM.int8(), can perform Int8 matrix multiplication with the same results as the 16-bit baseline.\\nBut Int8 was already supported by the RTX 30 / A100 / Ampere generation GPUs, why is FP8 in the RTX 40 another big upgrade? The FP8 data type is much more stable than the Int8 data type and its easy to use it in functions like layer norm or non-linear functions, which are difficult to do with Integer data types. This will make it very straightforward to use it in training and inference. I think this will make FP8 training and inference relatively common in a couple of months.\\n\\nIf you want to read more about the advantages of Float vs Integer data types you can read my recent paper about k-bit inference scaling laws. Below you can see one relevant main result for Float vs Integer data types from this paper. We can see that bit-by-bit, the FP4 data type preserve more information than Int4 data type and thus improves the mean LLM zeroshot accuracy across 4 tasks.\\n\\n\\n4-bit Inference scaling laws for Pythia Large Language Models for different data types. We see that bit-by-bit, 4-bit float data types have better zeroshot accuracy compared to the Int4 data types.\\nRaw Performance Ranking of GPUs\\nBelow we see a chart of raw relevative performance across all GPUs. We see that there is a gigantic gap in 8-bit performance of H100 GPUs and old cards that are optimized for 16-bit performance.\\n\\n\\nShown is raw relative transformer performance of GPUs. For example, an RTX 4090 has about 0.33x performance of a H100 SMX for 8-bit inference. In other words, a H100 SMX is three times faster for 8-bit inference compared to a RTX 4090.\\nFor this data, I did not model 8-bit compute for older GPUs. I did so, because 8-bit Inference and training are much more effective on Ada/Hopper GPUs because of the 8-bit Float data type and Tensor Memory Accelerator (TMA) which saves the overhead of computing read/write indices which is particularly helpful for 8-bit matrix multiplication. Ada/Hopper also have FP8 support, which makes in particular 8-bit training much more effective.\\n\\nI did not model numbers for 8-bit training because to model that I need to know the latency of L1 and L2 caches on Hopper/Ada GPUs, and they are unknown and I do not have access to such GPUs. On Hopper/Ada, 8-bit training performance can well be 3-4x of 16-bit training performance if the caches are as fast as rumored.\\n\\nBut even with the new FP8 tensor cores there are some additional issues which are difficult to take into account when modeling GPU performance. For example, FP8 tensor cores do not support transposed matrix multiplication which means backpropagation needs either a separate transpose before multiplication or one needs to hold two sets of weights — one transposed and one non-transposed — in memory. I used two sets of weight when I experimented with Int8 training in my LLM.int8() project and this reduced the overall speedups quite significantly. I think one can do better with the right algorithms/software, but this shows that missing features like a transposed matrix multiplication for tensor cores can affect performance.\\n\\nFor old GPUs, Int8 inference performance is close to the 16-bit inference performance for models below 13B parameters. Int8 performance on old GPUs is only relevant if you have relatively large models with 175B parameters or more. If you are interested in 8-bit performance of older GPUs, you can read the Appendix D of my LLM.int8() paper where I benchmark Int8 performance.\\n\\nGPU Deep Learning Performance per Dollar\\nBelow we see the chart for the performance per US dollar for all GPUs sorted by 8-bit inference performance. How to use the chart to find a suitable GPU for you is as follows:\\n\\nDetermine the amount of GPU memory that you need (rough heuristic: at least 12 GB for image generation; at least 24 GB for work with transformers)\\nWhile 8-bit inference and training is experimental, it will become standard within 6 months. You might need to do some extra difficult coding to work with 8-bit in the meantime. Is that OK for you? If not, select for 16-bit performance.\\nUsing the metric determined in (2), find the GPU with the highest relative performance/dollar that has the amount of memory you need.\\nWe can see that the RTX 4070 Ti is most cost-effective for 8-bit and 16-bit inference while the RTX 3080 remains most cost-effective for 16-bit training. While these GPUs are most cost-effective, they are not necessarily recommended as they do not have sufficient memory for many use-cases. However, it might be the ideal cards to get started on your deep learning journey. Some of these GPUs are excellent for Kaggle competition where one can often rely on smaller models. Since to do well in Kaggle competitions the method of how you work is more important than the models size, many of these smaller GPUs are excellent for Kaggle competitions.\\n\\nThe best GPUs for academic and startup servers seem to be A6000 Ada GPUs (not to be confused with A6000 Turing). The H100 SXM GPU is also very cost effective and has high memory and very strong performance. If I would build a small cluster for a company/academic lab, I would use 66-80% A6000 GPUs and 20-33% H100 SXM GPUs. If I get a good deal on L40 GPUs, I would also pick them instead of A6000, so you can always ask for a quote on these.\\n\\n\\nShown is relative performance per US Dollar of GPUs normalized by the cost for a desktop computer and the average Amazon and eBay price for each GPU. Additionally, the electricity cost of ownership for 5 years is added with an electricity price of 0.175 USD per kWh and a 15% GPU utilization rate. The electricity cost for a RTX 4090 is about $100 per year. How to read and interpret the chart: a desktop computer with RTX 4070 Ti cards owned for 5 years yields about 2x more 8-bit inference performance per dollar compared to a RTX 3090 GPU.\\nGPU Recommendations\\nI have a create a recommendation flow-chart that you can see below (click here for interactive app from Nan Xiao). While this chart will help you in 80% of cases, it might not quite work for you because the options might be too expensive. In that case, try to look at the benchmarks above and pick the most cost effective GPU that still has enough GPU memory for your use-case. You can estimate the GPU memory needed by running your problem in the vast.ai or Lambda Cloud for a while so you know what you need. The vast.ai or Lambda Cloud might also work well if you only need a GPU very sporadically (every couple of days for a few hours) and you do not need to download and process large dataset to get started. However, cloud GPUs are usually not a good option if you use your GPU for many months with a high usage rate each day (12 hours each day). You can use the example in the “When is it better to use the cloud vs a dedicated GPU desktop/server?” section below to determine if cloud GPUs are good for you.\\n\\n\\nGPU recommendation chart for Ada/Hopper GPUs. Follow the answers to the Yes/No questions to find the GPU that is most suitable for you. While this chart works well in about 80% of cases, you might end up with a GPU that is too expensive. Use the cost/performance charts above to make a selection instead. [interactive app]\\nIs it better to wait for future GPUs for an upgrade? The future of GPUs.\\nTo understand if it makes sense to skip this generation and buy the next generation of GPUs, it makes sense to talk a bit about what improvements in the future will look like.\\n\\nIn the past it was possible to shrink the size of transistors to improve speed of a processor. This is coming to an end now. For example, while shrinking SRAM increased its speed (smaller distance, faster memory access), this is no longer the case. Current improvements in SRAM do not improve its performance anymore and might even be negative. While logic such as Tensor Cores get smaller, this does not necessarily make GPU faster since the main problem for matrix multiplication is to get memory to the tensor cores which is dictated by SRAM and GPU RAM speed and size. GPU RAM still increases in speed if we stack memory modules into high-bandwidth modules (HBM3+), but these are too expensive to manufacture for consumer applications. The main way to improve raw speed of GPUs is to use more power and more cooling as we have seen in the RTX 30s and 40s series. But this cannot go on for much longer.\\n\\nChiplets such as used by AMD CPUs are another straightforward way forward. AMD beat Intel by developing CPU chiplets. Chiplets are small chips that are fused together with a high speed on-chip network. You can think about them as two GPUs that are so physically close together that you can almost consider them a single big GPU. They are cheaper to manufacture, but more difficult to combine into one big chip. So you need know-how and fast connectivity between chiplets. AMD has a lot of experience with chiplet design. AMD’s next generation GPUs are going to be chiplet designs, while NVIDIA currently has no public plans for such designs. This may mean that the next generation of AMD GPUs might be better in terms of cost/performance compared to NVIDIA GPUs.\\n\\nHowever, the main performance boost for GPUs is currently specialized logic. For example, the asynchronous copy hardware units on the Ampere generation (RTX 30 / A100 / RTX 40) or the extension, the Tensor Memory Accelerator (TMA), both reduce the overhead of copying memory from the slow global memory to fast shared memory (caches) through specialized hardware and so each thread can do more computation. The TMA also reduces overhead by performing automatic calculations of read/write indices which is particularly important for 8-bit computation where one has double the elements for the same amount of memory compared to 16-bit computation. So specialized hardware logic can accelerate matrix multiplication further.\\nLow-bit precision is another straightforward way forward for a couple of years. We will see widespread adoption of 8-bit inference and training in the next months. We will see widespread 4-bit inference in the next year. Currently, the technology for 4-bit training does not exists, but research looks promising and I expect the first high performance FP4 Large Language Model (LLM) with competitive predictive performance to be trained in 1-2 years time.\\n\\nGoing to 2-bit precision for training currently looks pretty impossible, but it is a much easier problem than shrinking transistors further. So progress in hardware mostly depends on software and algorithms that make it possible to use specialized features offered by the hardware.\\n\\nWe will probably be able to still improve the combination of algorithms + hardware to the year 2032, but after that will hit the end of GPU improvements (similar to smartphones). The wave of performance improvements after 2032 will come from better networking algorithms and mass hardware. It is uncertain if consumer GPUs will be relevant at this point. It might be that you need an RTX 9090 to run run Super HyperStableDiffusion Ultra Plus 9000 Extra or OpenChatGPT 5.0, but it might also be that some company will offer a high-quality API that is cheaper than the electricity cost for a RTX 9090 and you want to use a laptop + API for image generation and other tasks.\\n\\nOverall, I think investing into a 8-bit capable GPU will be a very solid investment for the next 9 years. Improvements at 4-bit and 2-bit are likely small and other features like Sort Cores would only become relevant once sparse matrix multiplication can be leveraged well. We will probably see some kind of other advancement in 2-3 years which will make it into the next GPU 4 years from now, but we are running out of steam if we keep relying on matrix multiplication. This makes investments into new GPUs last longer.\\n\\nQuestion & Answers & Misconceptions\\nDo I need PCIe 4.0 or PCIe 5.0?\\nGenerally, no. PCIe 5.0 or 4.0 is great if you have a GPU cluster. It is okay if you have an 8x GPU machine, but otherwise, it does not yield many benefits. It allows better parallelization and a bit faster data transfer. Data transfers are not a bottleneck in any application. In computer vision, in the data transfer pipeline, the data storage can be a bottleneck, but not the PCIe transfer from CPU to GPU. So there is no real reason to get a PCIe 5.0 or 4.0 setup for most people. The benefits will be maybe 1-7% better parallelization in a 4 GPU setup.\\n\\nDo I need 8x/16x PCIe lanes?\\nSame as with PCIe 4.0 — generally, no. PCIe lanes are needed for parallelization and fast data transfers, which are seldom a bottleneck. Operating GPUs on 4x lanes is fine, especially if you only have 2 GPUs. For a 4 GPU setup, I would prefer 8x lanes per GPU, but running them at 4x lanes will probably only decrease performance by around 5-10% if you parallelize across all 4 GPUs.\\n\\nHow do I fit 4x RTX 4090 or 3090 if they take up 3 PCIe slots each?\\nYou need to get one of the two-slot variants, or you can try to spread them out with PCIe extenders. Besides space, you should also immediately think about cooling and a suitable PSU.\\n\\nPCIe extenders might also solve both space and cooling issues, but you need to make sure that you have enough space in your case to spread out the GPUs. Make sure your PCIe extenders are long enough!\\n\\nHow do I cool 4x RTX 3090 or 4x RTX 3080?\\nSee the previous section.\\n\\nCan I use multiple GPUs of different GPU types?\\nYes, you can! But you cannot parallelize efficiently across GPUs of different types since you will often go at the speed of the slowest GPU (data and fully sharded parallelism). So different GPUs work just fine, but parallelization across those GPUs will be inefficient since the fastest GPU will wait for the slowest GPU to catch up to a synchronization point (usually gradient update).\\n\\nWhat is NVLink, and is it useful?\\nGenerally, NVLink is not useful. NVLink is a high speed interconnect between GPUs. It is useful if you have a GPU cluster with +128 GPUs. Otherwise, it yields almost no benefits over standard PCIe transfers.\\n\\nI do not have enough money, even for the cheapest GPUs you recommend. What can I do?\\nDefinitely buy used GPUs. You can buy a small cheap GPU for prototyping and testing and then roll out for full experiments to the cloud like vast.ai or Lambda Cloud. This can be cheap if you train/fine-tune/inference on large models only every now and then and spent more time protoyping on smaller models.\\n\\nWhat is the carbon footprint of GPUs? How can I use GPUs without polluting the environment?\\nI built a carbon calculator for calculating your carbon footprint for academics (carbon from flights to conferences + GPU time). The calculator can also be used to calculate a pure GPU carbon footprint. You will find that GPUs produce much, much more carbon than international flights. As such, you should make sure you have a green source of energy if you do not want to have an astronomical carbon footprint. If no electricity provider in our area provides green energy, the best way is to buy carbon offsets. Many people are skeptical about carbon offsets. Do they work? Are they scams?\\n\\nI believe skepticism just hurts in this case, because not doing anything would be more harmful than risking the probability of getting scammed. If you worry about scams, just invest in a portfolio of offsets to minimize risk.\\n\\nI worked on a project that produced carbon offsets about ten years ago. The carbon offsets were generated by burning leaking methane from mines in China. UN officials tracked the process, and they required clean digital data and physical inspections of the project site. In that case, the carbon offsets that were produced were highly reliable. I believe many other projects have similar quality standards.\\n\\nWhat do I need to parallelize across two machines?\\nIf you want to be on the safe side, you should get at least +50Gbits/s network cards to gain speedups if you want to parallelize across machines. I recommend having at least an EDR Infiniband setup, meaning a network card with at least 50 GBit/s bandwidth. Two EDR cards with cable are about $500 on eBay.\\n\\nIn some cases, you might be able to get away with 10 Gbit/s Ethernet, but this is usually only the case for special networks (certain convolutional networks) or if you use certain algorithms (Microsoft DeepSpeed).\\n\\nIs the sparse matrix multiplication features suitable for sparse matrices in general?\\nIt does not seem so. Since the granularity of the sparse matrix needs to have 2 zero-valued elements, every 4 elements, the sparse matrices need to be quite structured. It might be possible to adjust the algorithm slightly, which involves that you pool 4 values into a compressed representation of 2 values, but this also means that precise arbitrary sparse matrix multiplication is not possible with Ampere GPUs.\\n\\nDo I need an Intel CPU to power a multi-GPU setup?\\nI do not recommend Intel CPUs unless you heavily use CPUs in Kaggle competitions (heavy linear algebra on the CPU). Even for Kaggle competitions AMD CPUs are still great, though. AMD CPUs are cheaper and better than Intel CPUs in general for deep learning. For a 4x GPU built, my go-to CPU would be a Threadripper. We built dozens of systems at our university with Threadrippers, and they all work great — no complaints yet. For 8x GPU systems, I would usually go with CPUs that your vendor has experience with. CPU and PCIe/system reliability is more important in 8x systems than straight performance or straight cost-effectiveness.\\n\\nDoes computer case design matter for cooling?\\nNo. GPUs are usually perfectly cooled if there is at least a small gap between GPUs. Case design will give you 1-3 C better temperatures, space between GPUs will provide you with 10-30 C improvements. The bottom line, if you have space between GPUs, cooling does not matter. If you have no space between GPUs, you need the right cooler design (blower fan) or another solution (water cooling, PCIe extenders), but in either case, case design and case fans do not matter.\\n\\nWill AMD GPUs + ROCm ever catch up with NVIDIA GPUs + CUDA?\\nNot in the next 1-2 years. It is a three-way problem: Tensor Cores, software, and community.\\n\\nAMD GPUs are great in terms of pure silicon: Great FP16 performance, great memory bandwidth. However, their lack of Tensor Cores or the equivalent makes their deep learning performance poor compared to NVIDIA GPUs. Packed low-precision math does not cut it. Without this hardware feature, AMD GPUs will never be competitive. Rumors show that some data center card with Tensor Core equivalent is planned for 2020, but no new data emerged since then. Just having data center cards with a Tensor Core equivalent would also mean that few would be able to afford such AMD GPUs, which would give NVIDIA a competitive advantage.\\n\\nLet’s say AMD introduces a Tensor-Core-like-hardware feature in the future. Then many people would say, “But there is no software that works for AMD GPUs! How am I supposed to use them?” This is mostly a misconception. The AMD software via ROCm has come to a long way, and support via PyTorch is excellent. While I have not seen many experience reports for AMD GPUs + PyTorch, all the software features are integrated. It seems, if you pick any network, you will be just fine running it on AMD GPUs. So here AMD has come a long way, and this issue is more or less solved.\\n\\nHowever, if you solve software and the lack of Tensor Cores, AMD still has a problem: the lack of community. If you have a problem with NVIDIA GPUs, you can Google the problem and find a solution. That builds a lot of trust in NVIDIA GPUs. You have the infrastructure that makes using NVIDIA GPUs easy (any deep learning framework works, any scientific problem is well supported). You have the hacks and tricks that make usage of NVIDIA GPUs a breeze (e.g., apex). You can find experts on NVIDIA GPUs and programming around every other corner while I knew much less AMD GPU experts.\\n\\nIn the community aspect, AMD is a bit like Julia vs Python. Julia has a lot of potential, and many would say, and rightly so, that it is the superior programming language for scientific computing. Yet, Julia is barely used compared to Python. This is because the Python community is very strong. Numpy, SciPy, Pandas are powerful software packages that a large number of people congregate around. This is very similar to the NVIDIA vs AMD issue.\\n\\nThus, it is likely that AMD will not catch up until Tensor Core equivalent is introduced (1/2 to 1 year?) and a strong community is built around ROCm (2 years?). AMD will always snatch a part of the market share in specific subgroups (e.g., cryptocurrency mining, data centers). Still, in deep learning, NVIDIA will likely keep its monopoly for at least a couple more years.\\n\\nWhen is it better to use the cloud vs a dedicated GPU desktop/server?\\nRule-of-thumb: If you expect to do deep learning for longer than a year, it is cheaper to get a desktop GPU. Otherwise, cloud instances are preferable unless you have extensive cloud computing skills and want the benefits of scaling the number of GPUs up and down at will.\\n\\nNumbers in the following paragraphs are going to change, but it serves as a scenario that helps you to understand the rough costs. You can use similar math to determine if cloud GPUs are the best solution for you.\\n\\nFor the exact point in time when a cloud GPU is more expensive than a desktop depends highly on the service that you are using, and it is best to do a little math on this yourself. Below I do an example calculation for an AWS V100 spot instance with 1x V100 and compare it to the price of a desktop with a single RTX 3090 (similar performance). The desktop with RTX 3090 costs $2,200 (2-GPU barebone + RTX 3090). Additionally, assuming you are in the US, there is an additional $0.12 per kWh for electricity. This compares to $2.14 per hour for the AWS on-demand instance.\\n\\nAt 15% utilization per year, the desktop uses:\\n\\n(350 W (GPU) + 100 W (CPU))*0.15 (utilization) * 24 hours * 365 days = 591 kWh per year\\n\\nSo 591 kWh of electricity per year, that is an additional $71.\\n\\nThe break-even point for a desktop vs a cloud instance at 15% utilization (you use the cloud instance 15% of time during the day), would be about 300 days ($2,311 vs $2,270):\\n\\n$2.14/h * 0.15 (utilization) * 24 hours * 300 days = $2,311\\n\\nSo if you expect to run deep learning models after 300 days, it is better to buy a desktop instead of using AWS on-demand instances.\\n\\nYou can do similar calculations for any cloud service to make the decision if you go for a cloud service or a desktop.\\n\\nCommon utilization rates are the following:\\n\\nPhD student personal desktop: < 15%\\nPhD student slurm GPU cluster: > 35%\\nCompany-wide slurm research cluster: > 60%\\nIn general, utilization rates are lower for professions where thinking about cutting edge ideas is more important than developing practical products. Some areas have low utilization rates (interpretability research), while other areas have much higher rates (machine translation, language modeling). In general, the utilization of personal machines is almost always overestimated. Commonly, most personal systems have a utilization rate between 5-10%. This is why I would highly recommend slurm GPU clusters for research groups and companies instead of individual desktop GPU machines.\\n\\nVersion History\\n2023-01-30: Improved font and recommendation chart. Added 5 years cost of ownership electricity perf/USD chart. Updated Async copy and TMA functionality. Slight update to FP8 training. General improvements.\\n2023-01-16: Added Hopper and Ada GPUs. Added GPU recommendation chart. Added information about the TMA unit and L2 cache.\\n2020-09-20: Added discussion of using power limiting to run 4x RTX 3090 systems. Added older GPUs to the performance and cost/performance charts. Added figures for sparse matrix multiplication.\\n2020-09-07: Added NVIDIA Ampere series GPUs. Included lots of good-to-know GPU details.\\n2019-04-03: Added RTX Titan and GTX 1660 Ti. Updated TPU section. Added startup hardware discussion.\\n2018-11-26: Added discussion of overheating issues of RTX cards.\\n2018-11-05: Added RTX 2070 and updated recommendations. Updated charts with hard performance data. Updated TPU section.\\n2018-08-21: Added RTX 2080 and RTX 2080 Ti; reworked performance analysis\\n2017-04-09: Added cost-efficiency analysis; updated recommendation with NVIDIA Titan Xp\\n2017-03-19: Cleaned up blog post; added GTX 1080 Ti\\n2016-07-23: Added Titan X Pascal and GTX 1060; updated recommendations\\n2016-06-25: Reworked multi-GPU section; removed simple neural network memory section as no longer relevant; expanded convolutional memory section; truncated AWS section due to not being efficient anymore; added my opinion about the Xeon Phi; added updates for the GTX 1000 series\\n2015-08-20: Added section for AWS GPU instances; added GTX 980 Ti to the comparison relation\\n2015-04-22: GTX 580 no longer recommended; added performance relationships between cards\\n2015-03-16: Updated GPU recommendations: GTX 970 and GTX 580\\n2015-02-23: Updated GPU recommendations and memory calculations\\n2014-09-28: Added emphasis for memory requirement of CNNs\\nAcknowledgments\\nI thank Suhail for making me aware of outdated prices on H100 GPUs, Gjorgji Kjosev for pointing out font issues, Anonymous for pointing out that the TMA unit does not exist on Ada GPUs, Scott Gray for pointing out that FP8 tensor cores have no transposed matrix multiplication, and reddit and HackerNews users for pointing out many other improvements.\\n\\nFor past updates of this blog post, I want to thank Mat Kelcey for helping me to debug and test custom code for the GTX 970; I want to thank Sander Dieleman for making me aware of the shortcomings of my GPU memory advice for convolutional nets; I want to thank Hannes Bretschneider for pointing out software dependency problems for the GTX 580; and I want to thank Oliver Griesel for pointing out notebook solutions for AWS instances. I want to thank Brad Nemire for providing me with an RTX Titan for benchmarking purposes. I want to thank Agrin Hilmkil, Ari Holtzman, Gabriel Ilharco, Nam Pho for their excellent feedback on the previous version of this blog post.\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What kind of model is the bling-phi-3 model'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "for i in range(0, len(df)):\n",
    "    print(i)\n",
    "    a = []\n",
    "    question = df.iloc[i, 1]\n",
    "    try:\n",
    "        docs = get_docs_rag(question)\n",
    "        string_check = \"\" \n",
    "        for doc in docs:\n",
    "            if doc.page_content not in string_check:\n",
    "                string_check += doc.page_content\n",
    "                a.append(doc.page_content)\n",
    "        contexts.append(a)\n",
    "    except:\n",
    "        contexts.append(\"Error\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What kind of model is the bling-phi-3 model</td>\n",
       "      <td>The bling-phi-3 model is the newest and most a...</td>\n",
       "      <td>[Which GPU(s) to Get for Deep Learning: My Exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the advantages and disadvantages of t...</td>\n",
       "      <td>The advantage of BM25 is that it is efficient....</td>\n",
       "      <td>[Which GPU(s) to Get for Deep Learning: My Exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who was Duke Stelmane?</td>\n",
       "      <td>Duke Stelmane was a major figure of the Knight...</td>\n",
       "      <td>[The Emperor is a mind flayer who appears in B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What items do Rudolf's family take from the Je...</td>\n",
       "      <td>Rudolf's wife, Hedwig, often receives luxury a...</td>\n",
       "      <td>[‘The Zone Of Interest’ Ending Explained &amp; Fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the rules for developing general purp...</td>\n",
       "      <td>General purpose AI models that were trained us...</td>\n",
       "      <td>[Why do we need to regulate the use of Artific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What can moss be used for?</td>\n",
       "      <td>Harvesting moss gives 1 foraging exp per moss ...</td>\n",
       "      <td>[Version History\\n(Redirected from Version his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>In what contexts is BERT mentioned?</td>\n",
       "      <td>It is mentioned that for BERT large during tra...</td>\n",
       "      <td>[A Survey on Retrieval-Augmented Text Generati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What enemies are encountered in the second enc...</td>\n",
       "      <td>26 kobolds and 1 kobold inventor are encounter...</td>\n",
       "      <td>[Bullet Kin\\nBullet Kin are one of the most co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What colour is Nan-E?</td>\n",
       "      <td>No answer</td>\n",
       "      <td>[Space Babies\\n\\nOriginal Airdate: 11 May 2024...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do sets in Python compare to sets in Gleam?</td>\n",
       "      <td>No answer</td>\n",
       "      <td>[Gleam for Python users\\nHello productive prag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Which masked language model was chosen for the...</td>\n",
       "      <td>No answer</td>\n",
       "      <td>[Semantic and Textual Inference Chatbot Interf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What did the goblins say?</td>\n",
       "      <td>No answer</td>\n",
       "      <td>[---The Paths through the Underground/Underdar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What relgion are the members of the Hoss family?</td>\n",
       "      <td>No answer</td>\n",
       "      <td>[‘The Zone Of Interest’ Ending Explained &amp; Fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What are the key topics of this article?</td>\n",
       "      <td>The key topics of this article are: \"why prior...</td>\n",
       "      <td>[The best sci-fi and fantasy books of 2023\\nIt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What data did was used to test the prototype?</td>\n",
       "      <td>Grace Hopper's Wikipedia page and Alan Turing'...</td>\n",
       "      <td>[Semantic and Textual Inference Chatbot Interf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>When did mushroom trees stop dropping wood?</td>\n",
       "      <td>Mushrrom trees stopped dropping wood in patch ...</td>\n",
       "      <td>[Version History\\n(Redirected from Version his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What do keybullet kin drop?</td>\n",
       "      <td>Keybullet kin drop a key upon death.</td>\n",
       "      <td>[Bullet Kin\\nBullet Kin are one of the most co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What kind of gun does the bandana bullet kin use?</td>\n",
       "      <td>The bandana bullet kin wields a machine pistol.</td>\n",
       "      <td>[Bullet Kin\\nBullet Kin are one of the most co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Who wrote 'Divine Rivals'?</td>\n",
       "      <td>Rebecca Ross wrote 'Divine Rivals'.</td>\n",
       "      <td>[The best sci-fi and fantasy books of 2023\\nIt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What happens on day 2?</td>\n",
       "      <td>After a few miles of winding tunnel, you emerg...</td>\n",
       "      <td>[---The Paths through the Underground/Underdar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0         What kind of model is the bling-phi-3 model   \n",
       "1   What are the advantages and disadvantages of t...   \n",
       "2                              Who was Duke Stelmane?   \n",
       "3   What items do Rudolf's family take from the Je...   \n",
       "4   What are the rules for developing general purp...   \n",
       "5                          What can moss be used for?   \n",
       "6                 In what contexts is BERT mentioned?   \n",
       "7   What enemies are encountered in the second enc...   \n",
       "8                               What colour is Nan-E?   \n",
       "9     How do sets in Python compare to sets in Gleam?   \n",
       "10  Which masked language model was chosen for the...   \n",
       "11                          What did the goblins say?   \n",
       "12   What relgion are the members of the Hoss family?   \n",
       "13           What are the key topics of this article?   \n",
       "14      What data did was used to test the prototype?   \n",
       "15        When did mushroom trees stop dropping wood?   \n",
       "16                        What do keybullet kin drop?   \n",
       "17  What kind of gun does the bandana bullet kin use?   \n",
       "18                         Who wrote 'Divine Rivals'?   \n",
       "19                             What happens on day 2?   \n",
       "\n",
       "                                         ground_truth  \\\n",
       "0   The bling-phi-3 model is the newest and most a...   \n",
       "1   The advantage of BM25 is that it is efficient....   \n",
       "2   Duke Stelmane was a major figure of the Knight...   \n",
       "3   Rudolf's wife, Hedwig, often receives luxury a...   \n",
       "4   General purpose AI models that were trained us...   \n",
       "5   Harvesting moss gives 1 foraging exp per moss ...   \n",
       "6   It is mentioned that for BERT large during tra...   \n",
       "7   26 kobolds and 1 kobold inventor are encounter...   \n",
       "8                                           No answer   \n",
       "9                                           No answer   \n",
       "10                                          No answer   \n",
       "11                                          No answer   \n",
       "12                                          No answer   \n",
       "13  The key topics of this article are: \"why prior...   \n",
       "14  Grace Hopper's Wikipedia page and Alan Turing'...   \n",
       "15  Mushrrom trees stopped dropping wood in patch ...   \n",
       "16               Keybullet kin drop a key upon death.   \n",
       "17    The bandana bullet kin wields a machine pistol.   \n",
       "18                Rebecca Ross wrote 'Divine Rivals'.   \n",
       "19  After a few miles of winding tunnel, you emerg...   \n",
       "\n",
       "                                             contexts  \n",
       "0   [Which GPU(s) to Get for Deep Learning: My Exp...  \n",
       "1   [Which GPU(s) to Get for Deep Learning: My Exp...  \n",
       "2   [The Emperor is a mind flayer who appears in B...  \n",
       "3   [‘The Zone Of Interest’ Ending Explained & Fil...  \n",
       "4   [Why do we need to regulate the use of Artific...  \n",
       "5   [Version History\\n(Redirected from Version his...  \n",
       "6   [A Survey on Retrieval-Augmented Text Generati...  \n",
       "7   [Bullet Kin\\nBullet Kin are one of the most co...  \n",
       "8   [Space Babies\\n\\nOriginal Airdate: 11 May 2024...  \n",
       "9   [Gleam for Python users\\nHello productive prag...  \n",
       "10  [Semantic and Textual Inference Chatbot Interf...  \n",
       "11  [---The Paths through the Underground/Underdar...  \n",
       "12  [‘The Zone Of Interest’ Ending Explained & Fil...  \n",
       "13  [The best sci-fi and fantasy books of 2023\\nIt...  \n",
       "14  [Semantic and Textual Inference Chatbot Interf...  \n",
       "15  [Version History\\n(Redirected from Version his...  \n",
       "16  [Bullet Kin\\nBullet Kin are one of the most co...  \n",
       "17  [Bullet Kin\\nBullet Kin are one of the most co...  \n",
       "18  [The best sci-fi and fantasy books of 2023\\nIt...  \n",
       "19  [---The Paths through the Underground/Underdar...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "df['contexts'] = contexts\n",
    "# df['contexts'] = df['contexts'].apply(lambda x: json.loads(x))\n",
    "df = df.iloc[:,1:]\n",
    "df=df.rename(columns={\n",
    "    \"answer\": \"ground_truth\"\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bling-phi-3 model is indeed the latest and most accurate model within the BLING/DRAGON framework. BLING models are designed to be small, CPU-based, and optimized for Retrieval-Augmented Generation (RAG), typically falling within the parameter range of 1 billion to 3 billion. These models are particularly focused on following instructions effectively, making them suitable for various applications that require responsive and context-aware interactions.\n",
      "\n",
      "On the other hand, DRAGON models represent a more robust category, being production-grade and optimized for RAG as well. These models generally have a larger parameter count, specifically in the range of 6 billion to 7 billion parameters. The DRAGON models are designed to deliver high-quality RAG capabilities, leveraging their larger size and enhanced architecture to provide more accurate and contextually relevant outputs.\n",
      "\n",
      "In summary, the bling-phi-3 model exemplifies the advancements in the BLING series, focusing on efficiency and instruction-following capabilities, while the DRAGON models cater to more demanding production environments, ensuring that RAG is effectively integrated into applications that require substantial computational resources and accuracy.\n",
      "The BM25 algorithm is a widely used information retrieval model that ranks documents based on the query terms they contain. Its advantages and disadvantages can be summarized as follows:\n",
      "\n",
      "### Advantages of BM25:\n",
      "1. **Efficiency**: BM25 is computationally efficient, making it suitable for large-scale information retrieval tasks. It can quickly rank documents based on their relevance to a given query.\n",
      "2. **Simplicity**: The algorithm is relatively straightforward to implement and understand, which makes it accessible for various applications in information retrieval.\n",
      "3. **Effectiveness**: BM25 has been shown to perform well in many practical scenarios, particularly in traditional text retrieval tasks, by leveraging term frequency and document length normalization.\n",
      "\n",
      "### Disadvantages of BM25:\n",
      "1. **Focus on Term Frequency and Presence**: BM25 primarily relies on the frequency of query terms within documents and their presence or absence. This means it may not fully capture the semantic meaning of the query or the context in which terms are used.\n",
      "2. **Overlooking Semantic Information**: Because BM25 does not consider the relationships between words or the overall meaning of the text, it can miss relevant documents that use synonyms or related terms that do not match the exact query terms.\n",
      "3. **Limited Contextual Understanding**: The algorithm does not account for the context in which terms appear, which can lead to less relevant results, especially in cases where the meaning of a term can vary significantly based on context.\n",
      "\n",
      "In summary, while BM25 is efficient and effective for many traditional retrieval tasks, its reliance on term frequency and presence can limit its ability to understand and retrieve documents based on semantic meaning. This limitation has led to the exploration of more advanced retrieval methods that incorporate semantic understanding, such as those utilizing deep learning and contextual embeddings.\n",
      "Duke Stelmane was indeed a significant character within the narrative of Baldur's Gate 3, particularly in relation to the Emperor. He was a prominent figure in the Knights of the Shield, which is described as a lawful and neutral evil organization composed of politicians and merchants who manipulate events from behind the scenes. His role as the Emperor's envoy suggests that he was deeply involved in the machinations of the Emperor, who is a mind flayer with a complex history and motives.\n",
      "\n",
      "The recent death of Duke Belynne Stelmane adds a layer of intrigue and potential conflict to the story. His demise could have significant implications for the power dynamics within the Knights of the Shield and the broader political landscape of Baldur's Gate. The fact that the Emperor had a controlling influence over Stelmane, keeping her enthralled, raises questions about the nature of their relationship and the extent of the Emperor's manipulations.\n",
      "\n",
      "This event may also affect the player's interactions and decisions within the game, as the power vacuum left by Stelmane's death could lead to shifts in alliances and rivalries among the remaining characters and factions. The narrative is designed to allow players to explore these themes and make choices that can influence the outcome of the story.\n",
      "In \"The Zone of Interest,\" Hedwig Hoss, the wife of Rudolf Hoss, is depicted as being completely desensitized to the atrocities occurring just outside her home, which is adjacent to the Auschwitz concentration camp. The film illustrates her character's moral detachment and complicity in the horrors of the Holocaust through her enjoyment of the luxuries that come from the suffering of others.\n",
      "\n",
      "One significant scene involves Hedwig receiving a fancy fur coat that was taken from a Jewish prisoner. When she tries on the coat, she discovers lipstick in one of the pockets, a stark reminder of the previous owner and the brutal reality of the circumstances surrounding its acquisition. Instead of feeling guilt or remorse, Hedwig is excited by the coat and the lipstick, showcasing her complete disconnection from the human cost of her lifestyle.\n",
      "\n",
      "Similarly, Rudolf's children are portrayed as being equally desensitized to the violence around them. They collect and play with gold teeth, which are remnants of the victims who were murdered in the camp. This behavior highlights the normalization of violence and the moral corruption that permeates their lives, as they treat the remains of the dead as mere playthings, reflecting the chilling impact of their upbringing in such a horrific environment.\n",
      "\n",
      "Overall, these elements serve to emphasize the film's exploration of complicity, moral blindness, and the chilling effects of systemic violence on individuals and families living in proximity to atrocity.\n",
      "The AI Act establishes a regulatory framework for the use of Artificial Intelligence (AI) within the European Union, particularly focusing on the governance of general-purpose AI models. According to the Act, general-purpose AI models that have been trained using a total computing power exceeding \\(10^{25}\\) FLOPs (Floating Point Operations per Second) are classified as carrying systemic risks. This classification is significant because it indicates that these models could potentially have widespread implications for safety, security, and fundamental rights due to their capabilities and the scale at which they operate.\n",
      "\n",
      "As part of the regulatory timeline, the AI Act stipulates that 12 months after its entry into force, specific obligations related to the governance of general-purpose AI will become applicable. This means that providers of such models will need to comply with certain requirements aimed at ensuring their responsible use and management.\n",
      "\n",
      "One of the key obligations for providers of general-purpose AI models is the requirement to disclose their energy consumption. This is particularly important given the high energy demands associated with training large AI models, which can have significant environmental impacts. By mandating transparency regarding energy usage, the AI Act aims to promote sustainability and accountability among AI providers.\n",
      "\n",
      "However, it is important to note that providers of free and open-source AI models are generally exempt from most of the obligations outlined in the AI Act. This exemption is designed to encourage innovation and accessibility in the AI field. Nevertheless, this exemption does not apply to providers of general-purpose AI models that are identified as having systemic risks. In such cases, these providers must still adhere to the relevant obligations, ensuring that even open-source models that could pose significant risks are subject to appropriate oversight and regulation.\n",
      "\n",
      "In summary, the AI Act introduces a structured approach to managing the risks associated with powerful AI models, particularly those that are widely used and have the potential for systemic impact. The focus on energy consumption disclosure and the specific obligations for high-risk models reflect a commitment to responsible AI development and deployment.\n",
      "In the context of the game mechanics described, harvesting moss grants the player 1 foraging experience point (exp) for each moss collected. Additionally, the crafting recipe for Speed Gro, a fertilizer that accelerates crop growth, requires five moss as an ingredient. This means that if a player collects five moss, they can use it to craft one Speed Gro, which will enhance their farming efficiency by increasing the growth rate of their crops. This mechanic encourages players to engage in foraging activities to gather resources that can be used for crafting beneficial items in the game.\n",
      "The author highlights the performance advantages of the Ada architecture, particularly in relation to the L2 cache size, which allows for efficient handling of large models like BERT Large during training. Specifically, the input and weight matrices for BERT Large can fit into the L2 cache of Ada GPUs, enabling faster access and computation compared to other architectures that may not have sufficient cache size.\n",
      "\n",
      "Additionally, the author conducted benchmarks on a 4x RTX 2080 Ti system, focusing on the inference performance of BERT Large. They chose this model because it is known to be particularly demanding on GPU resources, thus providing a good measure of the system's capabilities. The inference time for 500 mini-batches was recorded, excluding the softmax layer, to assess the efficiency of the GPU setup under a heavy workload. This benchmarking is crucial for understanding how well different GPU architectures can handle large-scale deep learning tasks, especially those that require significant computational power and memory bandwidth.\n",
      "In the second encounter during your journey through the tunnels, you will face a total of 26 kobolds and 1 kobold inventor. This encounter occurs on Day 4 of your travels, after navigating through a protracted tunnel that intersects with smaller, partially collapsed tunnels. \n",
      "\n",
      "As you enter a small chamber that has a rancid smell, a trap is triggered if someone enters first (unless it's Cad), causing a noose to snare their leg. This trap also creates a loud noise, alerting the kobolds nearby. The kobolds, upon hearing the commotion, rush into the room, chattering and demanding that you give them your belongings, threatening to stab if you refuse.\n",
      "\n",
      "The kobolds appear fearful despite their aggressive demands, and they mention that a \"big worm\" has caused chaos in the tunnels, which may provide an opportunity for negotiation or intimidation to avoid a fight. If you choose to engage with them, you will need to navigate the dynamics of this encounter carefully, as there are many kobolds present, and they may not be entirely willing to cooperate.\n",
      "I'm sorry, but I don't have an answer to your question.\n",
      "I'm sorry, but I don't have an answer to your question.\n",
      "I'm sorry, but I don't have an answer to your question.\n",
      "I'm sorry, but I don't have an answer to your question.\n",
      "I'm sorry, but I don't have an answer to your question.\n",
      "The article discusses several key topics related to maximizing impact as a Data Scientist, emphasizing the importance of focusing on impact rather than just output. Here’s a detailed breakdown of the key topics:\n",
      "\n",
      "1. **Why Prioritizing Impact Matters Not Just for Managers, but Also Individual Contributors (ICs)**:\n",
      "   - The article highlights that while managers are often seen as responsible for driving impact, individual contributors (ICs) also play a crucial role. Focusing on impact can lead to several benefits for ICs, including:\n",
      "     - **Reduced Frustration & Burnout**: ICs who see their work making a difference are less likely to feel frustrated or burned out.\n",
      "     - **Promotions**: Impact is a significant factor in promotions. Demonstrating an understanding of what drives business outcomes can position ICs for advancement.\n",
      "     - **Internal and External Opportunities**: ICs who make a noticeable impact are more likely to be recognized for internal opportunities and are more attractive to prospective employers.\n",
      "\n",
      "2. **Why Focusing on Impact is Hard**:\n",
      "   - The article explains that many people are accustomed to measuring their work in terms of inputs and outputs rather than impact. This mindset can create a sense of control, as individuals can directly influence their workload and deliverables. However, the actual impact of their work on business outcomes is often less clear and can feel disconnected from their daily tasks. Additionally, there is a tendency to view the responsibility for impact as belonging to others, which can hinder personal accountability.\n",
      "\n",
      "3. **How to Maximize Your Impact**:\n",
      "   - The article outlines several steps to help ICs become more impact-focused:\n",
      "     - **Understand What Impact Looks Like**: ICs should define success in terms of the impact their work has on business partners and downstream metrics.\n",
      "     - **Ensure Your Work Solves Real Business Problems**: ICs should dig deeper into requests to understand the underlying business priorities and ensure their work aligns with them.\n",
      "     - **Ensure There is Buy-In for Your Work**: Engaging stakeholders early and ensuring their support can help facilitate the implementation of your work.\n",
      "     - **Focus on the Highest-Impact Tasks**: Prioritizing time on strategic projects rather than getting bogged down by ad-hoc requests is essential for maximizing impact.\n",
      "\n",
      "4. **How to Overcome Common Challenges in Driving Real Impact**:\n",
      "   - The article discusses common pitfalls that can prevent work from having a meaningful impact, such as:\n",
      "     - **Lack of Clear Recommendations**: Providing analyses without actionable recommendations can lead to inaction.\n",
      "     - **Failure to Help Stakeholders Interpret Results**: Data scientists should assist stakeholders in understanding the implications of their findings.\n",
      "     - **Building Trust in Predictive Models**: Ensuring stakeholders understand and trust the outputs of predictive models is crucial for their adoption.\n",
      "     - **Creating Usable Dashboards**: Dashboards should be user-friendly and directly address urgent business needs to ensure they are utilized effectively.\n",
      "\n",
      "Overall, the article emphasizes that by shifting focus from mere outputs to tangible impacts, data scientists can enhance their effectiveness and career trajectories.\n",
      "In the prototype of the Semantic and Textual Inference Chatbot Interface (STICI-note), two versions were tested using the Wikipedia pages of Grace Hopper and Alan Turing. The question posed during the test was: \"What programming language was Grace Hopper known for creating?\"\n",
      "\n",
      "The results were as follows:\n",
      "\n",
      "1. **Version with Grace Hopper's Wikipedia Page**: This version provided a more accurate response regarding Hopper's contributions to programming languages. However, it contained several spelling errors, indicating that while the information was relevant, the output quality could still be improved. The response mentioned that Grace Hopper is credited with creating the first computer programming language called FLOW-MATIC and discussed her work on COBOL.\n",
      "\n",
      "2. **Version with Alan Turing's Wikipedia Page**: This version failed to provide a correct answer and instead generated a hallucinated response. It inaccurately claimed that Grace Hopper was best known for her work on developing an early version of BASIC, which is incorrect. This response highlighted the issue of hallucinations in language models, where the AI generates plausible-sounding but factually incorrect information.\n",
      "\n",
      "These tests underscored the importance of using relevant and accurate source material for generating reliable responses in AI systems, as well as the challenges of ensuring the quality and accuracy of the generated text.\n",
      "In the 1.4.0 update of Stardew Valley, mushroom trees were changed so that they no longer drop wood. This adjustment was part of a broader set of changes aimed at balancing gameplay and resource management within the game. Prior to this update, players could harvest wood from mushroom trees, but this feature was removed to enhance the uniqueness of mushroom trees and to encourage players to seek out other sources of wood in the game.\n",
      "Yes, Keybullet Kin drop a key upon death. However, if the player does not manage to kill them in time, they will disappear without dropping anything. Unlike other Bullet Kin, Keybullet Kin do not deal contact damage if they run into the player. Additionally, Jammed Keybullet Kin drop two keys instead of one and are faster, making them more challenging to catch.\n",
      "The Bandana Bullet Kin actually behave like regular Bullet Kin, but with a significantly increased fire rate and a higher magazine size than those wielding AK-47s. They wield Machine Pistols, making them more relentless in their attacks.\n",
      "Yes, Rebecca Ross is the author of \"Divine Rivals.\" The book is mentioned in the context of the best sci-fi and fantasy books of 2023, highlighting its inclusion among notable works in the genre. If you would like more information about the book or its themes, feel free to ask!\n",
      "In this scenario, you find yourself in a smaller grotto characterized by its impressive stalactites and stalagmites, which are glistening with condensation. The atmosphere is damp, and the air is filled with the earthy scent of wet stone and soil. As you explore this cavern, you notice that water is trickling down from various points, indicating that a water source is indeed nearby, although you cannot determine if it is the same underground river you had previously heard.\n",
      "\n",
      "Suddenly, you encounter two ropers. Ropers are typically large, tentacle-like creatures that can be quite dangerous. They often lie in wait, camouflaged among the rocks and stalagmites, ready to ensnare unsuspecting prey. In this case, they are likely seeking the next burrowed entrance left by the Kryn, which could lead to further exploration or potential danger.\n",
      "\n",
      "As you assess the situation, you have a few options:\n",
      "\n",
      "1. **Stealth Approach**: You could try to sneak past the ropers without drawing their attention. This would require a successful Dexterity check to avoid making noise or being seen.\n",
      "\n",
      "2. **Engage in Combat**: If you feel confident in your abilities, you could prepare for a fight. Ropers can be formidable foes, so ensure you are ready with your weapons and spells.\n",
      "\n",
      "3. **Use Diplomacy or Deception**: If you believe the ropers might be open to negotiation or if you have a way to deceive them, you could attempt to communicate. This could involve offering them something in exchange for safe passage.\n",
      "\n",
      "4. **Retreat**: If the situation seems too dangerous, you might consider retreating back into the tunnels to find another route.\n",
      "\n",
      "Your decision will determine how the encounter unfolds and what challenges or opportunities lie ahead in your journey through the underground.\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for i in range(0, len(df)):\n",
    "    question = df.iloc[i, 1]\n",
    "    try:\n",
    "        response = rag_chain.invoke({\"question\": question})\n",
    "        answers.append(response)\n",
    "        print(response)\n",
    "    except:\n",
    "        answers.append(\"Error\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What kind of model is the bling-phi-3 model</td>\n",
       "      <td>The bling-phi-3 model is the newest and most a...</td>\n",
       "      <td>[Which GPU(s) to Get for Deep Learning: My Exp...</td>\n",
       "      <td>The bling-phi-3 model is indeed the latest and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the advantages and disadvantages of t...</td>\n",
       "      <td>The advantage of BM25 is that it is efficient....</td>\n",
       "      <td>[Which GPU(s) to Get for Deep Learning: My Exp...</td>\n",
       "      <td>The BM25 algorithm is a widely used informatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who was Duke Stelmane?</td>\n",
       "      <td>Duke Stelmane was a major figure of the Knight...</td>\n",
       "      <td>[The Emperor is a mind flayer who appears in B...</td>\n",
       "      <td>Duke Stelmane was indeed a significant charact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What items do Rudolf's family take from the Je...</td>\n",
       "      <td>Rudolf's wife, Hedwig, often receives luxury a...</td>\n",
       "      <td>[‘The Zone Of Interest’ Ending Explained &amp; Fil...</td>\n",
       "      <td>In \"The Zone of Interest,\" Hedwig Hoss, the wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the rules for developing general purp...</td>\n",
       "      <td>General purpose AI models that were trained us...</td>\n",
       "      <td>[Why do we need to regulate the use of Artific...</td>\n",
       "      <td>The AI Act establishes a regulatory framework ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What can moss be used for?</td>\n",
       "      <td>Harvesting moss gives 1 foraging exp per moss ...</td>\n",
       "      <td>[Version History\\n(Redirected from Version his...</td>\n",
       "      <td>In the context of the game mechanics described...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>In what contexts is BERT mentioned?</td>\n",
       "      <td>It is mentioned that for BERT large during tra...</td>\n",
       "      <td>[A Survey on Retrieval-Augmented Text Generati...</td>\n",
       "      <td>The author highlights the performance advantag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What enemies are encountered in the second enc...</td>\n",
       "      <td>26 kobolds and 1 kobold inventor are encounter...</td>\n",
       "      <td>[Bullet Kin\\nBullet Kin are one of the most co...</td>\n",
       "      <td>In the second encounter during your journey th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What colour is Nan-E?</td>\n",
       "      <td>No answer</td>\n",
       "      <td>[Space Babies\\n\\nOriginal Airdate: 11 May 2024...</td>\n",
       "      <td>I'm sorry, but I don't have an answer to your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do sets in Python compare to sets in Gleam?</td>\n",
       "      <td>No answer</td>\n",
       "      <td>[Gleam for Python users\\nHello productive prag...</td>\n",
       "      <td>I'm sorry, but I don't have an answer to your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Which masked language model was chosen for the...</td>\n",
       "      <td>No answer</td>\n",
       "      <td>[Semantic and Textual Inference Chatbot Interf...</td>\n",
       "      <td>I'm sorry, but I don't have an answer to your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What did the goblins say?</td>\n",
       "      <td>No answer</td>\n",
       "      <td>[---The Paths through the Underground/Underdar...</td>\n",
       "      <td>I'm sorry, but I don't have an answer to your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What relgion are the members of the Hoss family?</td>\n",
       "      <td>No answer</td>\n",
       "      <td>[‘The Zone Of Interest’ Ending Explained &amp; Fil...</td>\n",
       "      <td>I'm sorry, but I don't have an answer to your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What are the key topics of this article?</td>\n",
       "      <td>The key topics of this article are: \"why prior...</td>\n",
       "      <td>[The best sci-fi and fantasy books of 2023\\nIt...</td>\n",
       "      <td>The article discusses several key topics relat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What data did was used to test the prototype?</td>\n",
       "      <td>Grace Hopper's Wikipedia page and Alan Turing'...</td>\n",
       "      <td>[Semantic and Textual Inference Chatbot Interf...</td>\n",
       "      <td>In the prototype of the Semantic and Textual I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>When did mushroom trees stop dropping wood?</td>\n",
       "      <td>Mushrrom trees stopped dropping wood in patch ...</td>\n",
       "      <td>[Version History\\n(Redirected from Version his...</td>\n",
       "      <td>In the 1.4.0 update of Stardew Valley, mushroo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What do keybullet kin drop?</td>\n",
       "      <td>Keybullet kin drop a key upon death.</td>\n",
       "      <td>[Bullet Kin\\nBullet Kin are one of the most co...</td>\n",
       "      <td>Yes, Keybullet Kin drop a key upon death. Howe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What kind of gun does the bandana bullet kin use?</td>\n",
       "      <td>The bandana bullet kin wields a machine pistol.</td>\n",
       "      <td>[Bullet Kin\\nBullet Kin are one of the most co...</td>\n",
       "      <td>The Bandana Bullet Kin actually behave like re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Who wrote 'Divine Rivals'?</td>\n",
       "      <td>Rebecca Ross wrote 'Divine Rivals'.</td>\n",
       "      <td>[The best sci-fi and fantasy books of 2023\\nIt...</td>\n",
       "      <td>Yes, Rebecca Ross is the author of \"Divine Riv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What happens on day 2?</td>\n",
       "      <td>After a few miles of winding tunnel, you emerg...</td>\n",
       "      <td>[---The Paths through the Underground/Underdar...</td>\n",
       "      <td>In this scenario, you find yourself in a small...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0         What kind of model is the bling-phi-3 model   \n",
       "1   What are the advantages and disadvantages of t...   \n",
       "2                              Who was Duke Stelmane?   \n",
       "3   What items do Rudolf's family take from the Je...   \n",
       "4   What are the rules for developing general purp...   \n",
       "5                          What can moss be used for?   \n",
       "6                 In what contexts is BERT mentioned?   \n",
       "7   What enemies are encountered in the second enc...   \n",
       "8                               What colour is Nan-E?   \n",
       "9     How do sets in Python compare to sets in Gleam?   \n",
       "10  Which masked language model was chosen for the...   \n",
       "11                          What did the goblins say?   \n",
       "12   What relgion are the members of the Hoss family?   \n",
       "13           What are the key topics of this article?   \n",
       "14      What data did was used to test the prototype?   \n",
       "15        When did mushroom trees stop dropping wood?   \n",
       "16                        What do keybullet kin drop?   \n",
       "17  What kind of gun does the bandana bullet kin use?   \n",
       "18                         Who wrote 'Divine Rivals'?   \n",
       "19                             What happens on day 2?   \n",
       "\n",
       "                                         ground_truth  \\\n",
       "0   The bling-phi-3 model is the newest and most a...   \n",
       "1   The advantage of BM25 is that it is efficient....   \n",
       "2   Duke Stelmane was a major figure of the Knight...   \n",
       "3   Rudolf's wife, Hedwig, often receives luxury a...   \n",
       "4   General purpose AI models that were trained us...   \n",
       "5   Harvesting moss gives 1 foraging exp per moss ...   \n",
       "6   It is mentioned that for BERT large during tra...   \n",
       "7   26 kobolds and 1 kobold inventor are encounter...   \n",
       "8                                           No answer   \n",
       "9                                           No answer   \n",
       "10                                          No answer   \n",
       "11                                          No answer   \n",
       "12                                          No answer   \n",
       "13  The key topics of this article are: \"why prior...   \n",
       "14  Grace Hopper's Wikipedia page and Alan Turing'...   \n",
       "15  Mushrrom trees stopped dropping wood in patch ...   \n",
       "16               Keybullet kin drop a key upon death.   \n",
       "17    The bandana bullet kin wields a machine pistol.   \n",
       "18                Rebecca Ross wrote 'Divine Rivals'.   \n",
       "19  After a few miles of winding tunnel, you emerg...   \n",
       "\n",
       "                                             contexts  \\\n",
       "0   [Which GPU(s) to Get for Deep Learning: My Exp...   \n",
       "1   [Which GPU(s) to Get for Deep Learning: My Exp...   \n",
       "2   [The Emperor is a mind flayer who appears in B...   \n",
       "3   [‘The Zone Of Interest’ Ending Explained & Fil...   \n",
       "4   [Why do we need to regulate the use of Artific...   \n",
       "5   [Version History\\n(Redirected from Version his...   \n",
       "6   [A Survey on Retrieval-Augmented Text Generati...   \n",
       "7   [Bullet Kin\\nBullet Kin are one of the most co...   \n",
       "8   [Space Babies\\n\\nOriginal Airdate: 11 May 2024...   \n",
       "9   [Gleam for Python users\\nHello productive prag...   \n",
       "10  [Semantic and Textual Inference Chatbot Interf...   \n",
       "11  [---The Paths through the Underground/Underdar...   \n",
       "12  [‘The Zone Of Interest’ Ending Explained & Fil...   \n",
       "13  [The best sci-fi and fantasy books of 2023\\nIt...   \n",
       "14  [Semantic and Textual Inference Chatbot Interf...   \n",
       "15  [Version History\\n(Redirected from Version his...   \n",
       "16  [Bullet Kin\\nBullet Kin are one of the most co...   \n",
       "17  [Bullet Kin\\nBullet Kin are one of the most co...   \n",
       "18  [The best sci-fi and fantasy books of 2023\\nIt...   \n",
       "19  [---The Paths through the Underground/Underdar...   \n",
       "\n",
       "                                               answer  \n",
       "0   The bling-phi-3 model is indeed the latest and...  \n",
       "1   The BM25 algorithm is a widely used informatio...  \n",
       "2   Duke Stelmane was indeed a significant charact...  \n",
       "3   In \"The Zone of Interest,\" Hedwig Hoss, the wi...  \n",
       "4   The AI Act establishes a regulatory framework ...  \n",
       "5   In the context of the game mechanics described...  \n",
       "6   The author highlights the performance advantag...  \n",
       "7   In the second encounter during your journey th...  \n",
       "8   I'm sorry, but I don't have an answer to your ...  \n",
       "9   I'm sorry, but I don't have an answer to your ...  \n",
       "10  I'm sorry, but I don't have an answer to your ...  \n",
       "11  I'm sorry, but I don't have an answer to your ...  \n",
       "12  I'm sorry, but I don't have an answer to your ...  \n",
       "13  The article discusses several key topics relat...  \n",
       "14  In the prototype of the Semantic and Textual I...  \n",
       "15  In the 1.4.0 update of Stardew Valley, mushroo...  \n",
       "16  Yes, Keybullet Kin drop a key upon death. Howe...  \n",
       "17  The Bandana Bullet Kin actually behave like re...  \n",
       "18  Yes, Rebecca Ross is the author of \"Divine Riv...  \n",
       "19  In this scenario, you find yourself in a small...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['answers'] = answers\n",
    "df = df.rename(columns={\"answers\": \"answer\"})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(df)):\n",
    "    question = df.iloc[i, 1]\n",
    "    if df.iloc[i, 3] == \"Error\":\n",
    "        try:\n",
    "            response = rag_chain.invoke({\"question\": question})\n",
    "            df.iloc[i, 3] = response\n",
    "            print(response)\n",
    "        except:\n",
    "            df.iloc[i, 3] = \"Error\"\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./RAG_responses.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  97%|█████████▋| 97/100 [01:30<00:03,  1.28s/it]Failed to parse output. Returning None.\n",
      "Evaluating: 100%|██████████| 100/100 [04:34<00:00,  2.74s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.6517, 'answer_relevancy': 0.6359, 'context_relevancy': 0.0006, 'context_precision': 0.6333, 'context_recall': 0.5500}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_relevancy,\n",
    "    context_recall,\n",
    "    context_precision\n",
    ")\n",
    "\n",
    "from ragas import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    dataset,\n",
    "    llm=llm,\n",
    "    raise_exceptions=False,\n",
    "    callbacks=None,\n",
    "    is_async=False,\n",
    "    embeddings=OpenAIEmbeddings(),\n",
    "    metrics=[\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_relevancy,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "    ],\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_pandas().to_csv('./RAG_evaluations.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.rename(columns={\"Question\":\"question\",\"Answer_RAG_FUSION\":\"answer\",'Context_RAG_FUSION':'contexts',\"Answer\":'ground_truth'})\n",
    "\n",
    "df['contexts'] = df['contexts'].apply(lambda x: json.loads(x))\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
